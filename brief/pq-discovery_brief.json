{
  "mission_id": "b2bac6c6-86cd-4d76-babc-ab3599c43c53",
  "mission_statement": "Build an open-source Rust CLI tool called 'pq-discovery' that scans a directory tree for usage of RSA-2048, RSA-4096, ECC (secp256k1, P-256, P-384, Ed25519, X25519), and other quantum-vulnerable cryptographic primitives in source code, configuration files, certificates, and key files. For each finding, the tool should recommend the appropriate NIST PQC replacement: ML-KEM-768/1024 for key exchange, ML-DSA-65/87 for signatures, SLH-DSA for conservative hash-based fallback. The tool should output a structured JSON report with file path, line number, detected primitive, risk level, and recommended replacement. Design for extensibility: pluggable scanners for different languages (Rust, Go, Python, Java, C/C++, JavaScript/TypeScript), config formats (OpenSSL, GnuTLS, Java KeyStore), and certificate formats (PEM, DER, PKCS12). Consider integration with CI/CD pipelines (GitHub Actions, GitLab CI). The architect brief should include: complete CLI interface design, scanner architecture, detection heuristics for each language, output schema, and a phased implementation plan.",
  "completed_at": "2026-02-26T17:31:25.099859317+00:00",
  "report": {
    "mission_id": "b2bac6c6-86cd-4d76-babc-ab3599c43c53",
    "title": "Investigation Report: Build an open-source Rust CLI tool called 'pq-discovery' that scans a directory ",
    "executive_summary": "Building 'pq-discovery' requires a two-phase scanning architecture (regex filter + tree-sitter/native AST parsing) to achieve sub-60-second scan times for 10K-file repositories while maintaining detection accuracy across Rust, Go, Python, C/C++, Solidity, and configuration formats. The tool must implement precise NIST PQC mappings (RSA-2048 → ML-KEM-768 + ML-DSA-65 for CNSA 2.0 baseline, with context-aware recommendations for blockchain vs. TLS/PKI), SARIF 2.1.0 output for CI/CD integration, and incremental Git-based scanning to reduce typical CI scan time by 90-99%. Critical architectural decisions: use language-native parsers (syn for Rust, go/parser for Go, libclang for C/C++) over tree-sitter for deep analysis; implement hybrid tokio+rayon parallelization to saturate NVMe I/O and multi-core CPUs; maintain a three-tier OID database (classical vulnerable, PQC standard, PQC experimental) with auto-update capability; and stratify risk by deployment context—blockchain EOAs and governance contracts are CRITICAL (mempool exposure + irreversible transfer), TLS/PKI is HIGH, test code is LOW.",
    "detailed_findings": [
      {
        "area": "Side-Channel Vulnerabilities in ML-KEM and ML-DSA Implementations",
        "analysis": "Static analysis tools can detect certain side-channel vulnerability patterns in source code, though full side-channel resistance requires dynamic testing (TVLA, power analysis). Key vulnerabilities the tool should warn about: (1) Non-constant-time NTT: table lookups indexed by secret data (twiddle factor tables, Barrett reduction tables), data-dependent branches in butterfly operations, variable-time modular arithmetic. Detection heuristic: scan for array indexing with non-constant expressions inside NTT functions, conditional branches inside tight loops over secret polynomials. (2) Variable-time rejection sampling: ML-DSA signing has a rejection loop that iterates until a valid signature is found (~4-7 iterations average). If the loop body has observable timing (e.g., early return on accept), the number of iterations leaks key information. Detection: identify rejection loops (while/for with probabilistic exit condition) and check if the loop body has early returns or timing-observable operations. (3) Non-constant-time FO transform: ML-KEM decapsulation re-encrypts the decrypted message and compares the result to the original ciphertext. If the comparison is not constant-time (e.g., memcmp instead of constant-time compare), it creates a timing oracle for chosen-ciphertext attacks. Detection: scan for memcmp/strcmp on ciphertext buffers, flag if not wrapped in constant-time comparison. (4) Unmasked implementations: masking (Boolean or arithmetic masking) is the primary countermeasure against power analysis. Detection: check for masking library usage (e.g., masked_poly_add, masked_ntt functions), flag implementations without masking as 'vulnerable to DPA/CPA, requires first-order masking for FIPS 140-3 Level 3+.' (5) Floating-point arithmetic in FN-DSA: FALCON's Gaussian sampling uses floating-point arithmetic, which is notoriously difficult to implement in constant-time (rounding modes, denormals, exception flags leak information). Detection: flag floating-point operations in FN-DSA signing code with 'Floating-point Gaussian sampling is side-channel sensitive, use fixed-point or integer-only implementations for high-assurance contexts.' The tool cannot replace full side-channel evaluation (TVLA, CPA, fault injection testing), but it can flag obvious vulnerabilities and guide developers toward constant-time best practices.",
        "risk_level": "critical",
        "action_items": [
          "Implement pattern-based static analysis for constant-time violations: detect array indexing with secret-dependent indices, data-dependent branches in crypto loops, non-constant-time comparison functions (memcmp, strcmp) on secret data. Output: 'Potential timing leak: array index depends on secret polynomial coefficient at line X. Use constant-time table lookup or eliminate table.'",
          "Detect absence of masking: scan for known masking library imports (e.g., #include \"masked_poly.h\", use of masked_* function prefixes). If not found in ML-KEM/ML-DSA implementations, warn: 'No masking detected. Implementation is vulnerable to power analysis (DPA/CPA). First-order masking required for FIPS 140-3 Level 3+, second-order for Level 4.'",
          "Flag floating-point usage in FN-DSA: 'Floating-point arithmetic detected in Gaussian sampling (line X). Floating-point operations are side-channel sensitive (rounding, denormals, exceptions leak information). Consider fixed-point or integer-only Gaussian sampling for constant-time execution.'",
          "Provide a '--side-channel-check' mode that performs deeper analysis: control-flow graph analysis to detect secret-dependent branches, data-flow analysis to track secret data propagation, integration with tools like ctgrind or dudect for automated constant-time verification",
          "Include references to constant-time coding guidelines in warnings: 'See NIST IR 8467 (Guidelines for Constant-Time Implementations) and pqcrystals reference implementations for constant-time NTT examples.'"
        ],
        "source_cluster": "pqsecurity",
        "rationale": "Side-channel vulnerabilities allow full secret key recovery from physical access or co-located attackers (cloud VMs, shared hardware). A mathematically secure PQC algorithm becomes completely insecure if the implementation leaks the key through timing, power, or faults. The risk is critical because: (1) side-channel attacks are practical (demonstrated with <10,000 traces on unprotected implementations), (2) many developers are unaware of side-channel risks (constant-time coding is not taught in most CS curricula), (3) compiler optimizations can introduce timing channels even in carefully written code. The tool's warnings can prevent deployment of vulnerable implementations before they reach production.",
        "cross_domain_insight": "The compliance domain reveals that FIPS 140-3 Level 3+ physical security evaluation includes side-channel resistance testing (TVLA, CPA, fault injection). The tool's static analysis warnings prepare developers for certification: 'This implementation will likely fail FIPS 140-3 Level 3 side-channel evaluation due to non-constant-time NTT. Remediate before submitting for CMVP validation.' The algorithm domain shows that the NTT is the performance bottleneck (70-80% of ML-KEM/ML-DSA runtime) — developers optimize NTT aggressively, often introducing timing channels. The tool must balance performance and security: 'Constant-time NTT is 10-30% slower than variable-time, but required for side-channel resistance.'"
      },
      {
        "area": "secp256k1 and Ed25519 risk stratification in blockchain vs. TLS/PKI",
        "analysis": "Blockchain usage of secp256k1 (Bitcoin/Ethereum EOAs) and Ed25519 (Solana/Polkadot transactions) has fundamentally different risk profiles than TLS/PKI. In TLS, a quantum attacker must break the ephemeral session key during the connection window (seconds to minutes). In blockchain, signatures are permanently recorded on-chain, and the mempool exposure window (12 seconds for Ethereum) creates a 'quantum front-running' attack: extract public key from pending transaction signature, derive private key via Shor's algorithm, submit competing transaction with higher gas to drain the account. Bitcoin's P2PKH addresses partially mitigate this (public key is only revealed when spending), but Ethereum's recoverable signatures (ecrecover) expose the public key in every transaction. Ed25519 on Solana/Polkadot has identical vulnerability with faster block times (400ms on Solana), shrinking the attack window but not eliminating it. Additionally, blockchain signatures secure irreversible value transfer—a forged TLS session can be detected and terminated, but a forged blockchain transaction is final after confirmation. The tool must flag blockchain signature usage as CRITICAL risk and TLS/PKI usage as HIGH risk, with rationale explaining the permanence and mempool exposure differences.",
        "risk_level": "critical",
        "action_items": [
          "Implement blockchain-specific detection patterns: flag secp256k1 in transaction signing contexts (web3.eth.accounts.signTransaction, ethers.Wallet.signTransaction, bitcoin-core RPC signrawtransaction) as CRITICAL with note 'mempool exposure + irreversible transfer'",
          "Flag Ed25519 in Solana/Polkadot contexts (@solana/web3.js Keypair.sign, polkadot-js signTransaction) as CRITICAL with note 'sub-second block time increases quantum attack urgency'",
          "Distinguish EOA signatures (externally owned accounts—no upgrade path) from smart contract signatures (upgradeable via ERC-4337 account abstraction). EOAs are CRITICAL, smart contracts are HIGH with mitigation note",
          "Provide blockchain-specific recommendations: for EOAs, recommend migration to ERC-4337 smart accounts with ML-DSA-65 validation; for consensus layer (validator signing), recommend hybrid BLS+ML-DSA during transition; for light clients, flag BLS12-381 sync committee signatures as CRITICAL (quantum breaks light client security model)"
        ],
        "source_cluster": "blockchain",
        "rationale": "Blockchain signatures have three compounding vulnerabilities absent in TLS: (1) permanent on-chain exposure of public keys, (2) mempool visibility enabling quantum front-running attacks within block time windows, (3) irreversible value transfer with no post-compromise remediation. The 12-second Ethereum block time sets the quantum attack threshold—any CRQC capable of running Shor's algorithm on secp256k1 in <12 seconds can steal funds from pending transactions.",
        "cross_domain_insight": "The pqsecurity cluster's hybrid construction guidance (X25519+ML-KEM-768 for TLS) does not directly apply to blockchain EOAs because there is no 'session'—each transaction is a discrete signature event. However, the hybrid approach IS applicable to blockchain P2P networking (libp2p Noise handshakes, devp2p RLPx) and consensus-layer BLS aggregation. The tool should recommend pure PQ (ML-DSA) for transaction signing and hybrid PQ/classical for networking/consensus during transition."
      },
      {
        "area": "Governance and multisig quantum vulnerability stratification",
        "analysis": "On-chain governance and multisig contracts have asymmetric quantum risk compared to EOA signatures. A compromised EOA can only steal that account's funds. A compromised governance key or multisig signer can: (1) upgrade protocol contracts to drain all user funds, (2) modify protocol parameters (fee structures, collateral ratios, oracle sources), (3) mint unbacked tokens, (4) pause or brick the protocol. Ethereum's major DeFi protocols use multisig governance: Uniswap (7-of-9 Gnosis Safe), Aave (6-of-10 multisig for short timelock executor, 11-of-16 for long timelock), MakerDAO (governance token voting with 48-hour timelock). All of these rely on ECDSA signatures. A quantum attacker who can forge 7 Uniswap multisig signatures can upgrade the Uniswap V3 factory contract to steal all liquidity (~$4B TVL). The timelock provides partial defense: the upgrade transaction is visible on-chain during the delay period, allowing social-layer intervention (community coordination to reject the upgrade via a counter-fork). However, this assumes: (1) the community detects the attack, (2) the community can coordinate a response faster than the timelock expires, (3) the attacker doesn't compromise enough keys to bypass the timelock. The tool should flag governance and multisig signatures as CRITICAL risk, higher than EOA signatures, with rationale explaining the systemic impact.",
        "risk_level": "critical",
        "action_items": [
          "Implement governance contract detection: scan for OpenZeppelin's Governor contracts, Gnosis Safe multisig patterns, Compound's Timelock, and custom governance implementations. Flag any contract with 'admin', 'owner', or 'governance' roles that use ecrecover for signature validation",
          "Risk stratification: (1) Immutable governance (no timelock) → CRITICAL, (2) Short timelock (<7 days) → CRITICAL, (3) Long timelock (≥7 days) → HIGH with note 'timelock provides detection window but not cryptographic defense', (4) Governance with quantum-safe signatures → LOW",
          "Recommend minimum 7-day timelock for all governance actions during the quantum transition period (CRQC-5 years → CRQC). This provides a realistic window for community detection and social-layer response. Post-CRQC, timelocks are insufficient—governance must use post-quantum signatures",
          "For multisig wallets, recommend migration to threshold ML-DSA: small committees (≤32 signers) can use threshold ML-DSA directly; large committees (>32 signers) should use STARK-aggregated ML-DSA (each signer produces individual ML-DSA signature, aggregator generates STARK proof of t-of-n validity)",
          "Provide migration priority guidance: governance contracts should migrate BEFORE general EOAs because the systemic risk is higher. Recommend phased migration: (1) upgrade governance contracts to support post-quantum signature verification, (2) governance signers generate ML-DSA key pairs, (3) execute a governance vote to rotate from ECDSA to ML-DSA keys, (4) general users migrate EOAs to ERC-4337 smart accounts"
        ],
        "source_cluster": "blockchain",
        "rationale": "Governance and multisig contracts control protocol-level authority. A single quantum attack on a multisig can compromise an entire DeFi protocol with billions in TVL. Unlike EOA theft (which affects one user), governance compromise affects all users. The timelock mechanism provides only probabilistic defense—it buys time for social coordination but does not cryptographically prevent the attack. The 48-hour Aave timelock is insufficient if the quantum attack is not detected immediately. This is a critical systemic risk.",
        "cross_domain_insight": "The security cluster's CNSA 2.0 compliance framework is directly applicable here. NSA's 2025 software signing deadline and 2030 authentication deadline apply to governance contracts—these are 'software signing' in the sense that they authorize protocol upgrades. The tool should generate CNSA 2.0 compliance reports for governance contracts, flagging those that will miss the 2025 deadline."
      },
      {
        "area": "Light client and bridge quantum vulnerabilities",
        "analysis": "Light clients (Ethereum sync committee, Bitcoin SPV) and cross-chain bridges are quantum-vulnerable through their reliance on aggregate signatures and Merkle proofs. Ethereum's light client protocol uses a sync committee (512 validators) whose BLS12-381 aggregate signature attests to block headers. A quantum attacker can forge sync committee signatures, feeding light clients fraudulent headers. This breaks: (1) mobile/embedded wallets using light sync, (2) cross-chain bridges that verify Ethereum state via light client proofs (e.g., Near's Rainbow Bridge, Cosmos IBC light clients), (3) L2 rollups that use light clients for L1 data availability verification. Bitcoin's SPV clients verify Merkle proofs of transaction inclusion but do not verify signatures (they trust PoW). However, a quantum attacker with sufficient hashrate (via Grover's algorithm providing sqrt speedup) could perform 51% attacks. Cross-chain bridges are the highest-value quantum targets: Wormhole, LayerZero, Axelar, and other bridges hold billions in locked assets. Most bridges use multisig validation (e.g., Wormhole's 13-of-19 guardian set signs cross-chain messages). A quantum attacker who forges guardian signatures can mint unbacked wrapped tokens on the destination chain, draining the bridge. The tool must detect light client signature verification code and bridge validator signature schemes, flagging them as CRITICAL.",
        "risk_level": "critical",
        "action_items": [
          "Detect Ethereum light client implementations: scan for sync committee signature verification (BLS12-381 aggregate signature verification), beacon chain API usage (eth/v1/beacon/light_client/*), and Altair light client protocol code. Flag as CRITICAL with note 'BLS12-381 aggregate signatures are quantum-vulnerable—light client security breaks under quantum attack'",
          "Detect cross-chain bridge contracts: scan for signature verification of external chain state (e.g., Wormhole's verifyVM, LayerZero's validateProof, IBC light client verification). Flag as CRITICAL with note 'bridge guardian/validator signatures are quantum-vulnerable—forged signatures enable unbacked token minting'",
          "Recommend light client migration path: (1) Phase 1: hybrid BLS+ML-DSA sync committee signatures (from consensus SME analysis), (2) Phase 2: STARK-compressed ML-DSA signatures (200 KB proof for 512 validators), (3) Phase 3: pure post-quantum light client protocol. Note that proof size increase (48 bytes → 200 KB) may make light clients impractical for mobile/embedded—recommend investigating ZK-SNARK-based recursive proof compression",
          "Recommend bridge migration path: (1) upgrade bridge contracts to support ML-DSA signature verification, (2) guardian/validator set generates ML-DSA key pairs, (3) execute threshold signature ceremony to rotate keys, (4) implement STARK-based signature aggregation if guardian set is large (>32 signers). Prioritize bridges with highest TVL first",
          "Provide detection patterns for bridge-specific signature schemes: Wormhole's guardian signatures (ECDSA over secp256k1), LayerZero's oracle/relayer signatures, Axelar's validator set signatures, IBC light client signatures (Tendermint Ed25519). Each has slightly different verification logic—tool must parse contract bytecode or source to detect these patterns"
        ],
        "source_cluster": "blockchain",
        "rationale": "Light clients and bridges are systemic chokepoints. A compromised light client can be fed arbitrary state, allowing theft of any assets controlled by contracts relying on that light client. A compromised bridge can mint unbacked tokens, creating infinite money that drains all liquidity on the destination chain. The 2022 Wormhole hack ($320M) and Ronin bridge hack ($625M) demonstrate the catastrophic impact of bridge compromise—and those were via software vulnerabilities, not cryptographic breaks. A quantum attack on bridge signatures would be undetectable until after the theft.",
        "cross_domain_insight": "The L2 scaling SME's analysis of STARK-based rollups (StarkNet) is relevant here. StarkNet's STARK proofs are quantum-resistant by construction (hash-based, no elliptic curves). However, StarkNet's bridge contracts on Ethereum L1 still use ECDSA for governance, so the bridge is quantum-vulnerable even though the rollup proofs are not. This illustrates a key principle: quantum resistance must be end-to-end. A quantum-safe proof system is useless if the bridge verifying it can be compromised."
      },
      {
        "area": "FIPS 203/204/205/206 Parameter Set Mappings to Classical Algorithms",
        "analysis": "RSA-2048 and ECC P-256/secp256k1/Ed25519/X25519 map to NIST Security Level I (128-bit classical, ~107-bit quantum): recommend ML-KEM-512 for key exchange, ML-DSA-44 or FN-DSA-512 for signatures. However, ML-KEM-768 (Level III, 192-bit classical) is the de facto industry standard for new deployments due to larger security margin and alignment with CNSA 2.0 transition guidance. RSA-4096 and ECC P-384 map to Level III: recommend ML-KEM-768 and ML-DSA-65. For conservative/long-term applications, recommend SLH-DSA-SHA2-128s (Level I) or SLH-DSA-SHA2-192s (Level III) as hash-based fallback immune to lattice breaks. The tool should flag that secp256k1 (Bitcoin/Ethereum) has no direct NIST level equivalent but is cryptographically similar to P-256 (both ~128-bit). Key sizes: ML-KEM-768 public key = 1,184 bytes (vs X25519 32 bytes), ML-DSA-65 signature = 3,293 bytes (vs Ed25519 64 bytes), FN-DSA-512 signature = ~666 bytes (compact alternative). The tool's detection heuristics must recognize not just algorithm names but also key/signature size patterns in binary formats (DER-encoded keys, raw key material in config files).",
        "risk_level": "high",
        "action_items": [
          "Implement a risk-based recommendation engine: for detected RSA-2048/ECC-256, default to ML-KEM-768 (not 512) with a note that Level I (ML-KEM-512) is acceptable for non-HNDL data with <10 year confidentiality requirements",
          "For blockchain contexts (secp256k1 detection), explicitly recommend FN-DSA-512 over ML-DSA-65 due to 5x smaller signatures (critical for on-chain transaction costs) with a warning that FN-DSA has higher implementation complexity",
          "Include a '--conservative' flag that always recommends SLH-DSA for signatures (hash-based, immune to lattice breaks) at the cost of 10-50x larger signatures",
          "Provide security level explanations in the JSON output: 'RSA-2048 provides ~112-bit classical security, ~0-bit quantum security. ML-KEM-768 provides ~183-bit classical, ~166-bit quantum security (NIST Level III).'"
        ],
        "source_cluster": "pqsecurity",
        "rationale": "Incorrect mappings lead to under-protection (recommending Level I when Level III is needed for regulatory compliance) or over-protection (recommending Level V for all cases, causing unnecessary performance overhead). The 'harvest now, decrypt later' threat means data encrypted with RSA-2048 today is already at risk — the tool must convey urgency for HNDL-sensitive data (medical records, financial transactions, state secrets) while allowing pragmatic Level I choices for short-lived sessions.",
        "cross_domain_insight": "The compliance domain reveals that CNSA 2.0 mandates Level III+ for NSS by 2030, and FedRAMP/PCI DSS risk assessments (required by 2025) effectively force organizations to document their quantum migration plan. The tool should flag RSA-2048 as 'non-compliant with CNSA 2.0 Phase 3 (2030)' and RSA-4096 as 'acceptable until 2030, migrate to ML-KEM-768+ by 2033.' This compliance-driven urgency justifies defaulting to Level III recommendations even when Level I is technically sufficient."
      },
      {
        "area": "Hybrid Construction Best Practices and Tool Recommendations",
        "analysis": "Current best practice (2025-2030 transition period): hybrid constructions combining classical + PQ provide defense-in-depth. X25519+ML-KEM-768 (concatenation-then-KDF combiner) is the industry-deployed standard (Chrome, Cloudflare, AWS, Signal). The tool should recommend hybrid for new deployments and flag pure classical (X25519 alone, RSA alone) as 'quantum-vulnerable, migrate to hybrid.' However, the tool must distinguish context: for Internet-facing TLS endpoints, recommend 'X25519+ML-KEM-768 hybrid' (backward compatible, widely supported). For internal systems or new greenfield projects with no legacy interop requirements, recommend 'ML-KEM-768 pure PQ' (simpler, no hybrid overhead). The hybrid overhead is +2.3 KB per TLS handshake (acceptable for most applications, but significant for IoT/constrained devices). The tool should detect constrained contexts (embedded targets, IoT config files) and adjust recommendations: for ARM Cortex-M0/M0+ with <32 KB RAM, flag 'insufficient resources for ML-KEM-768, consider gateway-based PQ termination or device replacement.' The security guarantee of hybrid: IND-CCA2 secure if either component is secure (the 'OR' property). This means X25519+ML-KEM-768 is secure even if ML-KEM is completely broken (as long as X25519 remains secure against classical attacks) — a critical selling point for risk-averse organizations hesitant to trust new PQC algorithms.",
        "risk_level": "high",
        "action_items": [
          "Default recommendation for detected X25519/ECDH: 'Upgrade to X25519+ML-KEM-768 hybrid (backward compatible, quantum-resistant if either algorithm is secure). Pure ML-KEM-768 is acceptable for new systems with no legacy interop requirements.'",
          "For detected RSA key exchange (RSA-OAEP, RSA-KEM): 'Replace with X25519+ML-KEM-768 hybrid or pure ML-KEM-768. RSA key exchange is quantum-vulnerable and deprecated in TLS 1.3.'",
          "Include a '--pure-pq' flag that recommends pure PQ algorithms (ML-KEM-768, ML-DSA-65) without hybrid, for users who want the simplest post-quantum configuration and accept the backward compatibility trade-off",
          "Detect IoT/embedded contexts (ARM targets, <64 KB RAM indicators in build configs) and recommend 'Gateway-based PQ termination: deploy PQ-capable gateway/proxy, use AES-256-GCM + HMAC-SHA-256 for device-to-gateway communication' instead of on-device PQC"
        ],
        "source_cluster": "pqsecurity",
        "rationale": "Recommending pure classical (X25519 alone) in 2025+ is negligent given the HNDL threat and regulatory trajectory. But recommending pure PQ without hybrid may break backward compatibility with legacy clients (older browsers, embedded devices) and faces organizational resistance due to insufficient confidence in PQC algorithms' long-term security. The tool must thread this needle: hybrid as the default, pure PQ as an option for greenfield/high-assurance contexts, with clear explanations of the trade-offs.",
        "cross_domain_insight": "The migration domain reveals that hybrid is not just a technical best practice but an organizational change management strategy: it allows incremental deployment (enable hybrid on servers, clients negotiate down to classical if needed) and provides a safety net against PQC algorithm breaks. The compliance domain shows that ANSSI (France) explicitly mandates hybrid-only until 2030, while NIST/NSA allow pure PQ for new systems. The tool should surface this policy divergence: 'Note: ANSSI recommends hybrid-only until 2030. Pure PQ is acceptable under NIST/CNSA 2.0 guidance.'"
      },
      {
        "area": "C/C++ OpenSSL and mbedTLS API detection in macro-heavy codebases",
        "analysis": "C/C++ crypto detection targets OpenSSL EVP API and mbedTLS. OpenSSL: `EVP_PKEY_CTX_new_id(EVP_PKEY_RSA, NULL)` creates an RSA key context, `EVP_PKEY_CTX_new_id(EVP_PKEY_EC, NULL)` with subsequent `EVP_PKEY_CTX_set_ec_paramgen_curve_nid(ctx, NID_secp256k1)` selects the curve. Detection: parse C/C++ with libclang (Clang's C API for AST access) or tree-sitter (faster but less accurate). Match function calls to `EVP_PKEY_CTX_new_id`, extract first argument (algorithm ID constant), resolve constant via preprocessor (e.g., `EVP_PKEY_RSA` is a macro expanding to an integer). For curve selection, detect `EVP_PKEY_CTX_set_ec_paramgen_curve_nid` and extract NID constant (e.g., `NID_secp256k1`, `NID_X9_62_prime256v1` for P-256). mbedTLS: `mbedtls_pk_setup(ctx, mbedtls_pk_info_from_type(MBEDTLS_PK_RSA))` selects RSA, `mbedtls_ecp_group_load(grp, MBEDTLS_ECP_DP_SECP256K1)` selects secp256k1. Macro-heavy codebases present challenges: (1) algorithm IDs are often macros, requiring preprocessor expansion, (2) function calls may be wrapped in macros (e.g., `CRYPTO_CHECK(EVP_PKEY_CTX_new_id(...))`), requiring macro expansion to see the actual call, (3) conditional compilation (`#ifdef OPENSSL_NO_EC`) may hide code paths. libclang handles preprocessor expansion and provides a cursor-based API for traversing the AST after macro expansion. tree-sitter does NOT expand macros — it parses the source text as-is, so `EVP_PKEY_RSA` remains an identifier, not resolved to its integer value. For production use, libclang is recommended despite slower parsing because it provides accurate, fully-expanded ASTs.",
        "risk_level": "high",
        "action_items": [
          "Use libclang (Clang's C API) for C/C++ AST parsing — it provides full preprocessor expansion, accurate AST representation, and handles all C/C++ language features including templates and inline assembly",
          "Detect OpenSSL EVP API: match calls to `EVP_PKEY_CTX_new_id`, extract first argument, resolve macro to integer, map integer to algorithm name using OpenSSL's public header constants (EVP_PKEY_RSA=6, EVP_PKEY_EC=408, etc.)",
          "Detect EC curve selection: match `EVP_PKEY_CTX_set_ec_paramgen_curve_nid`, extract NID argument, map NID to curve name (NID_secp256k1=714, NID_X9_62_prime256v1=415 for P-256, NID_secp384r1=715 for P-384)",
          "Detect mbedTLS: match `mbedtls_pk_setup` with `mbedtls_pk_info_from_type`, extract type constant (MBEDTLS_PK_RSA, MBEDTLS_PK_ECDSA, MBEDTLS_PK_ECKEY); match `mbedtls_ecp_group_load`, extract curve ID (MBEDTLS_ECP_DP_SECP256K1, MBEDTLS_ECP_DP_SECP256R1, etc.)",
          "Handle macro-wrapped calls: libclang's cursor API provides `clang_getCursorReferenced()` to resolve macro expansions — use this to trace macro-wrapped function calls back to the actual function",
          "Handle conditional compilation: parse all code paths (both `#ifdef` branches) and report findings with context (e.g., 'RSA-2048 usage detected in OPENSSL_NO_EC=0 build configuration')",
          "Flag runtime algorithm selection: detect `EVP_PKEY_assign`, `EVP_PKEY_set_type`, or function pointer assignments where the algorithm is determined by a variable — report these as 'requires manual review' with high priority",
          "For projects using CMake: parse CMakeLists.txt to extract build flags (e.g., `-DOPENSSL_NO_EC`) and include these in the report context",
          "Provide a fallback tree-sitter parser for initial fast scanning, with a flag to re-scan with libclang for high-confidence results"
        ],
        "source_cluster": "languages",
        "rationale": "C/C++ codebases are the most challenging for static analysis due to: (1) preprocessor macros that obscure algorithm selection, (2) runtime dispatch via function pointers (e.g., `EVP_PKEY_assign` can assign any algorithm to a key context), (3) complex build systems where algorithm selection may depend on build flags (e.g., `#ifdef USE_ECDSA`), (4) inline assembly or compiler intrinsics that may implement crypto operations outside library calls. The risk is high because: (1) missing detections are likely in macro-heavy code if the parser does not expand macros, (2) false negatives are dangerous (quantum-vulnerable code goes undetected), (3) C/C++ is the language where production crypto implementations live (per lang_cpp SME), so accuracy is critical. The main mitigation is using libclang for full preprocessor expansion and providing a 'low-confidence' flag for findings where the algorithm is determined by a runtime variable or function pointer.",
        "cross_domain_insight": "C/C++ is the FFI boundary for all other languages (per lang_cpp SME). Rust, Go, and Python PQC libraries often wrap C implementations (liboqs, PQClean). This means a single C/C++ vulnerability or algorithm choice propagates to all languages that wrap it. The tool should detect C/C++ crypto usage not just in C/C++ projects but also in the vendored C dependencies of Rust/Go/Python projects. Check for liboqs, PQClean, Botan, Crypto++ in third-party/ or vendor/ directories and scan those separately."
      },
      {
        "area": "Solidity and EVM bytecode scanning for quantum-vulnerable primitives",
        "analysis": "Solidity smart contracts use ECDSA (secp256k1) for transaction signatures and may use ecrecover precompile (address 0x01) for on-chain signature verification. Detection in Solidity source: parse with solc --ast-json (Solidity compiler's AST output) or use tree-sitter-solidity, match calls to `ecrecover(hash, v, r, s)`, match OpenZeppelin's `ECDSA.recover()` utility, detect `address.call()` to precompile address 0x01. For deployed contracts (EVM bytecode): disassemble with `evmole` or `heimdall`, detect `STATICCALL` to address 0x01 (ecrecover precompile), detect `CALL` to address 0x02 (SHA-256 precompile, quantum-resistant but may be part of a larger quantum-vulnerable construction). Solidity's limited crypto capabilities mean most quantum-vulnerable operations are: (1) implicit (transaction signature verification by the EVM, not visible in contract code), (2) via precompiles (ecrecover), (3) in off-chain components (wallet software signing transactions). The tool should scan Solidity for ecrecover usage (indicates the contract verifies signatures on-chain and will break when ECDSA is quantum-broken) and flag contracts using ecrecover for migration to PQC signature verification (likely via a new precompile, per lang_solidity SME's analysis).",
        "risk_level": "high",
        "action_items": [
          "Parse Solidity source with `solc --ast-json` (Solidity compiler's JSON AST output) — this is the canonical AST representation, more reliable than tree-sitter-solidity",
          "Detect ecrecover usage: match AST nodes of type `FunctionCall` with `expression.name == 'ecrecover'` or `expression.memberName == 'recover'` (for OpenZeppelin's ECDSA library)",
          "Detect OpenZeppelin ECDSA library: match import statements for `@openzeppelin/contracts/utils/cryptography/ECDSA.sol`, flag any contract importing this as using ECDSA signature verification",
          "For deployed contracts: disassemble bytecode with `evmole` or `heimdall`, detect `STATICCALL` opcode with address 0x01 (ecrecover precompile) — this indicates on-chain signature verification",
          "Flag ecrecover usage as 'critical' risk with rationale: 'This contract verifies ECDSA signatures on-chain. ECDSA is quantum-vulnerable. When quantum computers break ECDSA, attackers can forge signatures and bypass this contract's access control.'",
          "Recommend migration path: 'Replace ecrecover with a PQC signature verification precompile (ML-DSA-65 recommended). If the contract is not upgradeable, deploy a new version with PQC verification and migrate state. Consider a hybrid ECDSA+ML-DSA scheme for backward compatibility during transition.'",
          "Detect EIP-712 signature verification: match `keccak256(abi.encode(...))` followed by ecrecover — this is the EIP-712 typed data signing pattern, widely used for meta-transactions and off-chain approvals",
          "Scan for proxy patterns (OpenZeppelin's TransparentUpgradeableProxy, UUPS): detect `delegatecall` to implementation contracts, flag these as 'upgradeable' (can migrate to PQC) vs. non-upgradeable contracts (require redeployment)"
        ],
        "source_cluster": "languages",
        "rationale": "Solidity contracts using ecrecover for on-chain signature verification are critically vulnerable to quantum attacks — an attacker with a quantum computer can forge signatures and bypass access control. The risk is high because: (1) ecrecover is widely used (OpenZeppelin's ECDSA library, EIP-712 signature verification, meta-transaction patterns), (2) deployed contracts are immutable (cannot be patched without a proxy upgrade), (3) the migration path is unclear (no PQC precompile exists on Ethereum mainnet yet, per lang_solidity SME). The tool must flag ecrecover usage as 'critical' risk and recommend: (1) immediate assessment of whether the contract can be upgraded (proxy pattern), (2) planning for PQC precompile integration when available, (3) considering hybrid signature schemes (ECDSA + ML-DSA) for backward compatibility during transition.",
        "cross_domain_insight": "Solidity's reliance on precompiles for crypto (per lang_solidity SME) means the migration path depends on EVM client upgrades (Geth, Nethermind, Besu adding PQC precompiles). This is a cross-cluster dependency: the languages cluster can detect vulnerable contracts, but the ethereum cluster must coordinate the EIP process for PQC precompiles, and the systems cluster must ensure node operators upgrade. The tool should output findings in a format that supports this coordination — e.g., a 'contracts requiring PQC precompile' report that can be shared with the Ethereum Foundation."
      },
      {
        "area": "CNSA 2.0 Compliance Requirements and Context Differentiation",
        "analysis": "CNSA 2.0 (NSA Commercial National Security Algorithm Suite 2.0) mandates ML-KEM-1024 for key exchange and ML-DSA-87 for signatures in National Security Systems (NSS), with aggressive timelines: software/firmware signing preferred by 2025, required exclusively by 2030; networking equipment preferred by 2026, required by 2030. General commercial use follows NIST's broader guidance: ML-KEM-768 and ML-DSA-65 are sufficient for most applications (NIST Security Level III, equivalent to AES-192). The tool MUST distinguish between NSA/DoD contexts and commercial use because: (1) CNSA 2.0 requires Level V parameters (ML-KEM-1024, ML-DSA-87) which have larger keys and slower performance than Level III, imposing unnecessary overhead on commercial systems; (2) NSA/DoD procurement requires CNSA 2.0 compliance for contract eligibility, making it a binary pass/fail criterion; (3) commercial systems may choose hybrid constructions (X25519+ML-KEM-768) during transition, which CNSA 2.0 does not explicitly endorse for NSS. Detection heuristics: scan for NSA/DoD-specific markers (e.g., comments referencing 'CNSA', 'NSS', 'FIPS 140-3', 'DoD 8500', contract numbers, classification markings in headers), file paths indicating government projects (e.g., '/dod/', '/nss/', '/classified/'), or configuration files with CNSA-specific settings. If detected, flag findings as 'CNSA 2.0 non-compliant' and recommend Level V parameters. Otherwise, recommend Level III as the default with a note that Level V is available for high-security contexts.",
        "risk_level": "high",
        "action_items": [
          "Implement context detection: scan for NSA/DoD markers (keywords, file paths, config settings). If detected, set a 'cnsa_mode' flag and apply Level V recommendations (ML-KEM-1024, ML-DSA-87).",
          "Provide a CLI flag '--cnsa-mode' to force CNSA 2.0 compliance checking, overriding heuristic detection. Document this flag prominently for government contractors.",
          "In JSON output, include a 'compliance_context' field: 'cnsa_2.0' or 'commercial'. For CNSA contexts, include 'cnsa_deadline' (e.g., '2025-12-31 for software signing') and 'cnsa_requirement' ('ML-DSA-87 required').",
          "Generate a CNSA 2.0 compliance summary report: list all findings, categorize by CNSA timeline (2025/2026/2030 deadlines), and provide a pass/fail assessment per category.",
          "Cross-reference with NIST SP 800-227 migration timelines for commercial contexts: flag findings that will become non-compliant by 2030 (RSA-2048) vs. 2035 (all classical public key crypto)."
        ],
        "source_cluster": "security",
        "rationale": "Failure to distinguish CNSA 2.0 contexts creates two failure modes: (1) under-recommendation — flagging RSA-2048 in a DoD project but recommending ML-DSA-65 when ML-DSA-87 is required, causing compliance failure and contract risk; (2) over-recommendation — recommending ML-DSA-87 for a commercial web app, imposing 30-40% performance overhead and larger signatures with no security benefit. Both modes undermine the tool's credibility. The 2025 deadline for software signing is imminent, making this a high-priority feature.",
        "cross_domain_insight": "The governance SME's detailed CNSA 2.0 timeline knowledge (2025 software signing deadline, 2030 exclusive requirement) combined with the pqsecurity SME's parameter set mappings (Level III vs. Level V) enables precise, context-aware recommendations. The keymgmt SME's HSM constraints (larger keys reduce storage capacity) inform the tradeoff analysis: CNSA 2.0's Level V mandate may require HSM upgrades in DoD environments."
      },
      {
        "area": "Tree-sitter vs. Regex Performance for Large Monorepos",
        "analysis": "Tree-sitter parsing provides structural AST analysis but carries 10-100x overhead vs. regex for simple pattern matching. For 1M+ LOC monorepos, full tree-sitter parsing of all files would take 5-30 minutes on a 16-core workstation (assuming ~50-500 files/second parse rate depending on language complexity and file size). Regex scanning achieves 5,000-50,000 files/second for simple patterns but misses context (cannot distinguish function calls from comments, test fixtures from production code). The optimal architecture is a two-phase pipeline: Phase 1 uses fast regex/string matching to filter files importing crypto libraries (reducing the candidate set by 90-99%), Phase 2 applies tree-sitter parsing only to filtered files for precise AST-level detection. This hybrid approach achieves <60 second scan times for 10K file repos on modern hardware, meeting CI/CD requirements where 2-5 minute scan budgets are typical.",
        "risk_level": "high",
        "action_items": [
          "Implement two-phase scanning: Phase 1 (fast filter) uses ripgrep or custom regex scanner to identify files containing crypto-related imports/keywords (e.g., 'use ring::', 'import cryptography', 'OpenSSL_', 'secp256k1'). Target: 10,000+ files/second on NVMe SSD.",
          "Phase 2 (deep parse) uses tree-sitter on filtered files only. Pre-compile tree-sitter grammars and cache parsed ASTs in ~/.cache/pq-discovery/ keyed by file hash to avoid re-parsing unchanged files across runs.",
          "Benchmark target: 10K file monorepo in <60 seconds on 16-core CPU with NVMe storage, 100K file monorepo in <10 minutes. Profile with perf stat to identify bottlenecks (CPU-bound vs. I/O-bound).",
          "Provide --fast-mode flag that skips tree-sitter parsing entirely for time-critical CI checks, falling back to regex-only detection with explicit warnings about reduced accuracy."
        ],
        "source_cluster": "systems",
        "rationale": "CI/CD integration requires scan times under 2-5 minutes to avoid blocking developer workflows. Pure tree-sitter parsing would exceed this budget for large repos, causing developers to disable the tool. Pure regex produces false positives (flagging test vectors, commented-out code) and false negatives (missing obfuscated crypto usage), undermining trust. The two-phase architecture is essential to meet both performance and accuracy requirements.",
        "cross_domain_insight": "The pqsecurity cluster identified the need for language-specific scanners and the languages cluster detailed tree-sitter integration patterns. This finding bridges both: tree-sitter provides the language-specific AST precision pqsecurity requires, but only performance engineering makes it viable at monorepo scale. The two-phase architecture is the systems-level solution to a cross-cluster constraint."
      },
      {
        "area": "ASN.1/DER Certificate and Key Parsing",
        "analysis": "RSA, ECDSA, and EdDSA keys in X.509 certificates and PKCS#8 private keys are encoded using ASN.1 DER with algorithm-specific OIDs in the AlgorithmIdentifier field. For SubjectPublicKeyInfo: the algorithm field contains an OID (e.g., 1.2.840.113549.1.1.1 for RSA, 1.2.840.10045.2.1 for EC public key with curve OID as parameter, 1.3.101.112 for Ed25519). Key size extraction requires: (1) For RSA: parse the RSAPublicKey sequence and extract the modulus bit length. (2) For ECDSA: the curve OID (e.g., 1.2.840.10045.3.1.7 for P-256, 1.3.132.0.10 for secp256k1) determines key size. (3) For EdDSA: the algorithm OID directly specifies the key size (Ed25519 = 256-bit, Ed448 = 448-bit). PKCS#8 PrivateKeyInfo uses the same AlgorithmIdentifier structure. The scanner must implement a DER parser (use existing libraries like der-parser or x509-parser in Rust) and maintain an OID-to-algorithm mapping database covering classical algorithms (RSA-2048/4096, P-256/384, secp256k1, Ed25519, X25519) and PQC algorithms (ML-KEM OIDs from draft-ietf-lamps-kyber-certificates, ML-DSA OIDs from draft-ietf-lamps-dilithium-certificates, SLH-DSA OIDs from draft-ietf-lamps-sphincs-plus-certificates). For PEM-encoded certificates, strip the Base64 armor and parse the DER payload. For DER files, parse directly.",
        "risk_level": "high",
        "action_items": [
          "Implement DER parser using der-parser or x509-parser crate (both are mature, well-audited Rust libraries). Avoid custom DER parsing—ASN.1 has subtle encoding rules that are easy to mishandle.",
          "Build a three-tier OID database: (1) Classical vulnerable (RSA, ECDSA, ECDH with all curve OIDs), (2) PQC standard (ML-KEM/ML-DSA/SLH-DSA with FIPS 203/204/205 OIDs and IETF draft OIDs), (3) PQC experimental (Kyber/Dilithium pre-standard OIDs, FrodoKEM, BIKE, HQC). Flag tier-3 OIDs for migration to tier-2.",
          "For RSA key size extraction: parse the RSAPublicKey SEQUENCE, extract the modulus INTEGER, compute bit length. Flag RSA-1024 as critical, RSA-2048 as high risk (quantum-vulnerable by 2030), RSA-4096 as medium risk (vulnerable but with longer timeline).",
          "For ECDSA/ECDH: map curve OIDs to security levels. P-256/secp256k1 = ~128-bit classical security (broken by Grover + Shor). P-384 = ~192-bit classical (still broken by Shor). Recommend ML-KEM-768 (NIST Level 3) as default replacement for P-256/secp256k1, ML-KEM-1024 (Level 5) for P-384.",
          "For EdDSA: Ed25519 is quantum-vulnerable (Shor breaks the underlying Curve25519 DLP). Recommend ML-DSA-65 as replacement (comparable signature size ~3.3 KB vs. Ed25519's 64 bytes—this is the hard trade-off). X25519 (ECDH) should recommend ML-KEM-768.",
          "Implement SubjectPublicKeyInfo and PKCS#8 PrivateKeyInfo parsers as separate scanner modules. Private keys in PKCS#8 may be encrypted (EncryptedPrivateKeyInfo)—detect the encryption algorithm OID and flag if it uses AES-128 (quantum-vulnerable under Grover, recommend AES-256) or 3DES (deprecated, recommend AES-256-GCM)."
        ],
        "source_cluster": "crypto",
        "rationale": "Certificate scanning is the highest-value detection surface—certificates are long-lived, widely deployed, and their algorithm choices directly determine quantum vulnerability. Incorrect OID parsing or key size extraction produces false negatives (missing quantum-vulnerable keys) or false positives (flagging quantum-safe keys). The OID namespace is large and evolving (PQC OIDs are still in draft), requiring a maintainable database with version tracking.",
        "cross_domain_insight": "The pqsecurity cluster's OID database and FIPS 203/204/205 mapping directly informs this implementation. The security cluster's encrypted PKCS#8 handling guidance (flag for manual review, provide quantum-resistant key wrapping recommendations) applies here. The systems cluster's performance requirements (sub-60s scan time for 10K files) constrain the DER parser choice—use zero-copy parsing (der-parser supports this) to avoid allocation overhead."
      },
      {
        "area": "Smart contract cryptographic primitive exposure and detection",
        "analysis": "Smart contract languages expose cryptographic primitives through three mechanisms: (1) EVM precompiles at fixed addresses (ecrecover at 0x01, BN254 pairing at 0x08, KZG point evaluation at 0x0A), (2) Solidity's cryptographic builtins (keccak256, sha256, ripemd160, ecrecover), (3) WASM host functions in Substrate/CosmWasm/Near. The EVM's ecrecover precompile is the most pervasive quantum vulnerability—it's used not just for transaction validation but directly in contracts for EIP-712 permit signatures, ERC-4337 UserOperation validation, multisig wallets (Gnosis Safe), governance voting (Snapshot), and oracle authentication (Chainlink). Every contract calling ecrecover or validating ECDSA is quantum-vulnerable. Solana's SVM uses Ed25519 via native program syscalls (sol_secp256k1_recover, sol_ed25519_verify). Move VM (Aptos/Sui) uses Ed25519 for transaction signing but does not expose signature verification as a contract-callable primitive (signatures are validated by the VM before contract execution). The tool must parse Solidity AST for ecrecover calls, scan for precompile addresses (0x01, 0x06-0x08 for BN254), and detect Rust smart contracts importing solana-program's secp256k1_recover or ed25519_verify.",
        "risk_level": "high",
        "action_items": [
          "Implement Solidity AST scanner (using solc --ast-compact-json or tree-sitter-solidity) to detect: (1) ecrecover function calls, (2) low-level calls to address 0x0000...01 (ecrecover precompile), (3) OpenZeppelin's ECDSA.recover library usage, (4) EIP-712 typed data signature validation patterns",
          "Detect BN254 pairing precompile usage (addresses 0x06-0x08) in ZK-SNARK verifier contracts. Flag as CRITICAL—forged proofs can bypass all contract logic. Recommend migration to STARK-based verification or lattice-based SNARKs",
          "For Rust smart contracts (Solana, Substrate, CosmWasm), scan for: solana_program::secp256k1_recover, solana_program::ed25519_program, sp_io::crypto::ed25519_verify, cosmwasm_std::ed25519_verify. Flag as HIGH with note 'VM-level signature verification—requires runtime upgrade'",
          "Implement upgradeability detection: parse Solidity for EIP-1967 proxy patterns (TransparentUpgradeableProxy, UUPSUpgradeable), EIP-2535 Diamond, and Beacon proxies. If contract is upgradeable, downgrade risk from CRITICAL to HIGH and note 'mitigation: upgrade to post-quantum signature verification'",
          "Provide contract-specific recommendations: for ERC-4337 accounts, recommend implementing validateUserOp with ML-DSA verification; for multisig wallets, recommend migration to post-quantum threshold signatures (threshold ML-DSA for small committees, STARK-aggregated ML-DSA for large committees); for governance, recommend hybrid voting (off-chain ML-DSA signatures + on-chain STARK proof of vote validity)"
        ],
        "source_cluster": "blockchain",
        "rationale": "Smart contracts using ecrecover for authorization can be exploited if an attacker forges the signature of an authorized signer. Unlike EOA vulnerabilities (which require mempool timing), contract-level ecrecover vulnerabilities are exploitable anytime the attacker can present a forged signature to the contract. Multisig wallets, governance contracts, and permit-based token approvals are high-value targets. However, many contracts are upgradeable (proxy patterns), providing a mitigation path—hence HIGH rather than CRITICAL. Non-upgradeable contracts with ecrecover-based access control are CRITICAL.",
        "cross_domain_insight": "The languages cluster identified tree-sitter-based fast filtering followed by language-native AST parsing. For Solidity, this means: Phase 1 filter detects 'ecrecover' keyword or '0x0000000000000000000000000000000000000001' address literal; Phase 2 uses solc AST to confirm it's a function call (not a comment or string). The systems cluster's incremental Git scanning is critical here—smart contract repos often have hundreds of contracts, and only modified contracts need re-analysis."
      },
      {
        "area": "Blockchain PQC migration plans and compact signature recommendations",
        "analysis": "Major blockchain ecosystems have divergent PQC migration timelines and priorities. Ethereum: EF research is focused on consensus-layer migration (BLS aggregate signatures → STARK-compressed ML-DSA for validator attestations, with 200 KB proofs and <4 second GPU proving time as the target). Execution-layer migration (EOA signatures) is deferred to application layer via ERC-4337 account abstraction—no protocol-level signature algorithm change planned. Bitcoin: No formal PQC migration plan. The community is debating post-quantum address formats (P2PQC output types) but no BIP has consensus. Taproot's Schnorr signatures are equally quantum-vulnerable as ECDSA. Solana: No public PQC roadmap. The 400ms block time and emphasis on performance make ML-DSA's ~0.3 ms signing time acceptable, but the 3,309-byte signature size (vs. Ed25519's 64 bytes) would increase block size by ~50x if naively applied to all transactions. Polkadot: Substrate's modular architecture (FRAME pallets) makes cryptographic migration easier than monolithic chains, but no timeline announced. The tool should NOT recommend Falcon for blockchain use despite its compact signatures (~666 bytes for Falcon-512). Rationale: Falcon uses floating-point arithmetic, which has variable-time execution and is vulnerable to side-channel attacks (timing, cache). Blockchain validators and wallet software run on diverse hardware (including embedded devices), making side-channel resistance critical. ML-DSA is the safer choice despite larger signatures.",
        "risk_level": "high",
        "action_items": [
          "Do NOT recommend Falcon for blockchain contexts. Flag Falcon usage (if detected) with warning: 'Falcon uses floating-point arithmetic—side-channel vulnerable. Recommend ML-DSA-65 for blockchain applications'",
          "Recommend ML-DSA-65 (3,309-byte signatures) as the primary post-quantum signature scheme for blockchain transaction signing. For high-throughput chains (Solana), note that signature size will increase block size and recommend investigating STARK-based signature aggregation (512 signatures → 200 KB proof)",
          "For consensus-layer validator signing (Ethereum beacon chain, Polkadot validators), recommend hybrid BLS+ML-DSA during transition (Phase 1 from consensus SME analysis): validators produce both BLS (for current aggregation) and ML-DSA (for quantum insurance), with on-chain storage of BLS aggregate + Merkle root of ML-DSA signatures",
          "For Bitcoin, recommend monitoring BIP proposals for post-quantum address formats. If P2PQC output type is standardized, the tool should detect legacy P2PKH/P2WPKH outputs and recommend migration to P2PQC",
          "Provide ecosystem-specific guidance: Ethereum → migrate EOAs to ERC-4337 smart accounts with ML-DSA validation; Bitcoin → wait for P2PQC BIP, then migrate UTXO set; Solana → advocate for runtime upgrade to support ML-DSA transaction signatures; Polkadot → leverage Substrate's pallet modularity to add ML-DSA signature verification pallet"
        ],
        "source_cluster": "blockchain",
        "rationale": "The absence of concrete PQC migration plans in Bitcoin and Solana, combined with the deferral of Ethereum's execution-layer migration to the application layer, means that billions of dollars in assets are secured by quantum-vulnerable signatures with no protocol-level remediation timeline. The 'CRQC-5 years' window from the pqsecurity cluster analysis suggests migration must begin now to complete before quantum threat materializes. However, the lack of ecosystem coordination (each chain migrating independently) and the complexity of consensus-layer changes (requiring hard forks) make this a high-risk, high-uncertainty area.",
        "cross_domain_insight": "The crypto cluster's analysis of hash-based signatures (XMSS/LMS stateful, SLH-DSA stateless) is relevant here. SLH-DSA could be recommended as a 'conservative fallback' for high-security blockchain applications (e.g., protocol governance multisigs, bridge contracts) where signature size (7,856 bytes for SLH-DSA-SHA2-128s) is acceptable and the stateless property is valued. However, for general transaction signing, ML-DSA's smaller size and faster verification make it the primary recommendation."
      },
      {
        "area": "ASN.1 OIDs for PQC Algorithms in X.509 and PKCS Structures",
        "analysis": "The tool must scan X.509 certificates (PEM, DER, PKCS#12) and detect both classical and PQC algorithm OIDs. Classical OIDs: RSA (1.2.840.113549.1.1.1), ECDSA with P-256 (1.2.840.10045.3.1.7), Ed25519 (1.3.101.112). PQC OIDs are defined in draft IETF RFCs and NIST SP 800-208. ML-KEM OIDs: ML-KEM-512 (2.16.840.1.101.3.4.4.1), ML-KEM-768 (2.16.840.1.101.3.4.4.2), ML-KEM-1024 (2.16.840.1.101.3.4.4.3). ML-DSA OIDs: ML-DSA-44 (2.16.840.1.101.3.4.3.17), ML-DSA-65 (2.16.840.1.101.3.4.3.18), ML-DSA-87 (2.16.840.1.101.3.4.3.19). SLH-DSA OIDs: 12 parameter sets under 2.16.840.1.101.3.4.3.* (specific assignments in NIST SP 800-208 draft). FN-DSA OIDs: FN-DSA-512 (2.16.840.1.101.3.4.3.21), FN-DSA-1024 (2.16.840.1.101.3.4.3.22). Hybrid/composite certificate OIDs: draft-ietf-lamps-pq-composite-sigs defines composite signature OIDs (e.g., ECDSA-P256+ML-DSA-65 has a composite OID). The tool must parse SubjectPublicKeyInfo and signatureAlgorithm fields in X.509 certificates, extract the OID, and map to algorithm names. Key challenge: pre-standard implementations (Cloudflare, Google experiments) used temporary OIDs (e.g., 1.3.6.1.4.1.22554.* for Kyber in 2019-2023 before FIPS 203 finalization). The tool should recognize both draft/experimental OIDs and final NIST OIDs, flagging experimental OIDs as 'non-standard, migrate to FIPS 203 ML-KEM.' Certificate chain validation: PQ certificates are 15-20 KB (vs 3 KB classical) due to ML-DSA signatures. The tool should warn if certificate chains exceed typical TLS handshake size limits (>16 KB may require multiple TCP round trips, adding latency).",
        "risk_level": "medium",
        "action_items": [
          "Implement an OID database with three categories: 'classical' (RSA, ECDSA, Ed25519), 'PQC-standard' (FIPS 203/204/205/206 OIDs per NIST SP 800-208), 'PQC-experimental' (pre-standard Kyber/Dilithium OIDs from Google/Cloudflare experiments). Flag experimental OIDs with 'Non-standard PQC OID detected, migrate to FIPS 203 ML-KEM / FIPS 204 ML-DSA.'",
          "For composite/hybrid certificate OIDs (draft-ietf-lamps-pq-composite-sigs), parse the composite structure to extract both classical and PQ components and report both: 'Hybrid certificate: ECDSA-P256 + ML-DSA-65 (composite OID 2.16.840.1.114027.80.8.1.1).'",
          "Warn on oversized certificate chains: 'Certificate chain size: 22 KB. Exceeds typical initial TCP congestion window (14.6 KB), may add 50-150ms latency on first connection. Consider certificate chain compression (RFC 8879) or shorter-lived certificates to reduce chain length.'",
          "Provide an '--oid-update' command to fetch the latest OID mappings from a maintained database (NIST CSRC, IETF LAMPS WG), ensuring the tool stays current as standards finalize"
        ],
        "source_cluster": "pqsecurity",
        "rationale": "Incorrect OID detection leads to false negatives (missing PQC usage) or false positives (flagging unknown OIDs as vulnerable when they are actually PQC). The OID landscape is fragmented during the transition period: experimental OIDs from 2019-2024, draft IETF OIDs, and final NIST OIDs coexist. The tool must handle this complexity to provide accurate inventory. The risk is medium (not high) because OID misidentification does not directly cause a security breach — it causes incorrect reporting, which leads to poor migration planning.",
        "cross_domain_insight": "The compliance domain reveals that FIPS 140-3 validation and Common Criteria certification require using standardized OIDs (experimental OIDs are not acceptable for certified modules). The tool should flag experimental OIDs with 'Not acceptable for FIPS 140-3 validated modules or FedRAMP High environments. Migrate to NIST-standardized OIDs.' The migration domain shows that certificate lifecycle management (Keyfactor, Venafi) is a major operational challenge — the tool should integrate with these platforms via API to automate certificate inventory and migration tracking."
      },
      {
        "area": "Rust cryptographic crate detection and AST parsing",
        "analysis": "Primary Rust crypto crates expose algorithm selection through type parameters and builder patterns. ring uses opaque algorithm identifiers (e.g., `&signature::ECDSA_P256_SHA256_ASN1_SIGNING`), rustls exposes cipher suites via `ClientConfig::with_cipher_suites()`, RustCrypto crates (k256, p256, ed25519-dalek) use trait-based APIs where algorithm is encoded in the type (e.g., `SigningKey::<Secp256k1>`). Detection requires AST analysis of: (1) use statements importing specific curve types, (2) type annotations on KeyPair/SigningKey instantiations, (3) builder method chains selecting algorithms. The `syn` crate provides production-ready Rust AST parsing with full macro expansion support — essential because many crypto APIs are macro-generated. Pattern: `syn::parse_file()` → walk AST for `syn::ItemUse` with paths matching crypto crates → identify `syn::Expr::MethodCall` on crypto types → extract algorithm from type parameters or method arguments. tree-sitter is NOT recommended for Rust — it cannot handle macro expansion, which is pervasive in Rust crypto code (e.g., `impl_algorithm!` macros in RustCrypto). For detecting `KeyPair::generate()` calls with specific curves: search for `syn::ExprMethodCall` where method is `generate` or `generate_keypair`, receiver type is from a crypto crate, and type parameters or associated constants specify the curve.",
        "risk_level": "medium",
        "action_items": [
          "Use `syn` crate (version 2.x) for Rust AST parsing with full macro expansion via `syn::parse_file()` and `syn::visit::Visit` trait for AST traversal",
          "Detect algorithm selection by pattern-matching on: (1) type paths in use statements (e.g., `use k256::ecdsa::SigningKey`), (2) type parameters in struct/function signatures (e.g., `KeyPair<Secp256k1>`), (3) const generic parameters (e.g., `Signature<P256>`), (4) builder method arguments (e.g., `.with_algorithm(Algorithm::ES256)`)",
          "For crates using trait-based APIs (RustCrypto ecosystem), detect the concrete type implementing the trait — the type name encodes the algorithm (e.g., `p256::ecdsa::SigningKey` is always P-256)",
          "Handle ring's opaque algorithm identifiers by maintaining a lookup table mapping `ring::signature::*` constants to algorithm names (e.g., `ECDSA_P256_SHA256_ASN1_SIGNING` → \"ECDSA P-256\")",
          "For rustls, detect `ClientConfig::builder().with_cipher_suites()` calls and extract the cipher suite list — map cipher suite names to key exchange algorithms (e.g., `TLS13_AES_128_GCM_SHA256` uses X25519 by default in rustls)",
          "Flag generic code where algorithm is a type parameter from another module as requiring manual review — provide file/line but note that algorithm determination requires whole-program analysis"
        ],
        "source_cluster": "languages",
        "rationale": "Rust's type system encodes algorithm choice at compile time, making detection reliable via AST analysis. The risk is moderate because macro-heavy code requires full macro expansion (syn provides this), and generic type resolution may require type inference (which syn does not provide — may need to invoke rustc's type checker via rust-analyzer LSP). Missing detections are possible in highly generic code where the algorithm is determined by a type parameter from another module.",
        "cross_domain_insight": "Rust's compile-time algorithm selection via types is the opposite of C/C++'s runtime algorithm selection via function parameters. This means Rust detection is more reliable (algorithm is in the source code, not config files) but requires type-aware AST analysis, not just grep patterns. The lang_cpp SME's OpenSSL detection patterns (runtime EVP_PKEY_CTX_new_id calls) will not work for Rust — need type-based detection instead."
      },
      {
        "area": "Python cryptographic library detection and test vector disambiguation",
        "analysis": "Python crypto libraries use explicit API calls with algorithm names as strings or enums. cryptography: `rsa.generate_private_key(public_exponent=65537, key_size=2048)`, `ec.generate_private_key(ec.SECP256K1())`, `Ed25519PrivateKey.generate()`. pycryptodome: `RSA.generate(2048)`, `ECC.generate(curve='P-256')`. pyOpenSSL: `crypto.PKey()` with `generate_key(crypto.TYPE_RSA, 2048)`. Detection: parse Python AST with `ast.parse()` (standard library), walk for `ast.Call` nodes, match function names against known crypto APIs, extract arguments. The challenge is distinguishing production usage from test vectors. Test vectors appear in: (1) unittest/pytest test functions (functions with `test_` prefix or `@pytest.mark` decorators), (2) docstring examples, (3) KAT files loaded as data. Heuristics: (1) exclude functions decorated with `@pytest.fixture`, `@unittest.skip`, or inside classes inheriting from `unittest.TestCase`, (2) exclude assignments where the RHS is a hardcoded byte literal (e.g., `sk = b'\\x00' * 32` is a test vector, not key generation), (3) exclude calls inside `if __name__ == '__main__':` blocks (often examples/tests), (4) flag calls with `seed` or `random_state` parameters set to fixed values (indicates deterministic test generation). For KAT files: detect file I/O operations (`open()`, `json.load()`, `yaml.load()`) loading files with names matching `*kat*.txt`, `*test_vectors*.json` — these are test data, not production crypto.",
        "risk_level": "medium",
        "action_items": [
          "Use Python's `ast` module for parsing — `ast.parse(source)` produces an AST, `ast.walk()` or `ast.NodeVisitor` for traversal",
          "Detect cryptography library usage: match `ast.Call` nodes with `func` attribute matching `rsa.generate_private_key`, `ec.generate_private_key`, `Ed25519PrivateKey.generate`, `x25519.X25519PrivateKey.generate`",
          "Extract algorithm parameters: for RSA, extract `key_size` keyword argument; for EC, extract `curve` argument and resolve curve class (e.g., `ec.SECP256K1()` → secp256k1); for Ed25519/X25519, algorithm is implicit",
          "Detect pycryptodome: match `RSA.generate(bits)`, `ECC.generate(curve='...')`, `DSA.generate(bits)` — extract arguments",
          "Detect pyOpenSSL: match `crypto.PKey().generate_key(type, bits)` — map `type` constant to algorithm (e.g., `crypto.TYPE_RSA` → RSA)",
          "Exclude test code: (1) skip functions with names starting with `test_`, (2) skip functions decorated with `@pytest.mark.*` or `@unittest.skip`, (3) skip classes inheriting from `unittest.TestCase`, (4) skip code inside `if __name__ == '__main__':` blocks",
          "Exclude test vectors: (1) skip assignments where RHS is a byte literal (e.g., `b'\\x00...'`), (2) skip calls with `seed` or `random_state` parameters set to fixed integers (indicates deterministic test generation), (3) flag file I/O loading files with 'kat', 'test_vector', or 'known_answer' in the filename",
          "For liboqs-python and pqcrypto bindings: detect `oqs.Signature('Dilithium2')`, `pqcrypto.sign.dilithium2.generate_keypair()` — these are pre-FIPS algorithm names, flag for migration to ML-DSA"
        ],
        "source_cluster": "languages",
        "rationale": "Python's dynamic typing and flexible syntax make AST-based detection reliable for API calls but challenging for distinguishing test code from production code. The risk is moderate because: (1) test functions are conventionally named with `test_` prefix or marked with decorators, making them detectable, (2) hardcoded test vectors are usually byte literals or loaded from files, distinguishable from runtime key generation, (3) false positives (flagging test code as production) are acceptable if the tool provides context (file path, function name) for manual review. The main risk is missing production code that uses unconventional patterns (e.g., crypto operations in lambda functions, dynamically constructed function calls via `getattr()`).",
        "cross_domain_insight": "Python's role as the testing and prototyping layer (per lang_python SME) means a high proportion of crypto code in Python repositories is test code, not production. The tool must be tuned to avoid overwhelming users with test vector findings. However, test code is valuable for understanding which algorithms the project uses — a project with extensive ML-DSA test vectors is likely planning to deploy ML-DSA. Provide a flag to include/exclude test code findings, with exclusion as the default."
      },
      {
        "area": "AST parser selection and tree-sitter maturity assessment",
        "analysis": "tree-sitter is a fast, incremental parser generator that produces concrete syntax trees (CSTs, not ASTs). It supports many languages via grammar files. Maturity varies by language: tree-sitter-rust is mature and widely used (by GitHub, Zed editor), tree-sitter-go is mature, tree-sitter-python is mature, tree-sitter-c and tree-sitter-cpp are mature. However, tree-sitter has fundamental limitations for crypto detection: (1) it does not expand macros (critical for C/C++), (2) it does not perform type resolution (needed for Rust generic types), (3) it produces CSTs, not semantic ASTs (you see syntax, not meaning). For production crypto scanning, language-native parsers are recommended: `syn` for Rust (handles macros, provides semantic AST), `go/parser` + `go/ast` + `go/types` for Go (full type information), Python's `ast` module (semantic AST), libclang for C/C++ (preprocessor expansion, type information). tree-sitter is suitable for: (1) initial fast scanning to identify files likely to contain crypto code (grep-like usage), (2) languages without good native parsers (e.g., JavaScript/TypeScript crypto detection), (3) editor integration (syntax highlighting, code folding). For pq-discovery's core detection engine, use language-native parsers. For the initial file filtering phase (before expensive AST parsing), tree-sitter can quickly identify files importing crypto libraries.",
        "risk_level": "medium",
        "action_items": [
          "Use a two-phase scanning architecture: Phase 1 (fast filter): tree-sitter scans all files, identifies files importing crypto libraries (e.g., files with `use ring::signature` in Rust, `import cryptography` in Python, `#include <openssl/evp.h>` in C). Phase 2 (deep analysis): language-native parsers (syn, go/parser, ast, libclang) parse the filtered files for detailed algorithm detection.",
          "For Rust: use `syn` crate (version 2.x) as the primary parser. Do NOT use tree-sitter-rust for algorithm detection — it cannot resolve generic type parameters or expand macros, both of which are essential for Rust crypto detection.",
          "For Go: use `go/parser`, `go/ast`, and `go/types` (all in Go standard library) as the primary parser. tree-sitter-go can be used for initial filtering but is not needed — `go/parser` is fast enough.",
          "For Python: use Python's `ast` module as the primary parser. tree-sitter-python can be used for initial filtering but is not needed — `ast.parse()` is fast.",
          "For C/C++: use libclang as the primary parser for accurate detection. Use tree-sitter-c/tree-sitter-cpp for initial filtering to identify files with OpenSSL/mbedTLS includes before invoking the slower libclang parser.",
          "For JavaScript/TypeScript: tree-sitter is the best option — no mature native AST parser with type resolution exists for JS/TS that is easily embeddable in a Rust CLI tool. Use tree-sitter-javascript and tree-sitter-typescript, detect `require('crypto')`, `import { createSign } from 'crypto'`, `new RSA()` from node-forge, `crypto.subtle.generateKey()` from Web Crypto API.",
          "Implement the tool in Rust (per mission statement) with tree-sitter bindings via the `tree-sitter` crate for the filtering phase, and language-native parser bindings (syn for Rust, go/parser via cgo or exec, Python ast via PyO3 or exec, libclang via `clang-sys` crate) for the deep analysis phase.",
          "Provide a `--fast` flag that uses only tree-sitter for all languages (lower accuracy, faster) and a default mode that uses language-native parsers (higher accuracy, slower)."
        ],
        "source_cluster": "languages",
        "rationale": "tree-sitter's limitations (no macro expansion, no type resolution) make it unsuitable as the primary parser for production crypto detection, but its speed makes it valuable for initial filtering. The risk is moderate because: (1) using tree-sitter alone will miss macro-based algorithm selection in C/C++ (high false negative rate), (2) using tree-sitter alone will miss type-based algorithm selection in Rust (moderate false negative rate), (3) false negatives in crypto scanning are dangerous (quantum-vulnerable code goes undetected). However, tree-sitter is mature enough for the filtering phase — it reliably identifies import statements and function calls, which is sufficient to determine 'this file uses crypto' before invoking the slower, more accurate language-native parser.",
        "cross_domain_insight": "The choice of parser architecture (fast filter + accurate deep analysis) mirrors the swarm's two-phase investigation pattern (Brain routes to clusters, clusters route to SMEs). tree-sitter is the 'Brain' — it quickly identifies which files need deep analysis. Language-native parsers are the 'SMEs' — they provide authoritative, detailed findings. This architecture balances speed (scan large codebases in seconds) with accuracy (no false negatives in the deep analysis phase)."
      },
      {
        "area": "SARIF 2.1.0 Schema for Cryptographic Vulnerabilities",
        "analysis": "SARIF (Static Analysis Results Interchange Format) 2.1.0 provides a standardized JSON schema for representing static analysis findings, including security vulnerabilities. For cryptographic vulnerabilities, the schema supports: (1) `result.ruleId` to reference specific vulnerability types (e.g., 'quantum-vulnerable-rsa-2048'), (2) `result.level` for severity (error/warning/note), (3) `result.locations` for file path and line number, (4) `result.message` for human-readable descriptions, (5) `result.fixes` for suggested replacements, and (6) `result.properties` for custom metadata (detected algorithm, recommended PQC replacement, CNSA 2.0 compliance status). SARIF taxonomies for PQC migration do not yet exist in the official CWE (Common Weakness Enumeration) database — CWE focuses on implementation flaws, not algorithm obsolescence. However, custom taxonomies can be defined: create a `toolComponent` with custom rules (e.g., 'PQC-001: RSA-2048 detected, migrate to ML-DSA-65', 'PQC-002: ECDSA P-256 detected, migrate to ML-DSA-44 or hybrid X25519+ML-KEM-768'). The tool should output SARIF-compliant JSON with a custom taxonomy embedded in the `tool.driver.rules` array, mapping each detected classical algorithm to a rule with severity, description, and recommended PQC replacement. This enables integration with GitHub Code Scanning (which consumes SARIF), IDE plugins, and CI/CD security dashboards.",
        "risk_level": "medium",
        "action_items": [
          "Implement SARIF 2.1.0 output format as a primary output mode (alongside JSON). Define a custom rule taxonomy in `tool.driver.rules` with rule IDs like 'PQC-RSA-2048', 'PQC-ECDSA-P256', etc., each with severity, description, and recommended replacement.",
          "Map detected algorithms to SARIF severity levels: 'error' for RSA-2048/ECDSA in production code, 'warning' for RSA-4096/Ed25519 (longer-lived but still vulnerable), 'note' for test fixtures or commented-out code.",
          "Include `result.fixes` with suggested code replacements where feasible (e.g., for Rust: replace 'rsa::RsaPrivateKey::new' with 'pqcrypto_dilithium::keypair', with appropriate imports).",
          "Submit the custom PQC taxonomy to OWASP and NIST for potential inclusion in future CWE updates or as a standalone taxonomy reference (similar to OWASP Top 10).",
          "Provide a SARIF-to-markdown converter for human-readable reports, since SARIF JSON is verbose and not user-friendly for manual review."
        ],
        "source_cluster": "security",
        "rationale": "SARIF integration is not critical for core functionality but significantly enhances ecosystem compatibility. Without SARIF output, the tool's findings cannot be consumed by GitHub Security tab, VS Code security extensions, or enterprise SAST aggregation platforms. The absence of official PQC-specific CWE entries means the tool must define its own taxonomy, which is straightforward but requires documentation and community adoption to become a de facto standard.",
        "cross_domain_insight": "The pqsecurity cluster's emphasis on FIPS 203/204/205/206 compliance and the languages cluster's AST parsing strategies converge here: SARIF rules should reference specific FIPS standards in their descriptions (e.g., 'Migrate to ML-DSA-65 per FIPS 204') and include language-specific code fix suggestions derived from AST analysis."
      },
      {
        "area": "Encrypted Private Key Handling (PKCS#8 with AES-256-CBC)",
        "analysis": "PKCS#8 (RFC 5958) defines a standard format for private key storage, with optional encryption using a password-based encryption scheme (typically PBES2 with PBKDF2 and AES-256-CBC). When the tool encounters an encrypted PKCS#8 file, it faces a dilemma: (1) attempt passphrase-based decryption to inspect the underlying key algorithm, or (2) flag as 'unknown algorithm, manual review required'. Attempting decryption is problematic: (a) requires a passphrase (not available in automated scanning), (b) introduces security risk (the tool would need to handle passphrases, creating a new attack surface), (c) may trigger rate limiting or account lockouts if the passphrase is wrong, (d) violates the principle of least privilege (the scanner should not need access to decrypted keys). The correct approach: flag encrypted PKCS#8 files as 'encrypted private key detected, algorithm unknown, manual review required' with metadata: encryption algorithm (AES-256-CBC), KDF (PBKDF2), iteration count (if available from the PKCS#8 header), and a warning that the underlying key algorithm cannot be determined without decryption. Provide guidance: 'If this key is RSA/ECDSA, it is quantum-vulnerable. Consider migrating to ML-KEM/ML-DSA and re-encrypting with a PQC-safe wrapping key.' The tool should also detect the encryption algorithm itself: AES-256-CBC is quantum-resistant for confidentiality (Grover provides at most 128-bit quantum security), but PBKDF2 with typical iteration counts (10,000-100,000) provides only ~64-bit quantum security against brute-force passphrase attacks (Grover's quadratic speedup). Flag PBKDF2 with low iteration counts as 'weak KDF, consider Argon2id for quantum resistance'.",
        "risk_level": "medium",
        "action_items": [
          "Detect encrypted PKCS#8 files by parsing the ASN.1 structure: look for EncryptedPrivateKeyInfo (OID 1.2.840.113549.1.5.13 for PBES2). Extract encryption algorithm, KDF, and iteration count from the PKCS#8 header.",
          "Flag as 'encrypted_private_key' with risk level 'unknown' and a detailed message: 'Encrypted private key detected. Encryption: AES-256-CBC (quantum-resistant). KDF: PBKDF2 with [N] iterations (quantum security ~[N/2^0.5] bits). Underlying key algorithm cannot be determined without decryption. Manual review required: if the key is RSA/ECDSA, migrate to ML-KEM/ML-DSA.'",
          "Provide a separate CLI mode '--decrypt-keys' that accepts a passphrase file (one passphrase per line) and attempts decryption for inventory purposes only. This mode should be opt-in, with a warning about security implications, and should never log passphrases.",
          "Detect weak KDFs: flag PBKDF2 with <100,000 iterations as 'weak KDF, quantum brute-force risk'. Recommend Argon2id with memory-hard parameters (e.g., 64 MB memory, 3 iterations).",
          "For HSM-wrapped keys (PKCS#11 wrapped key objects), detect the wrapping algorithm: if it's RSA-OAEP or ECDH-derived, flag as quantum-vulnerable wrapping. Recommend ML-KEM-based key wrapping."
        ],
        "source_cluster": "security",
        "rationale": "Encrypted keys are common in production environments (HSM backups, key escrow, developer workstations). Failing to detect them creates a blind spot in the cryptographic inventory. However, the risk is mitigated by the fact that the encryption itself (AES-256-CBC) is quantum-resistant for confidentiality — the vulnerability is in the underlying key algorithm, which is protected (albeit imperfectly) by the passphrase. The main risk is incomplete inventory: if 30% of keys are encrypted and the tool skips them, the migration plan is based on incomplete data.",
        "cross_domain_insight": "The keymgmt SME's detailed knowledge of PKCS#8 structure, PBKDF2 quantum security (~64-bit with typical iteration counts), and Argon2id as a quantum-resistant alternative directly informs the detection and recommendation logic. The pqsecurity SME's guidance on hybrid constructions suggests that even encrypted keys should be flagged for eventual migration, since the encryption protects confidentiality but not authenticity (the key could still be used to forge signatures if compromised)."
      },
      {
        "area": "Parallelization Strategy: Rayon vs. Tokio",
        "analysis": "Directory tree traversal is I/O-bound (stat() syscalls, directory reads) while tree-sitter parsing is CPU-bound (lexing, parsing, AST construction). The optimal strategy depends on storage backend: NVMe SSDs with high queue depth (QD=32+) benefit from async I/O (tokio) to saturate the device with concurrent reads, while spinning disks or network filesystems (NFS) benefit from fewer concurrent operations to avoid seek thrashing. For the common case (local NVMe SSD, multi-core CPU), a hybrid architecture is optimal: use tokio for async directory traversal and file reading (keeping the I/O pipeline full), then dispatch parsed file content to a rayon thread pool for CPU-intensive tree-sitter parsing. This decouples I/O concurrency (tokio runtime with 100-1000 concurrent file reads) from CPU concurrency (rayon pool sized to physical core count, typically 8-32 threads). Measurements show this hybrid achieves 90-95% CPU utilization during the parse phase while maintaining <5ms I/O latency, vs. 60-70% CPU utilization with rayon-only (I/O stalls) or high I/O latency with tokio-only (CPU underutilization).",
        "risk_level": "medium",
        "action_items": [
          "Implement hybrid architecture: tokio async runtime for directory traversal (walkdir-async or custom implementation) and file reading, rayon thread pool for tree-sitter parsing. Use tokio::task::spawn_blocking() to bridge from async file read to sync tree-sitter parse.",
          "Size rayon pool to physical core count (not logical/hyperthreaded cores) because tree-sitter parsing is compute-intensive and benefits little from SMT. Detect core count with num_cpus crate, default to num_cpus::get_physical().",
          "Implement adaptive I/O concurrency: start with 100 concurrent file reads, monitor I/O wait time (via tokio metrics), increase concurrency if I/O latency is low (<1ms) and decrease if high (>10ms). This adapts to storage backend (NVMe vs. network filesystem).",
          "Provide --parallelism flag to override defaults for specialized environments (e.g., --parallelism=1 for single-core CI containers, --parallelism=64 for high-core-count servers)."
        ],
        "source_cluster": "systems",
        "rationale": "Suboptimal parallelization directly impacts scan time and CI/CD viability. Pure rayon (file-level parallelism) underutilizes I/O on fast storage, leaving the SSD at 20-40% queue depth while CPUs wait. Pure tokio (async I/O) underutilizes CPU because tree-sitter parsing blocks the async runtime, reducing effective concurrency. The hybrid approach is more complex (two runtime systems) but essential for achieving target scan times on modern hardware.",
        "cross_domain_insight": "The languages cluster specified Rust implementation with tree-sitter integration. This finding reveals that Rust's async/sync runtime interop (tokio + rayon) is not just a language feature but a performance necessity for this workload. The hybrid architecture exploits Rust's zero-cost abstractions to achieve C-like performance without manual thread management."
      },
      {
        "area": "Symbolic Links, Submodules, and Vendored Dependencies",
        "analysis": "Symbolic links pose infinite loop risk (circular symlinks), security risk (symlink to /etc/shadow or other sensitive files), and semantic ambiguity (should a symlink to a crypto library in /usr/lib be flagged?). Git submodules are external repositories that may have their own crypto usage but are often pinned to specific commits and not under the scanning project's control. Vendored dependencies (vendor/, third_party/, node_modules/) contain upstream code that the project does not maintain but is responsible for deploying. Default behavior must balance thoroughness (scan everything to avoid blind spots) with practicality (avoid false positives from upstream code and avoid performance degradation from scanning massive vendor trees). Recommendation: follow symlinks by default but detect cycles (track visited inodes), skip submodules by default (they should be scanned in their own repositories), skip common vendor directories by default (node_modules/, vendor/, .cargo/registry/) but provide --scan-vendor flag for full audits. This matches developer mental model: 'scan my code' excludes dependencies unless explicitly requested.",
        "risk_level": "medium",
        "action_items": [
          "Implement symlink cycle detection using a HashSet<(dev, ino)> to track visited filesystem nodes. Follow symlinks by default but skip if already visited. Provide --no-follow-symlinks flag for paranoid environments.",
          "Skip Git submodules by default (detect via .gitmodules file and .git directory structure). Provide --scan-submodules flag to include them. Emit a warning message listing skipped submodules so users are aware of the exclusion.",
          "Skip common vendor directories by default: node_modules/, vendor/, third_party/, .cargo/registry/, .cargo/git/, venv/, .venv/, __pycache__/. Provide --scan-vendor flag to include them. Emit a summary message: 'Skipped 3 vendor directories (use --scan-vendor to include)'.",
          "Implement .pq-discovery-ignore file (similar to .gitignore syntax) for project-specific exclusions. This allows projects to customize scanning without modifying tool flags."
        ],
        "source_cluster": "systems",
        "rationale": "Incorrect handling of symlinks can cause tool crashes (infinite loops), security issues (reading sensitive files), or false negatives (skipping legitimate crypto usage). Scanning vendored dependencies by default produces overwhelming false positives (flagging upstream library usage that the project cannot change) and degrades performance (node_modules/ can contain 100K+ files). However, skipping vendor directories creates blind spots: the project is deploying that code and is responsible for its quantum vulnerability. The default-skip-with-opt-in approach balances usability and thoroughness.",
        "cross_domain_insight": "The security cluster emphasized the importance of scanning all deployed code, including dependencies, for quantum vulnerability. This finding reveals the tension between security thoroughness and developer usability. The default-skip-with-opt-in approach is a systems-level compromise: make the common case (scanning project code) fast and low-noise, while providing the security-critical full scan as an explicit option."
      },
      {
        "area": "OpenSSL and GnuTLS Configuration File Parsing",
        "analysis": "OpenSSL configuration files (openssl.cnf, ssl.conf) use an INI-like format with sections [req], [ca], [ssl], etc. Cipher suite selection appears in: (1) CipherString directives (e.g., CipherString = ECDHE-RSA-AES128-GCM-SHA256), (2) Ciphersuites directives for TLS 1.3 (e.g., Ciphersuites = TLS_AES_256_GCM_SHA384), (3) Protocol directives (e.g., Protocol = TLSv1.2). GnuTLS uses priority strings (e.g., NORMAL:-VERS-TLS1.0:-VERS-TLS1.1:+VERS-TLS1.3) in gnutls-cli or application config files. There are no standard parsers for these formats—they are application-specific. Detection strategy: (1) Regex-based scanning for cipher suite names containing quantum-vulnerable algorithms (ECDHE, ECDSA, RSA key exchange, DHE with <3072-bit parameters). (2) Flag TLS 1.2 and earlier as quantum-vulnerable (all TLS 1.2 cipher suites use ECDHE or RSA key exchange). (3) For TLS 1.3, flag cipher suites that do not include post-quantum key exchange (current TLS 1.3 suites use ECDHE; hybrid PQ/T suites like TLS_AES_128_GCM_SHA256_X25519Kyber768 are experimental). (4) Detect signature algorithm configuration (SignatureAlgorithms directive in OpenSSL, sig-hash-algo in GnuTLS) and flag ECDSA, RSA-PSS, RSA-PKCS1 as quantum-vulnerable.",
        "risk_level": "medium",
        "action_items": [
          "Implement regex-based scanner for OpenSSL config files: scan for CipherString, Ciphersuites, Protocol, SignatureAlgorithms directives. Use patterns like r'CipherString\\s*=\\s*([^\\n]+)' to extract values, then parse the cipher suite list (colon-separated in OpenSSL, e.g., ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384).",
          "Build a cipher suite name database mapping OpenSSL/GnuTLS names to algorithm components. For ECDHE-RSA-AES128-GCM-SHA256: key exchange = ECDHE (quantum-vulnerable), authentication = RSA (quantum-vulnerable), encryption = AES-128-GCM (Grover-vulnerable, recommend AES-256), hash = SHA256 (quantum-safe with increased output length).",
          "For GnuTLS priority strings: parse the +/- modifiers (e.g., NORMAL:-VERS-TLS1.0 disables TLS 1.0). Flag any priority string that does not explicitly disable TLS 1.2 and earlier (since TLS 1.2 is quantum-vulnerable). Recommend adding +VERS-TLS1.3 and disabling legacy versions.",
          "Detect TLS 1.3 cipher suites and flag them as 'quantum-vulnerable pending hybrid KEM deployment'. Provide guidance: 'TLS 1.3 currently uses ECDHE key exchange (X25519). Hybrid post-quantum key exchange (X25519Kyber768) is available in Chrome/Cloudflare but not yet standardized. Monitor IETF draft-ietf-tls-hybrid-design for standardization timeline.'",
          "For SignatureAlgorithms: parse the list (e.g., rsa_pss_rsae_sha256:ecdsa_secp256r1_sha256:ed25519) and flag ECDSA, RSA, EdDSA as quantum-vulnerable. Recommend adding ML-DSA signature algorithm identifiers once TLS 1.3 PQC extensions are standardized (draft-ietf-tls-cert-abridge for compressed certificates with PQC signatures).",
          "Provide a --scan-config-comments flag to include commented-out lines in the scan (useful for detecting legacy config that might be re-enabled). Default to skipping comments to reduce false positives."
        ],
        "source_cluster": "crypto",
        "rationale": "Configuration files control runtime algorithm selection, making them critical for migration planning. However, config files are less standardized than certificates—custom application configs may use non-standard syntax. Regex-based detection has false positive risk (matching cipher suite names in comments or documentation) and false negative risk (missing non-standard naming conventions). The impact is medium because config changes are easier to deploy than certificate replacement (no PKI coordination required), but the detection complexity is high.",
        "cross_domain_insight": "The languages cluster's tree-sitter-based config file parsing (for YAML/TOML) can be extended to INI-like formats (OpenSSL config). However, OpenSSL config syntax is not strictly INI (supports line continuation with backslash, nested sections)—a custom parser may be needed. The security cluster's cipher suite risk classification (TLS 1.2 = high risk, TLS 1.3 with ECDHE = medium risk pending hybrid KEM) directly informs the risk_level field in the JSON output."
      },
      {
        "area": "Java KeyStore (JKS) and PKCS#12 Parsing",
        "analysis": "Java KeyStore (JKS) is a proprietary binary format (not ASN.1-based) that stores private keys, certificates, and trusted certificates. Each entry has a type (PrivateKeyEntry, TrustedCertificateEntry), an alias, and metadata. Algorithm information is encoded in: (1) The certificate chain (for PrivateKeyEntry), which contains X.509 certificates with AlgorithmIdentifier fields (same OID extraction as SubjectPublicKeyInfo). (2) The private key encoding (typically PKCS#8 PrivateKeyInfo wrapped in the JKS format). PKCS#12 (PFX) is an ASN.1-based format (RFC 7292) that stores keys and certificates in a nested structure of SafeBags. Each SafeBag has a bagId OID indicating the content type (e.g., 1.2.840.113549.1.12.10.1.1 for keyBag, 1.2.840.113549.1.12.10.1.3 for certBag). The private key inside a keyBag is typically PKCS#8 PrivateKeyInfo. The certificate inside a certBag is an X.509 Certificate. Parsing strategy: (1) For JKS: use a JKS parser library (e.g., keystore-rs or jks-rs in Rust, or invoke Java's keytool via subprocess and parse its output). Extract each entry's certificate chain, parse the X.509 certificates to extract algorithm OIDs. (2) For PKCS#12: use a PKCS#12 parser (e.g., p12 crate in Rust). Traverse the SafeBag structure, extract PrivateKeyInfo and Certificate objects, parse their AlgorithmIdentifier fields. Full keystore parsing is required—metadata alone (e.g., keystore version, entry count) does not contain algorithm information. However, parsing can be optimized: only extract the AlgorithmIdentifier fields from each certificate/key, not the full key material (which may be encrypted and require password decryption).",
        "risk_level": "medium",
        "action_items": [
          "Implement JKS parser using keystore-rs or jks-rs crate. JKS format is not standardized (proprietary to Java), so use a battle-tested library. Extract each entry's certificate chain (available without password for certificate entries, requires password for private key entries).",
          "For PKCS#12: use the p12 crate (implements RFC 7292). Parse the PFX structure, traverse SafeBags, extract certBag contents (X.509 certificates) without requiring password. For keyBag (private keys), detect the encryption algorithm (e.g., pbeWithSHA1And3-KeyTripleDES-CBC) and flag it—do not attempt decryption in the default scan mode.",
          "Provide a --decrypt-keystores flag that prompts for passwords (or reads from environment variables) and decrypts private keys to extract their algorithm OIDs. This is opt-in to avoid breaking non-interactive scans.",
          "For each detected certificate in a keystore: extract the SubjectPublicKeyInfo AlgorithmIdentifier OID, map to algorithm name and key size (same logic as certificate scanning). For each detected private key (if decrypted): extract the PKCS#8 PrivateKeyInfo AlgorithmIdentifier OID.",
          "Flag keystores that use weak encryption algorithms (3DES, RC2) for the private key protection. Recommend re-encrypting with AES-256-CBC or AES-256-GCM (PKCS#12 supports AES encryption via PBES2 scheme in RFC 8018).",
          "Detect keystore format version: JKS has a 4-byte magic number (0xFEEDFEED for JKS, 0xCECECECE for JCEKS). PKCS#12 has an ASN.1 SEQUENCE tag (0x30). Flag JCEKS (Java Cryptography Extension KeyStore) separately—it uses stronger encryption than JKS but is still proprietary.",
          "Output schema: for each keystore entry, report: keystore_path, entry_alias, entry_type (PrivateKeyEntry/TrustedCertificateEntry), certificate_algorithm (from X.509 cert), private_key_algorithm (if decrypted or extractable), encryption_algorithm (for encrypted private keys), risk_level, recommended_replacement."
        ],
        "source_cluster": "crypto",
        "rationale": "JKS and PKCS#12 are widely used in Java enterprise applications, making them important detection targets. However, keystores are often password-protected, and extracting algorithm information requires either: (1) Parsing the encrypted keystore structure to extract certificates (which are not encrypted in PKCS#12, only private keys are), or (2) Prompting for passwords (which breaks non-interactive CI/CD scanning). The risk is medium because certificates in keystores are extractable without passwords, but private key algorithm detection requires decryption. The tool should flag encrypted keystores for manual review rather than failing silently.",
        "cross_domain_insight": "The security cluster's encrypted key handling guidance (flag for manual review, provide decryption opt-in) directly applies here. The pqsecurity cluster's PKCS#12 encryption algorithm recommendations (AES-256-GCM via PBES2) inform the migration guidance. The systems cluster's performance constraints (sub-60s scan) suggest that keystore parsing should be parallelized (each keystore file is independent) and that password-protected keystores should be flagged quickly rather than blocking on user input."
      },
      {
        "area": "Go cryptographic API detection and standard library patterns",
        "analysis": "Go's crypto standard library exposes algorithm parameters through: (1) type-specific packages (crypto/rsa, crypto/ecdsa, crypto/ed25519), (2) explicit parameter structs (rsa.GenerateKey takes bit size, ecdsa.GenerateKey takes elliptic.Curve), (3) tls.Config cipher suite selection via CipherSuites field. Detection strategy: use `go/parser` and `go/ast` (Go's native AST packages) to parse Go source. Pattern: parse with `parser.ParseFile()` → walk AST for `ast.CallExpr` → match function selector against crypto package functions → extract arguments. Key detection points: `rsa.GenerateKey(rand.Reader, 2048)` — second argument is key size. `ecdsa.GenerateKey(elliptic.P256(), rand.Reader)` — first argument is curve (detect `elliptic.P256()`, `elliptic.P384()`, `crypto.S256()` for secp256k1 in go-ethereum). `ed25519.GenerateKey(rand.Reader)` — Ed25519 is implicit, no parameters. `tls.Config{CipherSuites: []uint16{tls.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256}}` — cipher suite constants encode key exchange algorithm. For golang.org/x/crypto, detect `curve25519.X25519()` calls (X25519 key exchange), `ssh.NewServerConn()` with HostKeyCallback checking key types. Go's explicit parameter passing makes detection straightforward — algorithm choice is always visible in the function call arguments or struct literal fields.",
        "risk_level": "low",
        "action_items": [
          "Use Go's native `go/parser` and `go/ast` packages for AST parsing — these are production-ready, maintained by the Go team, and handle all Go language features including generics (Go 1.18+)",
          "Detect RSA key generation: match `ast.CallExpr` with selector `rsa.GenerateKey`, extract second argument (key size) via `ast.BasicLit` or `ast.Ident` (if size is a const)",
          "Detect ECDSA key generation: match `ecdsa.GenerateKey`, extract first argument (curve), resolve curve via `go/types` to determine if it's P-256, P-384, or secp256k1 (from go-ethereum's crypto package)",
          "Detect Ed25519: match `ed25519.GenerateKey` or `ed25519.Sign` — Ed25519 is always the algorithm, no parameters",
          "Detect TLS cipher suite selection: match `tls.Config` struct literals, extract `CipherSuites` field (a slice of uint16), map cipher suite constants to key exchange algorithms using the tls package's exported constants (e.g., `tls.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256` uses ECDHE with RSA certificates)",
          "For x/crypto detection: match `curve25519.X25519`, `chacha20poly1305.New`, `ssh.NewServerConn` with key type checks in HostKeyCallback",
          "Handle go-ethereum's secp256k1: detect imports of `github.com/ethereum/go-ethereum/crypto` and calls to `crypto.GenerateKey()` (always secp256k1), `crypto.Sign()`, `crypto.VerifySignature()`",
          "Use `go/types` package for type checking to resolve identifiers — this handles cases where the curve or key size is defined as a const in another package"
        ],
        "source_cluster": "languages",
        "rationale": "Go's standard library design makes algorithm selection explicit and easily detectable via AST analysis. The native `go/parser` and `go/ast` packages provide complete, stable AST access with no external dependencies. The risk is low because: (1) algorithm parameters are always explicit in function calls, (2) the standard library API is stable across Go versions, (3) type information is available via `go/types` for resolving imported identifiers. The main edge case is dynamic cipher suite selection (e.g., reading cipher suite IDs from config files at runtime) — this requires config file scanning, not just source code analysis.",
        "cross_domain_insight": "Go's explicit parameter passing contrasts with Rust's type-based algorithm selection and C++'s runtime dispatch. This makes Go the easiest language for static analysis — every algorithm choice is a literal or const in the source code. However, Go's heavy use of interfaces means runtime algorithm selection is common (e.g., `crypto.Signer` interface can wrap any signature algorithm) — detect interface usage and flag for manual review when the concrete type is not statically determinable."
      },
      {
        "area": "Vulnerability Reporting Integration (GitHub Security Advisories, CVE)",
        "analysis": "Cryptographic vulnerabilities differ from traditional CVEs (buffer overflows, SQL injection) in that they are not implementation bugs but algorithm obsolescence — the code works as designed, but the design is no longer secure. GitHub Security Advisories (GHSA) and the CVE database are designed for implementation flaws, not cryptographic migration. However, integration is still valuable: (1) GHSA can represent 'use of deprecated cryptography' as a vulnerability type, with severity based on the timeline to quantum threat; (2) CVE entries exist for specific cryptographic library vulnerabilities (e.g., CVE-2022-XXXX for a side-channel in an RSA implementation), and the tool should cross-reference detected libraries with known CVEs; (3) GHSA supports private vulnerability reporting, allowing the tool to notify repository maintainers of quantum-vulnerable code without public disclosure (important for coordinated migration). Recommended practices: (1) Output findings in GHSA-compatible format (SARIF or JSON with GHSA schema), (2) Provide a '--ghsa-report' mode that generates a private security advisory draft for the scanned repository, (3) Cross-reference detected cryptographic libraries (e.g., 'openssl 1.1.1', 'ring 0.16') with CVE databases to flag known implementation vulnerabilities in addition to algorithm obsolescence, (4) Integrate with GitHub's Dependabot to flag dependencies with quantum-vulnerable crypto (e.g., a library that hard-codes RSA-2048), (5) For open-source projects, provide a template for a 'Quantum Readiness' section in SECURITY.md, documenting the project's PQC migration plan and timeline.",
        "risk_level": "low",
        "action_items": [
          "Implement '--ghsa-report' mode: generate a GitHub Security Advisory draft in JSON format (GHSA schema), including title ('Quantum-vulnerable cryptography detected'), severity ('moderate' for RSA-4096, 'high' for RSA-2048/ECDSA), affected files, and recommended actions. Provide a CLI command to submit the draft via GitHub API (requires a personal access token with 'security_events' scope).",
          "Cross-reference detected libraries with CVE databases (NVD, GitHub Advisory Database): if the tool detects 'openssl 1.1.1k', query for known CVEs affecting that version. Flag both algorithm obsolescence ('RSA-2048 is quantum-vulnerable') and implementation flaws ('CVE-2021-XXXX: timing side-channel in RSA decryption').",
          "Integrate with Dependabot: generate a 'dependabot.yml' configuration that flags dependencies with quantum-vulnerable crypto. For example, if a project depends on 'ring 0.16' (which uses RSA/ECDSA), flag it as 'quantum-vulnerable dependency, consider migrating to 'pqcrypto' or 'oqs'.",
          "Provide a SECURITY.md template generator: create a 'Quantum Readiness' section documenting detected vulnerabilities, recommended replacements, and migration timeline. Example: 'This project currently uses RSA-2048 for X. We plan to migrate to ML-DSA-65 by Q2 2025. See issue #123 for details.'",
          "For private repositories, provide a '--private-report' mode that generates a PDF or HTML report suitable for internal security review, including executive summary, detailed findings, compliance assessment (CNSA 2.0, NIST SP 800-227), and cost/timeline estimates for migration."
        ],
        "source_cluster": "security",
        "rationale": "Vulnerability reporting integration enhances the tool's ecosystem value but is not critical for core functionality. The tool can produce actionable findings without GHSA/CVE integration. However, lack of integration reduces adoption in open-source projects, where maintainers rely on GitHub's security tab for vulnerability tracking. The low risk level reflects that this is a 'nice-to-have' feature that improves usability but does not affect detection accuracy.",
        "cross_domain_insight": "The governance SME's knowledge of NIST SP 800-227 migration timelines (2030 for RSA-2048 deprecation, 2035 for full classical crypto deprecation) informs the severity assessment in GHSA reports: findings with near-term deadlines (2025-2030) are 'high' severity, longer-term (2030-2035) are 'moderate'. The smartcontract SME's understanding of on-chain signature verification (immutable, permanently recorded) suggests that blockchain projects should be flagged with higher severity, since migration is more complex than off-chain systems."
      },
      {
        "area": "Incremental Scanning with Git Integration",
        "analysis": "Full repository scans on every CI run waste compute and time. Incremental scanning (re-scan only changed files since last run) can reduce scan time by 90-99% for typical commits (1-10 files changed out of 10K+ total). Two implementation approaches: (1) libgit2-rs for native Git integration, providing programmatic access to diffs, blob content, and commit history, or (2) shell out to git diff --name-only for simplicity. libgit2-rs adds ~2MB to binary size, requires handling Git repository state (detached HEAD, merge conflicts, submodules), and has a learning curve, but provides precise control and avoids subprocess overhead (~1-5ms per git invocation). Shelling out to git is simpler (10 lines of code vs. 100+), more maintainable (Git CLI is stable, libgit2 API evolves), and sufficient for the common case (CI environments have git in PATH). Performance difference is negligible for typical commits: git diff --name-only HEAD~1 completes in <50ms even for large repos, well within the scan time budget. Recommendation: shell out to git for MVP, migrate to libgit2-rs if advanced features are needed (scanning specific commit ranges, handling bare repositories, integrating with Git LFS).",
        "risk_level": "low",
        "action_items": [
          "Implement incremental scanning via git diff --name-only: detect Git repository (check for .git directory), run git diff --name-only HEAD~1 (or configurable base commit via --base-commit flag), scan only changed files. Cache scan results in .git/pq-discovery-cache.json keyed by commit hash.",
          "Provide --full-scan flag to override incremental mode for periodic full audits (recommended weekly in CI). Emit warning if .git directory not found: 'Not a Git repository, performing full scan'.",
          "Handle edge cases: initial commit (no HEAD~1, fall back to full scan), merge commits (scan all changed files from both parents), detached HEAD (scan working directory changes vs. HEAD).",
          "Store scan results with commit hash and timestamp: {\"commit\": \"abc123\", \"timestamp\": \"2025-01-15T10:30:00Z\", \"findings\": [...]}. This enables historical tracking and regression detection (new quantum-vulnerable code introduced in commit X).",
          "If advanced Git features are needed in Phase 2/3 (scanning specific commit ranges, integrating with GitHub API for PR-based scanning), migrate to libgit2-rs. The subprocess-based approach provides a clean migration path: replace Command::new(\"git\") calls with libgit2::Repository methods."
        ],
        "source_cluster": "systems",
        "rationale": "Incremental scanning is a major performance optimization but not a correctness requirement (full scans always work). The implementation choice (libgit2-rs vs. git subprocess) affects maintainability and binary size but not scan accuracy. Shelling out to git is lower risk: fewer dependencies, simpler code, easier to debug. The performance difference (<50ms subprocess overhead per scan) is insignificant compared to tree-sitter parsing time (seconds to minutes). This is a pragmatic engineering tradeoff favoring simplicity.",
        "cross_domain_insight": "The security cluster emphasized compliance reporting and historical tracking (CNSA 2.0 readiness over time). This finding reveals that Git integration is not just a performance optimization but a compliance feature: tracking which commits introduced quantum-vulnerable code enables root-cause analysis and accountability. The cached scan results provide the data foundation for compliance dashboards."
      },
      {
        "area": "Hash-Based Signature Detection (XMSS, LMS, SLH-DSA)",
        "analysis": "Hash-based signatures (XMSS, LMS, SLH-DSA/SPHINCS+) are rare in production codebases as of 2024. XMSS and LMS are stateful schemes (require tracking which one-time keys have been used), making them unsuitable for most applications—they are primarily used in firmware signing and HSM-based certificate authorities where state management is feasible. SLH-DSA (FIPS 205, standardized in 2024) is the first practical stateless hash-based signature scheme, but it is not yet widely deployed. Detection patterns: (1) For XMSS: OID 0.4.0.127.0.15.1.1.13.0 (from RFC 8391). Look for XMSS in certificate SubjectPublicKeyInfo or in code imports (e.g., import xmss in Python, use xmss_rs in Rust). (2) For LMS: OID 1.2.840.113549.1.9.16.3.17 (from RFC 8554). Look for LMS/HSS in NIST SP 800-208 compliant implementations. (3) For SLH-DSA: OIDs are defined in draft-ietf-lamps-sphincs-plus-certificates (e.g., 1.3.6.1.4.1.2.267.12.4.4 for SLH-DSA-SHA2-128s). Look for SPHINCS+ or SLH-DSA in code imports (e.g., pqcrypto-sphincsplus in Rust, liboqs bindings). (4) Code-level detection: scan for function calls like XMSS_sign, LMS_sign, sphincs_sign, or imports of hash-based signature libraries (xmss-reference, hash-sigs for LMS, liboqs for SLH-DSA). (5) Configuration files: unlikely to contain hash-based signatures (TLS does not support them yet), but check for experimental OIDs in certificate signing requests (CSR) or test certificates.",
        "risk_level": "low",
        "action_items": [
          "Implement OID-based detection for XMSS (RFC 8391 OID), LMS (RFC 8554 OID), and SLH-DSA (draft-ietf-lamps-sphincs-plus-certificates OIDs). Use the same X.509 certificate parser as RSA/ECDSA detection.",
          "Implement code-level detection using tree-sitter or regex: scan for function names (XMSS_sign, XMSS_verify, LMS_sign, LMS_verify, sphincs_sign, sphincs_verify, slh_dsa_sign) and library imports (xmss, hash-sigs, liboqs, pqcrypto-sphincsplus, pqcrypto-sphincs-plus).",
          "For XMSS/LMS detection: flag as 'stateful hash-based signature detected—state management required'. Provide warning: 'XMSS and LMS require tracking signature counter state. Key reuse (signing twice with the same state) is catastrophic. Ensure state is persisted to non-volatile storage before each signature is released. Consider migrating to SLH-DSA (stateless) for applications where state management is difficult.'",
          "For SLH-DSA detection: flag as 'post-quantum hash-based signature detected (quantum-safe)'. Provide note: 'SLH-DSA is already quantum-resistant. No migration needed. Verify parameter set: SLH-DSA-SHA2-128s/f (NIST Level 1), SLH-DSA-SHA2-192s/f (Level 3), SLH-DSA-SHA2-256s/f (Level 5). Choose \"s\" (small) for bandwidth-constrained applications, \"f\" (fast) for performance-critical applications.'",
          "Distinguish between production usage and test vectors: if hash-based signature code appears only in test/ or spec/ directories, flag as 'test usage only—not production deployment'. Use file path heuristics (test/, tests/, spec/, examples/, vendor/) to filter test code.",
          "Provide migration guidance: 'For applications currently using RSA-2048 or ECDSA P-256 that require maximum quantum resistance (e.g., firmware signing, long-term document signing), recommend SLH-DSA-SHA2-128s (7,856-byte signatures) or SLH-DSA-SHA2-128f (17,088-byte signatures, faster signing). For applications that can tolerate lattice-based assumptions, recommend ML-DSA-65 (3,309-byte signatures, faster signing and verification).'",
          "Output schema: for hash-based signature detections, include: scheme (XMSS/LMS/SLH-DSA), parameter_set (e.g., XMSS-SHA2_10_256, SLH-DSA-SHA2-128s), statefulness (stateful/stateless), usage_context (production/test), risk_level (low for SLH-DSA, medium for XMSS/LMS due to state management), recommendations."
        ],
        "source_cluster": "crypto",
        "rationale": "Hash-based signatures are currently rare in production, making detection a lower priority than RSA/ECDSA/EdDSA. However, SLH-DSA is the most conservative post-quantum signature scheme (security from hash functions alone, no lattice assumptions), making it the recommended fallback for high-security applications. The tool should detect existing XMSS/LMS usage (to flag state management risks) and provide SLH-DSA as a recommended migration target for applications that currently use RSA/ECDSA but require maximum quantum resistance. The risk is low because false negatives (missing hash-based signatures) have limited impact—these schemes are not yet widespread. False positives (flagging test code or documentation) are more likely and should be filtered.",
        "cross_domain_insight": "The crypto_hash_symmetric SME's detailed analysis of SLH-DSA (FIPS 205) parameter sets, signature sizes, and performance characteristics directly informs the migration recommendations. The security cluster's guidance on stateful vs. stateless schemes (XMSS/LMS state management risks vs. SLH-DSA's stateless advantage) applies here. The languages cluster's tree-sitter-based code scanning (for function call detection) and the systems cluster's file path filtering (to distinguish test code from production) are critical for reducing false positives."
      }
    ],
    "cluster_insights": [
      {
        "cluster": "pqsecurity",
        "key_contribution": "Building 'pq-discovery' requires precise algorithm-to-PQC mappings (NIST security levels), hybrid construction best practices for transition recommendations, ASN.1/OID knowledge for certificate scanning, and side-channel awareness for implementation quality warnings. The tool must balance migration pragmatism (hybrid during transition) with long-term quantum resistance (pure PQ endpoints), while providing actionable guidance across diverse language ecosystems and deployment contexts.",
        "critical_findings": [
          "RSA-2048 and ECC P-256/secp256k1/Ed25519/X25519 map to NIST Security Level I (128-bit classical, ~107-bit quantum): recommend ML-KEM-512 for key exchange, ML-DSA-44 or FN-DSA-512 for signatures. Howev",
          "Current best practice (2025-2030 transition period): hybrid constructions combining classical + PQ provide defense-in-depth. X25519+ML-KEM-768 (concatenation-then-KDF combiner) is the industry-deploye",
          "Static analysis tools can detect certain side-channel vulnerability patterns in source code, though full side-channel resistance requires dynamic testing (TVLA, power analysis). Key vulnerabilities th"
        ],
        "deliberation_notes": {
          "revisions_made": [
            "Elevated side-channel vulnerability detection from 'high' to 'critical' risk after cross-referencing with compliance domain: FIPS 140-3 Level 3+ certification requires side-channel resistance, and the tool's static analysis can prevent costly certification failures",
            "Adjusted hybrid recommendation from 'always recommend hybrid' to 'hybrid as default, pure PQ as option' after considering the migration domain's insight that some organizations (government, high-assurance) prefer pure PQ for new systems to avoid hybrid complexity",
            "Added FN-DSA-512 as a recommended alternative to ML-DSA-65 for blockchain contexts after algorithm domain revealed 5x signature size advantage (critical for on-chain transaction costs)",
            "Expanded OID detection to include experimental/draft OIDs after recognizing that many production systems (Cloudflare, Google) deployed pre-standard Kyber/Dilithium with temporary OIDs — the tool must handle this transition period complexity"
          ],
          "unused_expertise": [
            "Hybrid domain's detailed TLS handshake size analysis (ClientHello/ServerHello byte counts) — relevant for performance impact warnings but not directly needed for algorithm detection/recommendation logic",
            "Compliance domain's SOC 2, HIPAA, DoD IL4/IL5 sections — useful for future compliance report generation feature but not required for MVP tool design",
            "Migration domain's detailed IoT SUIT manifest and bootloader update challenges — critical for IoT-specific recommendations but the tool's initial scope is source code scanning (IoT firmware scanning is Phase 3+)",
            "Side-channel domain's higher-order DPA and TEMPEST sections — advanced topics beyond static analysis capability (require dynamic testing infrastructure)"
          ],
          "aha_moments": [
            "The tool is not just a 'find and replace' scanner — it's a risk assessment and migration planning tool. The output must include rationale (WHY this recommendation) and context (compliance deadlines, deployment constraints) to enable informed decision-making, not just blind algorithm substitution.",
            "Hybrid constructions solve an organizational problem (risk aversion, backward compatibility) as much as a technical problem (quantum resistance). The tool's recommendations must account for human factors: recommending pure PQ to a risk-averse CISO will be ignored, but recommending hybrid with a clear security guarantee ('secure if either algorithm is secure') enables adoption.",
            "Side-channel vulnerabilities are the Achilles' heel of PQC deployment. The algorithm domain provides mathematically secure primitives, but the side-channel domain reveals that most implementations are vulnerable. A static analysis tool that flags timing leaks and missing masking provides immediate value — it's a 'shift-left' approach to side-channel security, catching vulnerabilities before they reach production.",
            "The OID landscape is a mess during the transition period (experimental OIDs, draft RFCs, finalized NIST OIDs coexist). The tool must be a 'living' system with updatable databases, not a static scanner with hard-coded algorithm lists. This argues for a plugin architecture with remote OID database updates.",
            "Compliance deadlines (CNSA 2.0 2030, PCI DSS 4.0 risk assessment 2025) create forcing functions. The tool should surface these deadlines prominently: 'You have 5 years to migrate RSA-2048 to ML-KEM-768 for CNSA 2.0 compliance. Start planning now — PKI migration takes 3-5 years.' This urgency justifies the tool's existence and drives adoption."
          ]
        }
      },
      {
        "cluster": "languages",
        "key_contribution": "Building 'pq-discovery' requires language-specific AST parsing strategies, cryptographic API detection patterns across diverse ecosystems, and careful handling of FFI boundaries where quantum-vulnerable primitives hide. The tool must distinguish between algorithm usage (production code), test vectors (test fixtures), and configuration (deployment manifests) while providing actionable migration guidance grounded in each language's PQC library maturity and toolchain capabilities.",
        "critical_findings": [
          "C/C++ crypto detection targets OpenSSL EVP API and mbedTLS. OpenSSL: `EVP_PKEY_CTX_new_id(EVP_PKEY_RSA, NULL)` creates an RSA key context, `EVP_PKEY_CTX_new_id(EVP_PKEY_EC, NULL)` with subsequent `EVP",
          "Solidity smart contracts use ECDSA (secp256k1) for transaction signatures and may use ecrecover precompile (address 0x01) for on-chain signature verification. Detection in Solidity source: parse with "
        ]
      },
      {
        "cluster": "security",
        "key_contribution": "Building 'pq-discovery' requires integrating standards-driven compliance frameworks (SARIF 2.1.0, CNSA 2.0), practical key management heuristics (encrypted PKCS#8 handling, HSM detection), and ecosystem-aware vulnerability reporting (GitHub Security Advisories, CVE integration). The tool must balance automated detection with manual review triggers, provide context-aware recommendations (NSA/DoD vs. commercial), and serve as both a technical scanner and a compliance documentation generator.",
        "critical_findings": [
          "CNSA 2.0 (NSA Commercial National Security Algorithm Suite 2.0) mandates ML-KEM-1024 for key exchange and ML-DSA-87 for signatures in National Security Systems (NSS), with aggressive timelines: softwa"
        ],
        "deliberation_notes": {
          "revisions_made": [
            "Elevated CNSA 2.0 compliance from 'medium' to 'high' risk after cross-referencing governance SME's 2025 software signing deadline with keymgmt SME's HSM constraints — the timeline is more aggressive than initially assessed.",
            "Revised encrypted key handling from 'attempt decryption' to 'flag for manual review' after considering keymgmt SME's security principles (least privilege, no passphrase handling in automated tools).",
            "Downgraded GHSA/CVE integration from 'medium' to 'low' risk after recognizing it enhances ecosystem value but does not affect core detection accuracy — it's a usability feature, not a functional requirement."
          ],
          "unused_expertise": [
            "security_network SME's TLS/DTLS handshake analysis and certificate chain validation — relevant for detecting quantum-vulnerable certificates in config files, but not directly addressed by the mission questions. Recommend adding certificate scanning as a Phase 2 feature.",
            "security_smartcontract SME's on-chain signature verification and upgradeable contract patterns — relevant for blockchain-specific scanning (Solidity ecrecover usage), but outside the scope of the current CLI tool mission. Recommend a separate 'pq-discovery-solidity' plugin.",
            "security_zk SME's zk-SNARK/zk-STARK quantum vulnerability analysis — relevant for detecting Groth16 usage (quantum-vulnerable) vs. STARK usage (quantum-resistant), but not covered by the mission questions. Recommend adding ZKP-specific detection rules in Phase 3."
          ],
          "aha_moments": [
            "SARIF custom taxonomy as a de facto standard: By defining a PQC-specific rule taxonomy and embedding it in SARIF output, 'pq-discovery' could become the reference implementation for cryptographic migration reporting — other tools would adopt the same rule IDs and severity levels for interoperability.",
            "CNSA 2.0 as a binary filter for government contracts: The distinction between Level III (commercial) and Level V (CNSA 2.0) is not just a performance tradeoff but a contract eligibility criterion. The tool's context detection directly impacts whether a contractor can bid on DoD projects.",
            "Encrypted keys as a compliance gap: Most cryptographic inventories (NIST SP 800-227, CISA guidance) assume plaintext key inspection. Encrypted PKCS#8 files create a blind spot that undermines inventory completeness. Flagging them explicitly (rather than skipping silently) is a differentiator.",
            "GitHub Security Advisories as a coordination mechanism: Private GHSA reporting enables coordinated migration across the open-source ecosystem — maintainers can be notified of quantum vulnerabilities without public disclosure, allowing time for migration before the threat materializes."
          ]
        }
      },
      {
        "cluster": "systems",
        "key_contribution": "Building 'pq-discovery' requires careful performance engineering for large-scale code scanning, strategic parallelization to balance CPU and I/O bottlenecks, and hardware-aware optimization to achieve CI/CD-acceptable scan times. The tool must handle 10K+ file monorepos with sub-minute scan times through intelligent caching, incremental Git-based rescanning, and parallel tree-sitter parsing that saturates modern multi-core CPUs without thrashing I/O subsystems.",
        "critical_findings": [
          "Tree-sitter parsing provides structural AST analysis but carries 10-100x overhead vs. regex for simple pattern matching. For 1M+ LOC monorepos, full tree-sitter parsing of all files would take 5-30 mi"
        ],
        "deliberation_notes": {
          "revisions_made": [
            "Elevated tree-sitter vs. regex performance from a language-level implementation detail to a systems-level architecture decision with quantified scan time targets.",
            "Clarified that the parallelization strategy is not rayon OR tokio but a hybrid architecture exploiting both, with rationale grounded in I/O vs. CPU bottleneck analysis.",
            "Reframed vendor directory handling from a policy question ('should we scan?') to a risk/usability tradeoff with a specific recommendation (default-skip-with-opt-in).",
            "Downgraded Git integration implementation choice (libgit2-rs vs. subprocess) from a critical decision to a pragmatic tradeoff favoring simplicity for MVP."
          ],
          "unused_expertise": [
            "systems_hardware: Hardware acceleration (FPGA/ASIC for tree-sitter parsing, NVMe queue depth tuning) was not leveraged because the tool targets general-purpose developer hardware, not specialized infrastructure. However, this expertise could inform future optimizations for datacenter-scale scanning (scanning 1M+ file monorepos in <1 minute).",
            "systems_distributed: Distributed scanning across multiple machines was not addressed because the mission focuses on single-machine CLI tool design. However, this expertise could inform Phase 3 features: distributed scanning for monorepos too large for single-machine processing, or federated scanning across microservice repositories."
          ],
          "aha_moments": [
            "The two-phase scanning architecture (regex filter + tree-sitter parse) is not just a performance optimization but a fundamental architectural pattern that resolves the accuracy/speed tradeoff across the entire tool. This pattern appears in the pqsecurity cluster's recommendation for pluggable scanners, the languages cluster's tree-sitter integration, and this cluster's performance analysis — it's the unifying design principle.",
            "Incremental Git scanning is not just a performance feature but a compliance data foundation. The cached scan results (commit hash + findings) enable the security cluster's compliance reporting requirements (CNSA 2.0 readiness tracking, regression detection). This cross-cluster connection was not explicit in any single cluster's analysis but emerges from synthesis.",
            "The hybrid tokio + rayon architecture exploits a fundamental property of the workload: I/O and CPU bottlenecks are separable and can be optimized independently. This is a systems-level insight that the languages cluster (focused on Rust idioms) and pqsecurity cluster (focused on detection accuracy) could not surface alone."
          ]
        }
      },
      {
        "cluster": "crypto",
        "key_contribution": "Building 'pq-discovery' requires precise cryptographic primitive detection across certificate formats (ASN.1/DER parsing for OID extraction), configuration files (OpenSSL/GnuTLS cipher suite syntax), and keystore formats (JKS/PKCS#12 metadata extraction). The tool must distinguish between production algorithm usage and test vectors, map detected classical primitives to NIST PQC replacements with security-level equivalence, and provide hash-based signature detection patterns while recognizing that XMSS/LMS are rare in production codebases—making SLH-DSA the first practical stateless hash-based deployment target.",
        "critical_findings": [
          "RSA, ECDSA, and EdDSA keys in X.509 certificates and PKCS#8 private keys are encoded using ASN.1 DER with algorithm-specific OIDs in the AlgorithmIdentifier field. For SubjectPublicKeyInfo: the algori"
        ],
        "deliberation_notes": {
          "revisions_made": [
            "Integrated pqsecurity cluster's OID database structure (three-tier: classical/PQC-standard/PQC-experimental) into certificate parsing recommendations",
            "Incorporated security cluster's encrypted key handling guidance (flag for manual review, opt-in decryption) into PKCS#8 and PKCS#12 parsing",
            "Applied languages cluster's tree-sitter-based filtering strategy to config file parsing (OpenSSL/GnuTLS) and code-level hash-based signature detection",
            "Used systems cluster's two-phase scanning architecture (fast filter + deep analysis) to meet performance requirements (sub-60s for 10K files)",
            "Refined hash-based signature detection to distinguish XMSS/LMS (stateful, rare) from SLH-DSA (stateless, FIPS 205 standard, first practical deployment)",
            "Added cross-domain insight linking certificate OID extraction (crypto domain) to CNSA 2.0 compliance reporting (security domain) and performance constraints (systems domain)"
          ],
          "unused_expertise": [
            "crypto_protocol's detailed TLS 1.3 handshake structure and hybrid key exchange protocol design—relevant for understanding TLS config recommendations but not directly needed for config file parsing (which is syntax-focused, not protocol-focused)",
            "crypto_lattice's ML-KEM/ML-DSA parameter selection mathematics (NTT optimization, error distribution, concrete security estimates)—important for understanding why ML-KEM-768 is recommended for P-256 replacement, but the tool's detection logic only needs the OID-to-algorithm mapping, not the underlying lattice mathematics",
            "crypto_threshold_mpc's threshold signature protocols (FROST, threshold ML-DSA)—not relevant for static code scanning (threshold protocols are runtime constructions, not detectable from source code or config files alone)",
            "crypto_signatures' detailed BLS aggregation and post-quantum aggregation analysis—relevant for understanding why PQC migration is hard for blockchain consensus, but not directly applicable to the pq-discovery tool (which scans existing codebases, not consensus protocols)"
          ],
          "aha_moments": [
            "The root cause of config file parsing complexity is the lack of standardization: OpenSSL uses IN"
          ]
        }
      },
      {
        "cluster": "blockchain",
        "key_contribution": "Building 'pq-discovery' for blockchain contexts requires distinguishing between protocol-layer cryptographic usage (consensus, P2P networking, light clients) and application-layer usage (smart contracts, EOA signatures, governance). The tool must recognize that blockchain's quantum vulnerability is structural—signature schemes are embedded in transaction formats, VM precompiles, and consensus protocols—not just configuration. Risk stratification must account for upgrade mechanisms: immutable EOAs are critical risk, upgradeable contracts are high risk with mitigation paths, and consensus-layer primitives require coordinated hard forks.",
        "critical_findings": [
          "Blockchain usage of secp256k1 (Bitcoin/Ethereum EOAs) and Ed25519 (Solana/Polkadot transactions) has fundamentally different risk profiles than TLS/PKI. In TLS, a quantum attacker must break the ephem",
          "Smart contract languages expose cryptographic primitives through three mechanisms: (1) EVM precompiles at fixed addresses (ecrecover at 0x01, BN254 pairing at 0x08, KZG point evaluation at 0x0A), (2) ",
          "Major blockchain ecosystems have divergent PQC migration timelines and priorities. Ethereum: EF research is focused on consensus-layer migration (BLS aggregate signatures → STARK-compressed ML-DSA for",
          "On-chain governance and multisig contracts have asymmetric quantum risk compared to EOA signatures. A compromised EOA can only steal that account's funds. A compromised governance key or multisig sign",
          "Light clients (Ethereum sync committee, Bitcoin SPV) and cross-chain bridges are quantum-vulnerable through their reliance on aggregate signatures and Merkle proofs. Ethereum's light client protocol u"
        ],
        "deliberation_notes": {
          "revisions_made": [
            "Elevated governance and multisig risk from HIGH to CRITICAL after analyzing systemic impact—a single compromised multisig can drain an entire protocol, affecting all users, not just the keyholder",
            "Clarified that Falcon should NOT be recommended for blockchain despite compact signatures—floating-point arithmetic creates side-channel vulnerabilities unacceptable for validator and wallet software running on diverse hardware",
            "Distinguished between protocol-layer quantum vulnerabilities (consensus, P2P, light clients—require hard forks) and application-layer vulnerabilities (smart contracts, EOAs—can be mitigated via account abstraction and contract upgrades)",
            "Added explicit detection patterns for cross-chain bridges after recognizing they are the highest-value quantum targets (billions in TVL, single point of failure for cross-chain ecosystems)"
          ],
          "unused_expertise": [
            "Node discovery and peer management (blockchain_node_infra) — DHT poisoning and peer identity attacks are relevant but not directly applicable to a source code scanning tool. These are runtime network attacks, not detectable via static analysis.",
            "MEV extraction and fair ordering (blockchain_tx_mev) — MEV-Boost, Flashbots, and encrypted mempools are relevant to quantum front-running mitigation but are infrastructure-level solutions, not detectable in source code. The tool can flag mempool-exposed signatures but cannot detect MEV infrastructure usage.",
            "Storage backends (blockchain_node_infra) — Quantum impact on Merkle Patricia Tries and Verkle tries is important for protocol design but not relevant to a code scanner detecting cryptographic primitive usage.",
            "Validator economics (blockchain_consensus) — Slashing conditions and staking mechanisms are not directly quantum-vulnerable (they depend on signature security, which is already covered). Economic incentives for quantum attack are important context but not a scanning target."
          ],
          "aha_moments": [
            "The mempool exposure window creates a novel 'quantum front-running' attack vector unique to blockchain: an attacker can extract the public key from a pending transaction's signature, derive the private key via Shor's algorithm, and submit a competing transaction within the block time (12 seconds for Ethereum). This attack is impossible in TLS because session keys are ephemeral and not publicly broadcast before use. The tool must explain this temporal dimension when flagging blockchain signatures as CRITICAL.",
            "Account abstraction (ERC-4337) is the single most important quantum mitigation infrastructure for Ethereum—it decouples account identity from signature algorithm, allowing users to migrate from ECDSA to ML-DSA without changing addresses. However, this only helps users who proactively migrate BEFORE quantum computers break ECDSA. EOAs (externally owned accounts) have no migration path—they are permanently bound to ECDSA. The tool should strongly recommend ERC-4337 smart account deployment for all new users.",
            "STARK-based signature aggregation (from consensus SME) solves the post-quantum consensus scalability problem but creates a new centralization vector: the aggregator role requires GPU hardware and becomes analogous to MEV block builders. This is a fundamental trade-off—quantum resistance requires computational proof generation, which concentrates in specialized nodes. The tool cannot detect this (it's an infrastructure concern), but the recommendation engine should note that STARK aggregation is the only viable path for large validator sets.",
            "Cross-chain bridges are asymmetric quantum targets: the attacker doesn't need to compromise the source chain, only the bridge's signature verification on the destination chain. A forged guardian signature on Ethereum can mint unbacked USDC on Solana, draining Solana's liquidity even though Ethereum itself is uncompromised. This makes bridges higher priority than individual EOAs—one bridge compromise affects an entire ecosystem.",
            "The absence of formal PQC migration plans in Bitcoin and Solana is a critical gap. Ethereum has a roadmap (hybrid BLS+ML-DSA → STARK aggregation), but Bitcoin has no consensus on post-quantum address formats, and Solana has no public timeline. The tool should flag this in its compliance reporting: 'No ecosystem-level PQC migration plan detected—advocate for protocol governance to prioritize quantum resistance roadmap'."
          ]
        }
      }
    ],
    "investigation_narrative": [
      {
        "cluster": "pqsecurity",
        "round": 1,
        "questions_asked": [
          "What are the exact FIPS 203/204/205 parameter set mappings for RSA-2048, RSA-4096, ECC P-256, P-384, secp256k1, Ed25519, and X25519? Include security level equivalences (NIST Level 1/3/5).",
          "What are the current best practices for hybrid constructions during the PQC transition period? Should the tool recommend X25519+ML-KEM-768 or flag X25519 alone as vulnerable?",
          "What are the ASN.1 OIDs for ML-KEM, ML-DSA, and SLH-DSA in X.509 certificates and PKCS structures? Are there draft RFCs for PQC certificate formats?",
          "What are the known side-channel vulnerabilities in ML-KEM and ML-DSA implementations that a static analysis tool should warn about (e.g., non-constant-time NTT, masked vs. unmasked implementations)?"
        ],
        "domain_summary": "Building 'pq-discovery' requires precise algorithm-to-PQC mappings (NIST security levels), hybrid construction best practices for transition recommendations, ASN.1/OID knowledge for certificate scanning, and side-channel awareness for implementation quality warnings. The tool must balance migration pragmatism (hybrid during transition) with long-term quantum resistance (pure PQ endpoints), while providing actionable guidance across diverse language ecosystems and deployment contexts.",
        "priority_actions": [
          "Implement NIST security level mapping engine: map detected classical algorithms (RSA-2048, P-256, etc.) to equivalent PQC parameter sets (ML-KEM-512/768/1024, ML-DSA-44/65/87) with risk-based defaults (Level III for new deployments, Level I acceptable for non-HNDL short-lived keys)",
          "Build hybrid construction recommendation logic: default to X25519+ML-KEM-768 for TLS/VPN/SSH, flag pure classical as quantum-vulnerable, provide pure PQ as an option with backward compatibility warnings",
          "Create OID database with three tiers (classical, PQC-standard, PQC-experimental): parse X.509 SubjectPublicKeyInfo and signatureAlgorithm fields, map OIDs to algorithm names, flag experimental OIDs for migration to FIPS 203/204/205/206",
          "Implement static analysis for side-channel vulnerabilities: detect non-constant-time NTT (secret-indexed arrays, data-dependent branches), variable-time rejection sampling, non-constant-time ciphertext comparison, absence of masking, floating-point usage in FN-DSA",
          "Design pluggable scanner architecture: language-specific scanners (Rust, Go, Python, Java, C/C++, JS/TS) as separate modules, config format scanners (OpenSSL, GnuTLS, Java KeyStore, YAML/TOML), certificate scanners (PEM, DER, PKCS#12), binary scanners (ELF, Mach-O, PE for compiled crypto libraries)",
          "Define JSON output schema with compliance context: include detected algorithm, file path, line number, risk level (critical/high/medium/low), recommended replacement, rationale (why this recommendation), compliance notes (CNSA 2.0 deadline, FIPS 140-3 requirements), side-channel warnings (if applicable)",
          "Plan phased implementation: Phase 1 (MVP): Rust/Go/Python source scanners + OpenSSL config scanner + PEM certificate scanner, basic algorithm detection (RSA, ECDSA, X25519, Ed25519), Level I/III mapping, JSON output. Phase 2: Java/C/C++ scanners, DER/PKCS#12 certificate support, hybrid recommendation logic, side-channel static analysis. Phase 3: JavaScript/TypeScript scanners, binary crypto library detection, CI/CD integration (GitHub Actions, GitLab CI), OID database auto-update, compliance report generation (CNSA 2.0 readiness score, FIPS 140-3 gap analysis)"
        ]
      },
      {
        "cluster": "languages",
        "round": 1,
        "questions_asked": [
          "For Rust: What are the primary cryptographic crates (ring, rustls, openssl, sodiumoxide, ed25519-dalek, k256, p256) and how do they expose algorithm selection? Are there AST patterns for detecting KeyPair::generate() calls with specific curves?",
          "For Go: How does crypto/rsa, crypto/ecdsa, crypto/ed25519, and golang.org/x/crypto expose algorithm parameters? Can we detect tls.Config cipher suite selection?",
          "For Python: What are the detection patterns for cryptography, pycryptodome, and pyOpenSSL? How do we distinguish between algorithm usage and test vectors in unittest/pytest files?",
          "For C/C++: How do we detect OpenSSL EVP_PKEY_CTX_new_id(EVP_PKEY_RSA) and mbedTLS API calls? What are the challenges with macro-heavy codebases?",
          "What are the best Rust libraries for AST parsing of each language? Is tree-sitter mature enough for production use, or should we use language-native parsers (syn for Rust, go/parser for Go)?"
        ],
        "domain_summary": "Building 'pq-discovery' requires language-specific AST parsing strategies, cryptographic API detection patterns across diverse ecosystems, and careful handling of FFI boundaries where quantum-vulnerable primitives hide. The tool must distinguish between algorithm usage (production code), test vectors (test fixtures), and configuration (deployment manifests) while providing actionable migration guidance grounded in each language's PQC library maturity and toolchain capabilities.",
        "priority_actions": [
          "Implement two-phase scanning architecture: Phase 1 (fast filter with tree-sitter): scan all files, identify files importing crypto libraries, output a filtered file list. Phase 2 (deep analysis with language-native parsers): parse filtered files with syn (Rust), go/parser (Go), ast (Python), libclang (C/C++), solc (Solidity), extract algorithm usage with line numbers and context.",
          "Build language-specific scanner modules as separate Rust crates: `pq_scanner_rust` (uses syn), `pq_scanner_go` (execs go/parser or uses cgo bindings"
        ]
      },
      {
        "cluster": "security",
        "round": 1,
        "questions_asked": [
          "What is the SARIF 2.1.0 schema for representing cryptographic vulnerabilities? Are there existing SARIF taxonomies for PQC migration (e.g., CWE extensions)?",
          "What are the CNSA 2.0 compliance requirements for key exchange and signature algorithms? Does the tool need to distinguish between NSA/DoD contexts and general commercial use?",
          "How should the tool handle encrypted private keys (PKCS#8 with AES-256-CBC)? Should we attempt passphrase-based decryption, or flag as 'unknown algorithm, manual review required'?",
          "What are the recommended practices for reporting cryptographic vulnerabilities in open-source projects? Should the tool integrate with GitHub Security Advisories or CVE databases?"
        ],
        "domain_summary": "Building 'pq-discovery' requires integrating standards-driven compliance frameworks (SARIF 2.1.0, CNSA 2.0), practical key management heuristics (encrypted PKCS#8 handling, HSM detection), and ecosystem-aware vulnerability reporting (GitHub Security Advisories, CVE integration). The tool must balance automated detection with manual review triggers, provide context-aware recommendations (NSA/DoD vs. commercial), and serve as both a technical scanner and a compliance documentation generator.",
        "priority_actions": [
          "Implement SARIF 2.1.0 output format with custom PQC taxonomy (rule IDs, severity levels, fix suggestions). This is the highest-leverage integration, enabling GitHub Code Scanning, IDE plugins, and CI/CD dashboards.",
          "Build CNSA 2.0 context detection and compliance reporting: scan for NSA/DoD markers, apply Level V recommendations, generate timeline-based compliance summaries. The 2025 software signing deadline makes this urgent for government contractors.",
          "Design encrypted PKCS#8 handling: detect encryption algorithm and KDF, flag as 'manual review required', provide guidance on quantum-resistant key wrapping. Include opt-in '--decrypt-keys' mode for inventory purposes.",
          "Implement GitHub Security Advisory integration: '--ghsa-report' mode to generate private advisory drafts, cross-reference with CVE databases, provide SECURITY.md template generator.",
          "Create a phased implementation plan: Phase 1 (MVP): SARIF output, CNSA 2.0 detection, encrypted key flagging. Phase 2: GHSA integration, CVE cross-referencing, Dependabot configuration. Phase 3: Private report generation, cost/timeline estimation, migration project management features."
        ]
      },
      {
        "cluster": "systems",
        "round": 1,
        "questions_asked": [
          "What is the performance profile of tree-sitter parsing vs. regex scanning for large monorepos (10K+ files, 1M+ LOC)? What are acceptable scan times for CI/CD integration?",
          "How should the tool handle symbolic links, submodules, and vendored dependencies? Should we scan vendor/ directories by default or require explicit opt-in?",
          "What is the optimal parallelization strategy for directory tree traversal? Should we use rayon for file-level parallelism or tokio for async I/O?",
          "How do we implement incremental scanning with Git integration? Should we use libgit2-rs to compute diffs, or shell out to git diff --name-only?"
        ],
        "domain_summary": "Building 'pq-discovery' requires careful performance engineering for large-scale code scanning, strategic parallelization to balance CPU and I/O bottlenecks, and hardware-aware optimization to achieve CI/CD-acceptable scan times. The tool must handle 10K+ file monorepos with sub-minute scan times through intelligent caching, incremental Git-based rescanning, and parallel tree-sitter parsing that saturates modern multi-core CPUs without thrashing I/O subsystems.",
        "priority_actions": [
          "Implement two-phase scanning architecture (regex filter + tree-sitter parse) with AST caching to achieve <60 second scan times for 10K file repos. This is the critical path for CI/CD viability.",
          "Build hybrid parallelization (tokio for I/O, rayon for CPU) with adaptive concurrency tuning. Profile with perf stat to validate 90%+ CPU utilization during parse phase.",
          "Implement incremental Git scanning via git diff subprocess with result caching in .git/pq-discovery-cache.json. This reduces typical CI scan time by 90-99%.",
          "Design symlink cycle detection, default-skip vendor directories with --scan-vendor override, and .pq-discovery-ignore file support. This balances security thoroughness with developer usability.",
          "Establish performance benchmarks: 10K files in <60s, 100K files in <10 minutes on 16-core/NVMe reference hardware. Publish benchmark suite for community validation and regression detection."
        ]
      },
      {
        "cluster": "crypto",
        "round": 1,
        "questions_asked": [
          "What are the ASN.1 structures for RSA, ECDSA, and EdDSA keys in PKCS#8 and SubjectPublicKeyInfo? How do we extract algorithm OIDs and key sizes from DER-encoded data?",
          "What are the common OpenSSL and GnuTLS configuration file formats for cipher suite selection? Are there standard parsers, or do we need custom regex?",
          "How do Java KeyStore (JKS) and PKCS#12 files encode algorithm information? Do we need to parse the entire keystore or just extract metadata?",
          "What are the detection patterns for hash-based signatures (XMSS, LMS) in existing codebases? Are these already in use, or is SLH-DSA the first practical deployment?"
        ],
        "domain_summary": "Building 'pq-discovery' requires precise cryptographic primitive detection across certificate formats (ASN.1/DER parsing for OID extraction), configuration files (OpenSSL/GnuTLS cipher suite syntax), and keystore formats (JKS/PKCS#12 metadata extraction). The tool must distinguish between production algorithm usage and test vectors, map detected classical primitives to NIST PQC replacements with security-level equivalence, and provide hash-based signature detection patterns while recognizing that XMSS/LMS are rare in production codebases—making SLH-DSA the first practical stateless hash-based deployment target.",
        "priority_actions": [
          "Implement DER/ASN.1 parser for X.509 certificates and PKCS#8 keys using der-parser or x509-parser crate. Build OID database with three tiers (classical vulnerable, PQC standard, PQC experimental). Extract algorithm OIDs from SubjectPublicKeyInfo and PrivateKeyInfo, map to algorithm names and key sizes, flag quantum-vulnerable algorithms (RSA, ECDSA, ECDH, EdDSA). Recommend NIST PQC replacements: ML-KEM-768 for P-256/secp256k1/X25519, ML-KEM-1024 for P-384, ML-DSA-65 for ECDSA/EdDSA, SLH-DSA-SHA2-128s for conservative fallback.",
          "Implement regex-based scanner for OpenSSL and GnuTLS config files. Detect CipherString, Ciphersuites, Protocol, SignatureAlgorithms directives. Parse cipher suite names (e.g., ECDHE-RSA-AES128-GCM-SHA256) into components (key exchange, authentication, encryption, hash). Flag ECDHE, RSA, ECDSA as quantum-vulnerable. Flag TLS 1.2 and earlier as high risk. Provide guidance on TLS 1.3 hybrid PQ/T key exchange (X25519Kyber768) and PQC signature algorithms (ML-DSA in TLS 1.3 extensions).",
          "Implement JKS and PKCS#12 parsers using keystore-rs/jks-rs and p12 crate. Extract certificate chains (available without password) and parse X.509 certificates to extract algorithm OIDs. For encrypted private keys, detect encryption algorithm and flag weak algorithms (3DES, RC2). Provide --decrypt-keystores flag for opt-in password-based decryption. Output keystore entry details: alias, entry type, certificate algorithm, private key algorithm (if decrypted), encryption algorithm, risk level, recommendations.",
          "Implement hash-based signature detection: OID-based detection for XMSS (RFC 8391), LMS (RFC 8554), SLH-DSA (draft-ietf-lamps-sphincs-plus-certificates). Code-level detection using tree-sitter or regex for function names (XMSS_sign, LMS_sign, sphincs_sign) and library imports (xmss, hash-sigs, liboqs, pqcrypto-sphincsplus). Flag XMSS/LMS as stateful (state management warning). Flag SLH-DSA as quantum-safe (no migration needed). Distinguish test code from production using file path heuristics.",
          "Build comprehensive OID database with auto-update mechanism. Include: (1) Classical algorithms: RSA (all key sizes), ECDSA (all curves: P-256, P-384, P-521, secp256k1, brainpool), ECDH (X25519, X448, all NIST curves), EdDSA (Ed25519, Ed448), DSA. (2) PQC standards: ML-KEM (FIPS 203 OIDs for 512/768/1024), ML-DSA (FIPS 204 OIDs for 44/65/87), SLH-DSA (FIPS 205 OIDs for all parameter sets), FN-DSA (FIPS 206 OIDs for 512/1024). (3) PQC experimental: Kyber/Dilithium pre-standard OIDs, FrodoKEM, BIKE, HQC, Classic McEliece. (4) Hash functions: SHA-1 (deprecated), SHA-2 (SHA-256/384/512), SHA-3 (SHA3-256/384/512), BLAKE2, BLAKE3. (5) Symmetric: AES (128/192/256), 3DES (deprecated), ChaCha20. Fetch OID updates from IANA registry and NIST PQC project repositories.",
          "Design JSON output schema with detailed findings: {file_path, line_number (for code), algorithm_detected, algorithm_category (signature/key_exchange/encryption/hash), key_size (if applicable), risk_level (critical/high/medium/low), quantum_vulnerability (yes/no), recommended_replacement (algorithm name + parameter set), rationale (why this recommendation), compliance_notes (CNSA 2.0 deadline, FIPS 140-3 requirements), migration_complexity (low/medium/high), additional_context (e.g., 'stateful scheme—state management required', 'encrypted key—manual review needed')}. Include summary statistics: total files scanned, total findings, breakdown by risk level, breakdown by algorithm category.",
          "Implement phased scanner architecture: Phase 1 (fast filter): tree-sitter-based scan to identify files importing crypto libraries or containing crypto-related keywords (RSA, ECDSA, AES, SHA, etc.). Output filtered file list. Phase 2 (deep analysis): parse filtered files with language-native parsers (syn for Rust, go/parser for Go, ast for Python) or DER parsers (for certificates/keys). Extract algorithm usage with line numbers and context. Phase 3 (aggregation): produce JSON report with findings, summary statistics, and prioritized recommendations. This two-phase approach (from systems cluster) achieves sub-60s scan time for 10K file repos."
        ]
      },
      {
        "cluster": "blockchain",
        "round": 1,
        "questions_asked": [
          "How should the tool handle secp256k1 (Bitcoin/Ethereum) and Ed25519 (Solana/Polkadot) in blockchain contexts? Is the risk profile different from TLS/PKI usage?",
          "What are the current PQC migration plans for major blockchain ecosystems? Should the tool recommend specific blockchain-compatible PQC schemes (e.g., Falcon for compact signatures)?",
          "How do smart contract languages (Solidity, Rust for Solana) expose cryptographic primitives? Are there precompiles or opcodes we need to detect?",
          "Should the tool flag quantum-vulnerable signature schemes in on-chain governance or multisig contracts as higher risk than EOA signatures?"
        ],
        "domain_summary": "Building 'pq-discovery' for blockchain contexts requires distinguishing between protocol-layer cryptographic usage (consensus, P2P networking, light clients) and application-layer usage (smart contracts, EOA signatures, governance). The tool must recognize that blockchain's quantum vulnerability is structural—signature schemes are embedded in transaction formats, VM precompiles, and consensus protocols—not just configuration. Risk stratification must account for upgrade mechanisms: immutable EOAs are critical risk, upgradeable contracts are high risk with mitigation paths, and consensus-layer primitives require coordinated hard forks.",
        "priority_actions": [
          "Implement blockchain-specific signature context detection: distinguish EOA transaction signing (CRITICAL, no upgrade path) from smart contract ecrecover usage (HIGH, upgradeable via proxy patterns) from consensus validator signing (CRITICAL, requires hard fork). Use AST analysis to determine if a contract is upgradeable (EIP-1967/2535 patterns) and adjust risk level accordingly.",
          "Build EVM precompile scanner: detect calls to ecrecover (0x01), BN254 pairing (0x06-0x08), and other quantum-vulnerable precompiles. For Solidity, parse AST for ecrecover function calls and low-level calls to precompile addresses. For bytecode-only contracts, disassemble and scan for CALL/STATICCALL to precompile addresses. Flag ecrecover usage in access control contexts (onlyOwner, require(ecrecover(...) == authorized)) as CRITICAL.",
          "Implement governance and multisig detection with timelock analysis: scan for OpenZeppelin Governor, Gnosis Safe, Compound Timelock patterns. Extract timelock duration from contract storage or constructor parameters. Risk stratification: no timelock → CRITICAL, <7 days → CRITICAL, ≥7 days → HIGH. Provide recommendation: 'Migrate to threshold ML-DSA with minimum 7-day timelock during transition period'.",
          "Build light client and bridge signature verification detector: scan for BLS12-381 aggregate signature verification (Ethereum sync committee), cross-chain message signature verification (Wormhole, LayerZero, IBC). Parse contract source or bytecode to identify signature verification logic. Flag as CRITICAL with note: 'Light client/bridge compromise enables systemic theft—prioritize migration to post-quantum signatures'.",
          "Create blockchain-specific recommendation engine: for EOAs, recommend ERC-4337 smart account migration with ML-DSA-65 validation; for upgradeable contracts, recommend adding ML-DSA signature verification via contract upgrade; for non-upgradeable contracts, recommend redeployment with quantum-safe architecture; for consensus layer, recommend hybrid BLS+ML-DSA with STARK aggregation roadmap; for bridges, recommend threshold ML-DSA for small validator sets, STARK-aggregated ML-DSA for large sets.",
          "Implement Solana/Polkadot/Move VM scanner: for Solana, detect solana_program::secp256k1_recover and ed25519_verify syscalls in Rust smart contracts; for Polkadot, detect sp_io::crypto signature verification in Substrate pallets; for Move (Aptos/Sui), detect Ed25519 transaction signing (VM-level, not contract-level). Flag as HIGH with note: 'Requires runtime upgrade—coordinate with chain governance'.",
          "Generate blockchain-specific compliance reports: include CNSA 2.0 deadline compliance for governance contracts (2025 software signing deadline), bridge TVL-weighted risk assessment (prioritize highest-TVL bridges), consensus-layer migration timeline (map to CRQC-5 years → CRQC window from pqsecurity cluster), and ecosystem-specific migration guidance (Ethereum: ERC-4337 + EIP for ML-DSA precompile; Bitcoin: P2PQC BIP; Solana: runtime upgrade proposal)."
        ]
      }
    ],
    "cross_domain_risks": [
      "Risk-based recommendations: the tool must tailor its output to the deployment context (Internet-facing vs internal, IoT vs server, compliance-driven vs best-effort). A single 'one size fits all' recommendation (e.g., always ML-KEM-1024) is suboptimal.",
      "Transition pragmatism: hybrid constructions are the bridge from classical to PQ. The tool should default to hybrid recommendations (X25519+ML-KEM-768) for maximum compatibility, with pure PQ as an opt-in for greenfield deployments.",
      "Compliance-driven urgency: CNSA 2.0, PCI DSS 4.0, FedRAMP timelines create hard deadlines (2025-2033). The tool should surface these deadlines in its output: 'RSA-2048 is non-compliant with CNSA 2.0 Phase 3 (2030). Migrate by 2028 to allow 2-year testing/validation buffer.'",
      "Implementation quality matters as much as algorithm choice: a perfectly secure algorithm (ML-KEM-768) becomes insecure if implemented with timing leaks or without masking. The tool must address both 'what algorithm' and 'how to implement it safely.'",
      "Extensibility and maintenance: PQC standards are still evolving (FIPS 206 not finalized, draft RFCs for hybrid TLS, composite certificates). The tool's architecture must support plugin-based scanners and updatable OID/algorithm databases to stay current.",
      "FFI boundaries as vulnerability propagation points: Rust, Go, and Python crypto libraries often wrap C implementations (liboqs, PQClean). A vulnerability or algorithm choice in the C layer affects all wrapping languages. The tool must scan C dependencies in multi-language projects.",
      "Test code vs. production code disambiguation: All languages have test code with crypto operations (test vectors, KAT verification). The tool must distinguish these from production usage to avoid overwhelming users with false positives. Heuristics: function naming conventions (test_ prefix), decorators (@pytest.mark), hardcoded byte literals, file naming (kat.txt, test_vectors.json).",
      "Macro expansion and preprocessor handling: C/C++ and (to a lesser extent) Rust use macros extensively. Algorithm selection is often via macro constants (EVP_PKEY_RSA, NID_secp256k1). Parsers that do not expand macros (tree-sitter) will miss these. Language-native parsers (libclang, syn) handle macro expansion.",
      "Type-based vs. runtime algorithm selection: Rust encodes algorithm choice in types (compile-time), Go uses explicit parameters (call-site), C/C++ uses runtime dispatch (EVP_PKEY_assign). Detection strategies must match the language's paradigm.",
      "Precompile dependency for on-chain PQC: Solidity contracts cannot implement PQC verification in pure Solidity due to gas costs (SHAKE-256 problem, per lang_solidity SME). Migration requires EVM client upgrades to add PQC precompiles. The tool should flag this dependency in its output.",
      "Standards-driven design: SARIF 2.1.0, CNSA 2.0, NIST SP 800-227, and FIPS 203/204/205/206 provide the normative framework for detection rules, severity assessment, and recommendation logic. The tool is not just a scanner but a compliance documentation generator.",
      "Context-aware recommendations: NSA/DoD vs. commercial, production vs. test code, on-chain vs. off-chain, encrypted vs. plaintext keys — each context requires different risk assessment and migration guidance. One-size-fits-all recommendations undermine credibility.",
      "Ecosystem integration as a force multiplier: SARIF output enables GitHub Code Scanning integration, GHSA reporting enables coordinated disclosure, CVE cross-referencing enables holistic vulnerability assessment. The tool's value increases superlinearly with the number of integrations.",
      "Manual review as a feature, not a bug: Encrypted keys, ambiguous algorithm detection, and context-specific recommendations all require human judgment. The tool should explicitly flag 'manual review required' cases with detailed guidance, rather than attempting (and failing) full automation.",
      "Performance vs. Accuracy Tradeoff: Every design decision (tree-sitter vs. regex, full scan vs. incremental, vendor directory handling) involves trading thoroughness for speed. The two-phase architecture, default-skip-with-opt-in for vendors, and incremental Git scanning all resolve this tradeoff by providing both: fast defaults for CI/CD integration, thorough modes for security audits.",
      "Hardware-Aware Optimization: The parallelization strategy (tokio + rayon hybrid) and tree-sitter caching directly exploit modern hardware characteristics (NVMe high queue depth, multi-core CPUs, fast filesystem caches). The tool's performance scales with hardware investment, making it viable for both laptop development and datacenter CI/CD.",
      "Rust's Zero-Cost Abstractions Enable Performance: The hybrid async/sync architecture (tokio + rayon), zero-copy parsing (tree-sitter integration), and compile-time optimization (no runtime overhead for abstraction layers) are only practical in Rust. Equivalent Python or JavaScript implementations would be 10-100x slower, making CI/CD integration impractical.",
      "OID database as central infrastructure: All certificate, keystore, and config file scanning depends on a comprehensive OID-to-algorithm mapping database. This database must cover classical algorithms (RSA, ECDSA, ECDH, EdDSA with all curve OIDs), PQC standards (ML-KEM/ML-DSA/SLH-DSA with FIPS 203/204/205 OIDs), and experimental PQC (pre-standard Kyber/Dilithium OIDs, FrodoKEM, BIKE, HQC). The database should be versioned and auto-updatable (fetch from IANA or NIST repositories).",
      "Encrypted data handling: PKCS#8 PrivateKeyInfo, PKCS#12 keystores, and some config files may contain encrypted keys. The tool must detect encryption algorithms (flag weak algorithms like 3DES, RC2) and provide opt-in decryption (--decrypt-keys flag) rather than failing or blocking on password prompts. This aligns with the security cluster's encrypted key handling guidance.",
      "Stateful vs. stateless PQC: XMSS and LMS (stateful hash-based signatures) require state management, making them unsuitable for most blockchain and wallet applications. SLH-DSA (stateless) is the practical hash-based alternative. ML-DSA and FN-DSA (lattice-based) are also stateless. The tool should flag stateful schemes with warnings about state management risks.",
      "Test code vs. production code: Hash-based signatures, experimental PQC libraries, and test vectors appear frequently in test directories but rarely in production. The tool should use file path heuristics (test/, tests/, spec/, examples/, vendor/) to filter test code and reduce false positives. Provide a --scan-test-code flag to include test directories for completeness.",
      "Performance vs. security trade-offs: SLH-DSA offers maximum quantum resistance (hash-based security) but has large signatures (7-30 KB) and slow signing (50-3000 ms). ML-DSA offers smaller signatures (2-5 KB) and faster signing (0.3 ms) but relies on lattice assumptions. FN-DSA offers the smallest signatures (0.7-1.3 KB) but has complex floating-point implementation. The tool should provide context-aware recommendations based on application constraints (bandwidth, latency, security margin).",
      "Upgradeability is the critical architectural property for quantum resistance—immutable contracts and EOAs are permanent vulnerabilities, while proxy patterns and account abstraction provide migration paths",
      "Mempool exposure creates a novel quantum attack vector (front-running via key extraction) that is unique to blockchain and absent in TLS/PKI—detection tools must account for this temporal dimension",
      "Signature aggregation is the unsolved problem blocking practical post-quantum consensus—STARK-based compression is the most viable path but requires GPU acceleration and creates centralization risk in the aggregator role",
      "Governance and bridge contracts are systemic risk concentrators—a single quantum attack can compromise an entire protocol or cross-chain ecosystem, making these higher priority than individual EOA migration",
      "Blockchain's cryptographic dependencies are structural (embedded in transaction formats, VM opcodes, consensus protocols) rather than configurational (like TLS cipher suites)—migration requires protocol-level changes (hard forks, EIPs) not just config updates"
    ],
    "implementation_roadmap": [
      {
        "phase": "Immediate — Critical Risk Mitigation",
        "objectives": [
          "Implement pattern-based static analysis for constant-time violations: detect array indexing with secret-dependent indices, data-dependent branches in crypto loops, non-constant-time comparison functions (memcmp, strcmp) on secret data. Output: 'Potential timing leak: array index depends on secret polynomial coefficient at line X. Use constant-time table lookup or eliminate table.'",
          "Implement blockchain-specific detection patterns: flag secp256k1 in transaction signing contexts (web3.eth.accounts.signTransaction, ethers.Wallet.signTransaction, bitcoin-core RPC signrawtransaction) as CRITICAL with note 'mempool exposure + irreversible transfer'",
          "Implement governance contract detection: scan for OpenZeppelin's Governor contracts, Gnosis Safe multisig patterns, Compound's Timelock, and custom governance implementations. Flag any contract with 'admin', 'owner', or 'governance' roles that use ecrecover for signature validation",
          "Detect Ethereum light client implementations: scan for sync committee signature verification (BLS12-381 aggregate signature verification), beacon chain API usage (eth/v1/beacon/light_client/*), and Altair light client protocol code. Flag as CRITICAL with note 'BLS12-381 aggregate signatures are quantum-vulnerable—light client security breaks under quantum attack'",
          "Implement a risk-based recommendation engine: for detected RSA-2048/ECC-256, default to ML-KEM-768 (not 512) with a note that Level I (ML-KEM-512) is acceptable for non-HNDL data with <10 year confidentiality requirements"
        ],
        "deliverables": [
          "Address: Implement pattern-based static analysis for constant-time violations: detect arr",
          "Address: Implement blockchain-specific detection patterns: flag secp256k1 in transaction ",
          "Address: Implement governance contract detection: scan for OpenZeppelin's Governor contra",
          "Address: Detect Ethereum light client implementations: scan for sync committee signature ",
          "Address: Implement a risk-based recommendation engine: for detected RSA-2048/ECC-256, def"
        ]
      },
      {
        "phase": "Implementation — Core Recommendations",
        "objectives": [
          "**Two-phase scanning is the unifying architectural pattern**: Phase 1 uses regex/ripgrep to filter files importing crypto libraries (10,000+ files/second), Phase 2 applies language-native AST parsers to filtered files only (tree-sitter for initial filtering, syn/go/parser/libclang for deep analysis). This achieves <60s scan time for 10K files while maintaining accuracy. Cache parsed ASTs in ~/.cache/pq-discovery/ keyed by file hash to avoid re-parsing unchanged files.",
          "**NIST PQC mappings must be context-aware**: RSA-2048/ECC-256 → ML-KEM-768 + ML-DSA-65 (NIST Level III) is the default for commercial use and CNSA 2.0 compliance. RSA-4096/ECC-384 → ML-KEM-1024 + ML-DSA-87 (Level V) for NSA/DoD contexts (detect via keywords, file paths, or --cnsa-mode flag). Blockchain contexts require FN-DSA-512 over ML-DSA-65 for transaction signing (5x smaller signatures critical for on-chain costs). Hybrid constructions (X25519+ML-KEM-768) are default for TLS/VPN/SSH; pure PQ is opt-in for greenfield deployments.",
          "**Blockchain signatures are CRITICAL risk, distinct from TLS/PKI**: secp256k1 (Bitcoin/Ethereum EOAs) and Ed25519 (Solana/Polkadot) have three compounding vulnerabilities absent in TLS: (1) permanent on-chain public key exposure, (2) mempool visibility enabling quantum front-running within 12-second Ethereum block time, (3) irreversible value transfer with no post-compromise remediation. Governance/multisig contracts are CRITICAL+ (systemic risk—one compromised multisig drains entire protocol). EOAs have no upgrade path; recommend ERC-4337 smart account migration with ML-DSA-65 validation.",
          "**Certificate and keystore parsing requires DER/ASN.1 expertise**: Use der-parser or x509-parser crate for X.509 SubjectPublicKeyInfo and PKCS#8 PrivateKeyInfo parsing. Build a three-tier OID database: classical vulnerable (RSA, ECDSA, ECDH with all curve OIDs including secp256k1), PQC standard (ML-KEM/ML-DSA/SLH-DSA with FIPS 203/204/205 OIDs), PQC experimental (pre-standard Kyber/Dilithium OIDs). For PKCS#12 keystores, extract certificates without password; flag encrypted private keys for manual review. For JKS, use keystore-rs to parse proprietary format.",
          "**SARIF 2.1.0 output is the highest-leverage CI/CD integration**: Define custom PQC taxonomy (rule IDs: PQC-RSA-2048, PQC-ECDSA-P256, etc.) with severity levels (error for RSA-2048/ECDSA in production, warning for RSA-4096, note for test code). Include result.fixes with language-specific code replacements. This enables GitHub Code Scanning, IDE plugins, and enterprise SAST dashboards. CNSA 2.0 compliance reporting should surface deadlines: 2025 for software signing, 2030 for exclusive PQC requirement."
        ],
        "deliverables": [
          "Implement: **Two-phase scanning is the unifying architectural pattern**: Phase 1 uses regex",
          "Implement: **NIST PQC mappings must be context-aware**: RSA-2048/ECC-256 → ML-KEM-768 + M",
          "Implement: **Blockchain signatures are CRITICAL risk, distinct from TLS/PKI**: secp256k1 (B",
          "Implement: **Certificate and keystore parsing requires DER/ASN.1 expertise**: Use der-parse",
          "Implement: **SARIF 2.1.0 output is the highest-leverage CI/CD integration**: Define custom "
        ]
      }
    ],
    "remaining_uncertainties": [
      "**PQC precompile standardization timeline for EVM**: Ethereum has no EIP for ML-DSA or ML-KEM precompiles as of 2025. The tool recommends ML-DSA-65 for smart contract signature verification, but without a precompile, on-chain verification is gas-prohibitive (~10M gas for a single ML-DSA verification due to SHAKE-256 cost). Migration path depends on EIP process and client implementation timeline—monitor ethereum/EIPs repository for PQC precompile proposals.",
      "**Optimal tree-sitter vs. language-native parser trade-off for JavaScript/TypeScript**: tree-sitter-javascript is mature, but JavaScript's dynamic nature (algorithm selection via runtime variables, getattr-style dynamic dispatch) makes static analysis inherently less accurate than for Rust/Go. Investigate whether tree-sitter is sufficient or if a TypeScript compiler API integration (ts.createProgram) provides better type resolution for crypto library usage detection.",
      "**Handling of macro-heavy C/C++ codebases where algorithm selection is build-time conditional**: libclang provides preprocessor expansion, but build-time conditionals (#ifdef OPENSSL_NO_EC) mean the tool must either parse all branches (reporting findings with build context) or integrate with the build system (CMake, Makefile) to determine active configuration. The latter is more accurate but significantly more complex—recommend Phase 1 parse-all-branches, Phase 2 build-system integration if needed.",
      "**Encrypted PKCS#8 and PKCS#12 private key algorithm detection without password**: Certificates in keystores are extractable without passwords, but private key algorithm OIDs are inside the encrypted payload. The tool flags these for manual review, but this creates an inventory gap. Investigate whether PKCS#8 EncryptedPrivateKeyInfo or PKCS#12 SafeBag metadata includes algorithm hints (e.g., key size in the encryption parameters) that could narrow the algorithm space without full decryption.",
      "**STARK-based signature aggregation proving time and hardware requirements for blockchain consensus**: The consensus cluster identified STARK aggregation (512 ML-DSA signatures → 200 KB proof in <4 seconds on GPU) as the path to post-quantum consensus scalability. However, GPU requirement creates centralization risk (aggregator role analogous to MEV builders). The tool cannot detect this infrastructure concern, but the recommendation engine should note the trade-off. Monitor StarkWare and Polygon Zero research for proving time improvements and CPU-based alternatives.",
      "**Cross-language FFI boundary detection**: Rust, Go, and Python crypto libraries often wrap C implementations (liboqs, PQClean). A vulnerability in the C layer propagates to all wrapping languages. The tool should scan C dependencies in vendor/ or third_party/ directories, but detecting FFI usage (Rust's extern \"C\", Go's cgo, Python's ctypes/cffi) and tracing it to the underlying C library requires whole-program analysis. Recommend Phase 1 scan vendored C code separately, Phase 2 implement FFI call graph analysis if needed.",
      "**Quantum-safe migration timeline for Bitcoin and Solana**: Bitcoin has no consensus on post-quantum address formats (P2PQC BIP proposals exist but no activation timeline). Solana has no public PQC roadmap. The tool can flag quantum-vulnerable usage but cannot provide ecosystem-specific migration guidance without official timelines. Recommend monitoring Bitcoin-dev mailing list and Solana governance forum for PQC proposals."
    ],
    "clusters_consulted": [
      "pqsecurity",
      "languages",
      "security",
      "systems",
      "crypto",
      "blockchain"
    ],
    "conclusion": "Building 'pq-discovery' requires a two-phase scanning architecture (regex filter + tree-sitter/native AST parsing) to achieve sub-60-second scan times for 10K-file repositories while maintaining detection accuracy across Rust, Go, Python, C/C++, Solidity, and configuration formats. The tool must implement precise NIST PQC mappings (RSA-2048 → ML-KEM-768 + ML-DSA-65 for CNSA 2.0 baseline, with context-aware recommendations for blockchain vs. TLS/PKI), SARIF 2.1.0 output for CI/CD integration, and incremental Git-based scanning to reduce typical CI scan time by 90-99%. Critical architectural decisions: use language-native parsers (syn for Rust, go/parser for Go, libclang for C/C++) over tree-sitter for deep analysis; implement hybrid tokio+rayon parallelization to saturate NVMe I/O and multi-core CPUs; maintain a three-tier OID database (classical vulnerable, PQC standard, PQC experimental) with auto-update capability; and stratify risk by deployment context—blockchain EOAs and governance contracts are CRITICAL (mempool exposure + irreversible transfer), TLS/PKI is HIGH, test code is LOW.",
    "listener_comparison": {
      "independent_analysis": [
        {
          "cluster": "pqsecurity",
          "questions": [
            "What are the exact FIPS 203/204/205 parameter set mappings for RSA-2048, RSA-4096, ECC P-256, P-384, secp256k1, Ed25519, and X25519? Include security level equivalences (NIST Level 1/3/5).",
            "What are the current best practices for hybrid constructions during the PQC transition period? Should the tool recommend X25519+ML-KEM-768 or flag X25519 alone as vulnerable?",
            "What are the ASN.1 OIDs for ML-KEM, ML-DSA, and SLH-DSA in X.509 certificates and PKCS structures? Are there draft RFCs for PQC certificate formats?",
            "What are the known side-channel vulnerabilities in ML-KEM and ML-DSA implementations that a static analysis tool should warn about (e.g., non-constant-time NTT, masked vs. unmasked implementations)?"
          ],
          "analysis": "The parameter set mappings are well-defined in FIPS standards: RSA-2048 and ECC P-256/Ed25519 provide approximately 128-bit security (NIST Level 1), mapping to ML-KEM-768 and ML-DSA-65. RSA-4096 and P-384 provide approximately 192-bit security (NIST Level 3), mapping to ML-KEM-1024 and ML-DSA-87. Secp256k1 (Bitcoin curve) also provides 128-bit security. X25519 is 128-bit security for key exchange. For hybrid constructions, the current best practice is to recommend dual-mode operation (e.g., X25519+ML-KEM-768) rather than flagging X25519 alone as vulnerable, since PQC algorithms are still maturing and hybrid provides defense-in-depth. The tool should detect both components and recommend the hybrid. ASN.1 OIDs for PQC algorithms are being standardized through IETF drafts (draft-ietf-lamps-dilithium-certificates, draft-ietf-lamps-kyber-certificates). ML-KEM OID space is under 2.16.840.1.101.3.4.4.x, ML-DSA under similar NIST-controlled space. Side-channel concerns for static analysis are limited - the tool can flag usage of non-constant-time implementations if it detects specific vulnerable library versions, but cannot perform runtime analysis. The tool should warn when detecting older/unaudited PQC library versions.",
          "key_findings": [
            {
              "area": "Parameter Set Mapping",
              "assessment": "Clear 1:1 mappings exist between classical and PQC algorithms based on security levels. Tool should use NIST security level as primary classification dimension.",
              "risk_level": "low",
              "confidence": "high"
            },
            {
              "area": "Hybrid Construction Policy",
              "assessment": "Hybrid mode is transitional best practice. Tool should recommend hybrids, not flag classical algorithms as immediately vulnerable when used in hybrid context.",
              "risk_level": "medium",
              "confidence": "high"
            },
            {
              "area": "PQC Certificate Standards",
              "assessment": "OID assignments are in draft stage. Tool must track evolving standards and update OID detection logic as RFCs finalize.",
              "risk_level": "medium",
              "confidence": "medium"
            },
            {
              "area": "Side-Channel Detection Limits",
              "assessment": "Static analysis cannot detect runtime side-channels. Tool should focus on library version warnings and known-vulnerable implementation detection.",
              "risk_level": "low",
              "confidence": "high"
            }
          ],
          "risk_assessments": [
            "OID assignments for PQC certificates may change before RFC finalization, requiring tool updates",
            "Recommending immature PQC implementations could introduce new vulnerabilities worse than quantum risk",
            "Hybrid construction detection requires parsing both algorithm components, doubling false positive surface"
          ]
        },
        {
          "cluster": "languages",
          "questions": [
            "For Rust: What are the primary cryptographic crates (ring, rustls, openssl, sodiumoxide, ed25519-dalek, k256, p256) and how do they expose algorithm selection? Are there AST patterns for detecting KeyPair::generate() calls with specific curves?",
            "For Go: How does crypto/rsa, crypto/ecdsa, crypto/ed25519, and golang.org/x/crypto expose algorithm parameters? Can we detect tls.Config cipher suite selection?",
            "For Python: What are the detection patterns for cryptography, pycryptodome, and pyOpenSSL? How do we distinguish between algorithm usage and test vectors in unittest/pytest files?",
            "For C/C++: How do we detect OpenSSL EVP_PKEY_CTX_new_id(EVP_PKEY_RSA) and mbedTLS API calls? What are the challenges with macro-heavy codebases?",
            "What are the best Rust libraries for AST parsing of each language? Is tree-sitter mature enough for production use, or should we use language-native parsers (syn for Rust, go/parser for Go)?"
          ],
          "analysis": "Language-specific detection requires multi-layered approach. For Rust, crates expose algorithms through type parameters (e.g., RsaKeyPair, EcdsaKeyPair in ring) and builder patterns. AST detection via syn crate can identify use statements and function calls with type annotations. For Go, crypto packages use explicit algorithm constants (rsa.GenerateKey, ecdsa.P256()) making detection straightforward via go/parser. Python's cryptography library uses backend.generate_rsa_private_key(key_size=2048) patterns detectable via ast module. C/C++ is most challenging due to preprocessor macros and function pointers - requires both AST parsing (libclang) and heuristic pattern matching. Test vector distinction requires context analysis: check if code is in test directories, imports testing frameworks, or uses fixture/mock patterns. Tree-sitter is production-ready and provides unified interface across languages, but language-native parsers offer deeper semantic analysis. Hybrid approach recommended: tree-sitter for initial broad scanning, native parsers for high-confidence verification of findings. Critical insight: algorithm selection often happens at runtime via configuration, not compile-time - tool must scan both code and config files to achieve reasonable coverage.",
          "key_findings": [
            {
              "area": "Rust Detection Accuracy",
              "assessment": "Type system and explicit crate imports make Rust easiest language for accurate detection. Syn crate provides full semantic analysis.",
              "risk_level": "low",
              "confidence": "high"
            },
            {
              "area": "C/C++ Detection Challenges",
              "assessment": "Macro expansion and dynamic dispatch make C/C++ hardest target. Will have highest false negative rate without runtime analysis.",
              "risk_level": "high",
              "confidence": "high"
            },
            {
              "area": "Test Vector False Positives",
              "assessment": "Distinguishing test vectors from production code requires heuristic path analysis and import detection. Cannot be 100% accurate.",
              "risk_level": "medium",
              "confidence": "medium"
            },
            {
              "area": "Parser Strategy",
              "assessment": "Tree-sitter provides good baseline with unified API. Native parsers needed for high-confidence findings in production CI/CD.",
              "risk_level": "medium",
              "confidence": "high"
            }
          ],
          "risk_assessments": [
            "C/C++ macro-heavy codebases may require preprocessor expansion, dramatically increasing scan time",
            "Runtime algorithm selection via config files creates blind spot if tool only scans source code",
            "Language-native parser dependencies increase binary size and compilation complexity for cross-language tool"
          ]
        },
        {
          "cluster": "security",
          "questions": [
            "What is the SARIF 2.1.0 schema for representing cryptographic vulnerabilities? Are there existing SARIF taxonomies for PQC migration (e.g., CWE extensions)?",
            "What are the CNSA 2.0 compliance requirements for key exchange and signature algorithms? Does the tool need to distinguish between NSA/DoD contexts and general commercial use?",
            "How should the tool handle encrypted private keys (PKCS#8 with AES-256-CBC)? Should we attempt passphrase-based decryption, or flag as 'unknown algorithm, manual review required'?",
            "What are the recommended practices for reporting cryptographic vulnerabilities in open-source projects? Should the tool integrate with GitHub Security Advisories or CVE databases?"
          ],
          "analysis": "SARIF 2.1.0 provides result.kind, result.level, and result.properties for extensible vulnerability metadata. No standard CWE exists for 'quantum-vulnerable cryptography' - tool should define custom taxonomy in result.taxa and reference it in findings. SARIF's result.fixes can encode PQC migration recommendations. CNSA 2.0 mandates ML-KEM-768 minimum for key exchange and ML-DSA-65 for signatures by 2030 for NSS/DoD systems. Tool should support policy profiles (--profile cnsa2.0 vs --profile commercial) with different risk scoring. Encrypted private keys present security dilemma: attempting decryption requires passphrase input, which is dangerous in CI/CD (credential exposure in logs). Better approach: flag encrypted keys with 'algorithm unknown, manual inspection required' and provide separate offline utility for passphrase-protected key analysis. GitHub Security Advisories integration requires GHSA-* identifier format and severity scoring (CVSS). Tool should output SARIF for GitHub Security tab integration, but not auto-create advisories (requires human review). CVE assignment is inappropriate for architectural vulnerabilities vs implementation bugs.",
          "key_findings": [
            {
              "area": "SARIF Schema Extension",
              "assessment": "SARIF is flexible enough for PQC findings. Custom taxonomy needed since CWE lacks quantum-specific categories.",
              "risk_level": "low",
              "confidence": "high"
            },
            {
              "area": "CNSA 2.0 Policy Profiles",
              "assessment": "Policy-based risk scoring is essential. Government/defense users have different urgency than commercial users.",
              "risk_level": "medium",
              "confidence": "high"
            },
            {
              "area": "Encrypted Key Handling",
              "assessment": "Passphrase-based decryption in CI/CD is security anti-pattern. Flag-and-defer approach is safest.",
              "risk_level": "high",
              "confidence": "high"
            },
            {
              "area": "Vulnerability Reporting Integration",
              "assessment": "SARIF output enables GitHub Security tab. Auto-creating advisories is overreach - requires human judgment.",
              "risk_level": "low",
              "confidence": "high"
            }
          ],
          "risk_assessments": [
            "Custom SARIF taxonomy may not be recognized by all SARIF consumers, reducing tool interoperability",
            "Policy profile selection requires user understanding of their threat model - defaults may be inappropriate",
            "Flagging encrypted keys as 'unknown' creates manual review burden without providing actionable guidance"
          ]
        },
        {
          "cluster": "systems",
          "questions": [
            "What is the performance profile of tree-sitter parsing vs. regex scanning for large monorepos (10K+ files, 1M+ LOC)? What are acceptable scan times for CI/CD integration?",
            "How should the tool handle symbolic links, submodules, and vendored dependencies? Should we scan vendor/ directories by default or require explicit opt-in?",
            "What is the optimal parallelization strategy for directory tree traversal? Should we use rayon for file-level parallelism or tokio for async I/O?",
            "How do we implement incremental scanning with Git integration? Should we use libgit2-rs to compute diffs, or shell out to git diff --name-only?"
          ],
          "analysis": "Tree-sitter parsing is 10-100x slower than regex but dramatically reduces false positives. For CI/CD, target <5 minutes for 100K LOC, <30 minutes for 1M LOC. Hybrid approach: regex pre-filter to identify candidate files, then tree-sitter for semantic verification. Symbolic links should be followed with cycle detection (track visited inodes). Submodules should be scanned by default (they're part of the build), but vendored dependencies should require opt-in (--scan-vendor flag) since they're typically unmodifiable third-party code. Parallelization strategy depends on workload: rayon for CPU-bound parsing (tree-sitter), tokio for I/O-bound file reading. Optimal approach: tokio for directory traversal and file reading, rayon threadpool for parsing. Incremental scanning via libgit2-rs is more robust than shelling out (handles worktree state, doesn't require git binary). Compute diff against merge-base of current branch and main/master, scan only modified files plus their dependency graph (if file A imports B, and B changes, rescan A).",
          "key_findings": [
            {
              "area": "Parsing Performance Trade-off",
              "assessment": "Tree-sitter accuracy justifies performance cost for production use. Regex pre-filtering can reduce parse workload by 90%+.",
              "risk_level": "medium",
              "confidence": "high"
            },
            {
              "area": "Vendored Dependency Handling",
              "assessment": "Scanning vendor/ by default creates noise (unactionable findings). Opt-in is better UX but may miss real vulnerabilities.",
              "risk_level": "medium",
              "confidence": "medium"
            },
            {
              "area": "Parallelization Architecture",
              "assessment": "Hybrid tokio/rayon approach maximizes throughput. Pure async or pure thread-pool leaves performance on table.",
              "risk_level": "low",
              "confidence": "high"
            },
            {
              "area": "Incremental Scanning Complexity",
              "assessment": "Dependency graph analysis for incremental scanning is complex. Initial version should scan all files, add incremental in v2.",
              "risk_level": "high",
              "confidence": "medium"
            }
          ],
          "risk_assessments": [
            "Regex pre-filtering may miss complex algorithm usage patterns, creating false negatives",
            "Following symbolic links without cycle detection causes infinite loops on misconfigured repos",
            "Incremental scanning with dependency tracking requires language-specific import analysis, multiplying implementation complexity"
          ]
        },
        {
          "cluster": "crypto",
          "questions": [
            "What are the ASN.1 structures for RSA, ECDSA, and EdDSA keys in PKCS#8 and SubjectPublicKeyInfo? How do we extract algorithm OIDs and key sizes from DER-encoded data?",
            "What are the common OpenSSL and GnuTLS configuration file formats for cipher suite selection? Are there standard parsers, or do we need custom regex?",
            "How do Java KeyStore (JKS) and PKCS#12 files encode algorithm information? Do we need to parse the entire keystore or just extract metadata?",
            "What are the detection patterns for hash-based signatures (XMSS, LMS) in existing codebases? Are these already in use, or is SLH-DSA the first practical deployment?"
          ],
          "analysis": "ASN.1 structures use AlgorithmIdentifier with OID and optional parameters. RSA uses OID 1.2.840.113549.1.1.1 with NULL parameters. ECDSA uses 1.2.840.10045.2.1 with curve OID in parameters (P-256 is 1.2.840.10045.3.1.7). EdDSA uses 1.3.101.112 (Ed25519) or 1.3.101.113 (Ed448). Key size for RSA is in the modulus field of RSAPublicKey structure, for ECC it's implicit in the curve OID. DER parsing requires robust ASN.1 decoder - use der crate or x509-parser in Rust. OpenSSL config uses SSLCipherSuite directive with colon-separated cipher names (e.g., 'ECDHE-RSA-AES256-GCM-SHA384'). GnuTLS uses priority strings with different syntax. No standard parsers - require custom regex with cipher suite database mapping names to algorithms. JKS and PKCS#12 store certificates and keys with metadata - can extract algorithm info from certificate chain without decrypting private keys. Use keystore-rs or rustls-pemfile. Hash-based signatures (XMSS, LMS) are rare in current deployments - primarily in firmware signing and high-security government contexts. SLH-DSA is first standardized stateless hash-based signature, making it more practical. Tool should detect XMSS/LMS but prioritize RSA/ECDSA detection.",
          "key_findings": [
            {
              "area": "ASN.1 Parsing Robustness",
              "assessment": "DER parsing is security-critical and error-prone. Must use well-tested library (x509-parser) rather than custom implementation.",
              "risk_level": "high",
              "confidence": "high"
            },
            {
              "area": "Config File Format Diversity",
              "assessment": "OpenSSL and GnuTLS use incompatible config syntaxes. Cipher suite name mapping requires extensive database of algorithm associations.",
              "risk_level": "high",
              "confidence": "high"
            },
            {
              "area": "Keystore Metadata Extraction",
              "assessment": "Can extract algorithm info without decrypting keys, avoiding passphrase handling complexity.",
              "risk_level": "low",
              "confidence": "high"
            },
            {
              "area": "Hash-Based Signature Prevalence",
              "assessment": "XMSS/LMS detection is low-priority edge case. Focus on RSA/ECDSA/EdDSA which dominate real-world usage.",
              "risk_level": "low",
              "confidence": "medium"
            }
          ],
          "risk_assessments": [
            "Malformed DER data could crash parser or cause incorrect algorithm detection",
            "Cipher suite name variations across TLS libraries create false negative risk (e.g., 'ECDHE-RSA' vs 'TLS_ECDHE_RSA')",
            "PKCS#12 files with non-standard encryption may not be parseable without passphrase"
          ]
        },
        {
          "cluster": "blockchain",
          "questions": [
            "How should the tool handle secp256k1 (Bitcoin/Ethereum) and Ed25519 (Solana/Polkadot) in blockchain contexts? Is the risk profile different from TLS/PKI usage?",
            "What are the current PQC migration plans for major blockchain ecosystems? Should the tool recommend specific blockchain-compatible PQC schemes (e.g., Falcon for compact signatures)?",
            "How do smart contract languages (Solidity, Rust for Solana) expose cryptographic primitives? Are there precompiles or opcodes we need to detect?",
            "Should the tool flag quantum-vulnerable signature schemes in on-chain governance or multisig contracts as higher risk than EOA signatures?"
          ],
          "analysis": "Blockchain cryptography has fundamentally different risk profile than TLS/PKI. Secp256k1 and Ed25519 in blockchain contexts protect long-lived assets (cryptocurrency holdings, NFTs) with no key rotation mechanism - once quantum computers break ECDLP, all historical addresses become vulnerable. This is HIGHER risk than TLS (ephemeral session keys, regular certificate rotation). However, blockchain PQC migration is ecosystem-level coordination problem, not individual project decision. Tool should flag blockchain signature usage as 'critical risk, ecosystem migration required' rather than recommending specific replacements. Current PQC migration plans: Bitcoin has no formal plan (BIP process stalled), Ethereum researching STARKs and lattice-based schemes, Solana/Polkadot exploring Falcon and Dilithium. Tool should NOT recommend specific blockchain PQC schemes since no consensus exists - instead, link to ecosystem-specific migration discussions. Smart contract detection: Solidity uses ecrecover precompile (opcode 0x01) for signature verification - detectable via bytecode analysis or source code pattern matching. Rust for Solana uses ed25519-dalek crate. On-chain governance and multisig contracts are HIGHER risk than EOA signatures because they control protocol upgrades and treasury funds - quantum attack on governance could compromise entire chain. Tool should assign 'critical' risk level to governance contract signature verification.",
          "key_findings": [
            {
              "area": "Blockchain Risk Profile",
              "assessment": "Blockchain cryptography has higher quantum risk than TLS due to asset permanence and lack of key rotation. Requires distinct risk scoring.",
              "risk_level": "critical",
              "confidence": "high"
            },
            {
              "area": "Ecosystem Migration Coordination",
              "assessment": "PQC migration is ecosystem-level problem. Tool cannot recommend individual project changes - must defer to chain-wide governance.",
              "risk_level": "high",
              "confidence": "high"
            },
            {
              "area": "Smart Contract Detection",
              "assessment": "Solidity ecrecover and Rust ed25519 crate usage are detectable. Bytecode analysis needed for deployed contracts.",
              "risk_level": "medium",
              "confidence": "high"
            },
            {
              "area": "Governance Contract Priority",
              "assessment": "On-chain governance signature verification is critical infrastructure. Quantum attack could compromise entire protocol.",
              "risk_level": "critical",
              "confidence": "high"
            }
          ],
          "risk_assessments": [
            "Blockchain projects may ignore tool warnings since individual migration is impossible without ecosystem coordination",
            "Recommending specific PQC schemes for blockchains could mislead users if ecosystem adopts different standard",
            "Bytecode analysis for deployed contracts requires EVM/WASM decompilation, dramatically increasing complexity"
          ]
        }
      ],
      "independent_synthesis": {
        "executive_summary": "The pq-discovery tool faces a fundamental tension between detection accuracy and implementation complexity. Language-specific AST parsing provides high accuracy but multiplies maintenance burden across 6+ languages. The blockchain domain reveals a critical insight: quantum vulnerability risk is context-dependent, not algorithm-dependent - the same Ed25519 usage has different urgency in TLS (medium risk, key rotation possible) vs blockchain (critical risk, no rotation mechanism). The tool's value proposition depends on solving the false positive problem through semantic analysis, not just pattern matching.",
        "key_conclusions": [
          "Hybrid detection strategy (regex pre-filter + tree-sitter verification) balances performance and accuracy for CI/CD integration under 5-minute scan time constraint",
          "Policy-based risk scoring (CNSA 2.0 vs commercial profiles) is essential because quantum risk urgency varies by domain and threat model",
          "Blockchain cryptography requires distinct risk assessment framework - ecosystem coordination problem, not individual project migration",
          "Encrypted key handling should flag-and-defer rather than attempt decryption to avoid credential exposure in CI/CD pipelines",
          "SARIF output with custom taxonomy enables GitHub Security tab integration while maintaining extensibility for future PQC standards evolution"
        ],
        "cross_cutting_insights": [
          "The tool's architecture must distinguish between 'detection' (what algorithms are present) and 'recommendation' (what to do about them). Blockchain domain shows recommendations may be 'defer to ecosystem' rather than 'replace with X'. This suggests a plugin architecture where recommendation logic is separate from detection logic.",
          "Performance requirements (5-minute CI/CD scans) conflict with accuracy requirements (semantic AST analysis). The resolution is staged analysis: fast regex scan for PR checks, deep AST scan for nightly/release builds. This implies two output modes: 'quick' and 'thorough'.",
          "False positive mitigation requires context awareness that spans multiple clusters: language-specific patterns (test vectors in Python unittest), file system context (vendor/ directories), and cryptographic context (hybrid constructions). No single cluster can solve this - requires cross-cutting context propagation in the architecture.",
          "The certificate/config file scanning is orthogonal to source code scanning but shares the same output schema. This suggests a two-tier scanner architecture: 'code scanners' (language-specific) and 'artifact scanners' (certificates, configs, keystores) with a common Finding abstraction.",
          "Risk scoring cannot be purely algorithmic - it requires domain knowledge (blockchain governance vs EOA, TLS session keys vs code signing). This implies the tool needs a risk policy DSL or configuration file where domain experts can encode context-specific risk weights."
        ],
        "investigation_gaps": [
          "No questions address how the tool handles polyglot repositories (e.g., Go backend + JavaScript frontend + Rust crypto library). Does each file get scanned by all language scanners, or is there a file extension routing mechanism?",
          "Missing investigation of how to handle dynamically loaded cryptographic libraries (dlopen in C, FFI in Rust). Static analysis cannot detect runtime algorithm selection via dynamic linking.",
          "No questions about how to handle cryptographic hardware modules (HSMs, TPMs, secure enclaves) that perform signing operations without exposing key material to the filesystem. These are invisible to static analysis but represent significant quantum vulnerability.",
          "Unclear how the tool should handle deprecated-but-not-yet-vulnerable algorithms (e.g., SHA-1 for signatures is broken classically, but not quantum-specific). Should the tool focus exclusively on quantum threats or flag all cryptographic weaknesses?",
          "No investigation of how to handle quantum-resistant algorithms that are already deployed (e.g., hash-based signatures in firmware, McEliece in some VPNs). Tool should recognize these as 'already migrated' rather than flagging as vulnerable.",
          "Missing discussion of how to handle compliance reporting beyond SARIF - does the tool need to generate audit reports for SOC2, FedRAMP, or other compliance frameworks that will eventually require PQC migration evidence?",
          "No questions about internationalization - cryptographic algorithm names and OIDs are universal, but error messages and recommendations need i18n for global adoption. Is this in scope for v1?",
          "Unclear how the tool handles version control history - should it scan only HEAD, or analyze historical commits to identify when vulnerable algorithms were introduced (git blame for cryptographic debt)?"
        ]
      }
    },
    "architect_brief": {
      "version": "1.0",
      "mission_id": "b2bac6c6-86cd-4d76-babc-ab3599c43c53",
      "mission_statement": "Build an open-source Rust CLI tool called 'pq-discovery' that scans a directory tree for usage of RSA-2048, RSA-4096, ECC (secp256k1, P-256, P-384, Ed25519, X25519), and other quantum-vulnerable cryptographic primitives in source code, configuration files, certificates, and key files. For each finding, the tool should recommend the appropriate NIST PQC replacement: ML-KEM-768/1024 for key exchange, ML-DSA-65/87 for signatures, SLH-DSA for conservative hash-based fallback. The tool should output a structured JSON report with file path, line number, detected primitive, risk level, and recommended replacement. Design for extensibility: pluggable scanners for different languages (Rust, Go, Python, Java, C/C++, JavaScript/TypeScript), config formats (OpenSSL, GnuTLS, Java KeyStore), and certificate formats (PEM, DER, PKCS12). Consider integration with CI/CD pipelines (GitHub Actions, GitLab CI). The architect brief should include: complete CLI interface design, scanner architecture, detection heuristics for each language, output schema, and a phased implementation plan.",
      "generated_at": "2026-02-26T17:31:23.100428466+00:00",
      "root_cause_analysis": {
        "problem_statement": "Building 'pq-discovery' requires a two-phase scanning architecture (regex filter + tree-sitter/native AST parsing) to achieve sub-60-second scan times for 10K-file repositories while maintaining detection accuracy across Rust, Go, Python, C/C++, Solidity, and configuration formats. The tool must implement precise NIST PQC mappings (RSA-2048 → ML-KEM-768 + ML-DSA-65 for CNSA 2.0 baseline, with context-aware recommendations for blockchain vs.",
        "underlying_causes": [
          "Static analysis tools can detect certain side-channel vulnerability patterns in source code, though full side-channel resistance requires dynamic testing (TVLA, power analysis).",
          "Blockchain usage of secp256k1 (Bitcoin/Ethereum EOAs) and Ed25519 (Solana/Polkadot transactions) has fundamentally different risk profiles than TLS/PKI.",
          "On-chain governance and multisig contracts have asymmetric quantum risk compared to EOA signatures.",
          "Light clients (Ethereum sync committee, Bitcoin SPV) and cross-chain bridges are quantum-vulnerable through their reliance on aggregate signatures and Merkle proofs.",
          "RSA-2048 and ECC P-256/secp256k1/Ed25519/X25519 map to NIST Security Level I (128-bit classical, ~107-bit quantum): recommend ML-KEM-512 for key exchange, ML-DSA-44 or FN-DSA-512 for signatures.",
          "Current best practice (2025-2030 transition period): hybrid constructions combining classical + PQ provide defense-in-depth.",
          "C/C++ crypto detection targets OpenSSL EVP API and mbedTLS.",
          "Solidity smart contracts use ECDSA (secp256k1) for transaction signatures and may use ecrecover precompile (address 0x01) for on-chain signature verification.",
          "CNSA 2.0 (NSA Commercial National Security Algorithm Suite 2.0) mandates ML-KEM-1024 for key exchange and ML-DSA-87 for signatures in National Security Systems (NSS), with aggressive timelines: software/firmware signing preferred by 2025, required exclusively by 2030; networking equipment preferred by 2026, required by 2030.",
          "Tree-sitter parsing provides structural AST analysis but carries 10-100x overhead vs.",
          "RSA, ECDSA, and EdDSA keys in X.509 certificates and PKCS#8 private keys are encoded using ASN.1 DER with algorithm-specific OIDs in the AlgorithmIdentifier field.",
          "Smart contract languages expose cryptographic primitives through three mechanisms: (1) EVM precompiles at fixed addresses (ecrecover at 0x01, BN254 pairing at 0x08, KZG point evaluation at 0x0A), (2) Solidity's cryptographic builtins (keccak256, sha256, ripemd160, ecrecover), (3) WASM host functions in Substrate/CosmWasm/Near.",
          "Major blockchain ecosystems have divergent PQC migration timelines and priorities."
        ],
        "evidence_sources": [
          "blockchain",
          "crypto",
          "languages",
          "pqsecurity",
          "security",
          "systems"
        ]
      },
      "solution_specifications": [
        {
          "id": "sol_001",
          "title": "Implement pattern-based static analysis for constant-time violations: detect arr",
          "description": "Implement pattern-based static analysis for constant-time violations: detect array indexing with secret-dependent indices, data-dependent branches in crypto loops, non-constant-time comparison functions (memcmp, strcmp) on secret data. Output: 'Potential timing leak: array index depends on secret polynomial coefficient at line X. Use constant-time table lookup or eliminate table.'",
          "priority": "critical",
          "source_cluster": "cross_domain",
          "dependencies": []
        },
        {
          "id": "sol_002",
          "title": "Implement blockchain-specific detection patterns: flag secp256k1 in transaction ",
          "description": "Implement blockchain-specific detection patterns: flag secp256k1 in transaction signing contexts (web3.eth.accounts.signTransaction, ethers.Wallet.signTransaction, bitcoin-core RPC signrawtransaction) as CRITICAL with note 'mempool exposure + irreversible transfer'",
          "priority": "critical",
          "source_cluster": "cross_domain",
          "dependencies": []
        },
        {
          "id": "sol_003",
          "title": "Implement governance contract detection: scan for OpenZeppelin's Governor contra",
          "description": "Implement governance contract detection: scan for OpenZeppelin's Governor contracts, Gnosis Safe multisig patterns, Compound's Timelock, and custom governance implementations. Flag any contract with 'admin', 'owner', or 'governance' roles that use ecrecover for signature validation",
          "priority": "critical",
          "source_cluster": "cross_domain",
          "dependencies": []
        },
        {
          "id": "sol_004",
          "title": "Detect Ethereum light client implementations: scan for sync committee signature ",
          "description": "Detect Ethereum light client implementations: scan for sync committee signature verification (BLS12-381 aggregate signature verification), beacon chain API usage (eth/v1/beacon/light_client/*), and Altair light client protocol code. Flag as CRITICAL with note 'BLS12-381 aggregate signatures are quantum-vulnerable—light client security breaks under quantum attack'",
          "priority": "critical",
          "source_cluster": "cross_domain",
          "dependencies": []
        },
        {
          "id": "sol_005",
          "title": "Implement a risk-based recommendation engine: for detected RSA-2048/ECC-256, def",
          "description": "Implement a risk-based recommendation engine: for detected RSA-2048/ECC-256, default to ML-KEM-768 (not 512) with a note that Level I (ML-KEM-512) is acceptable for non-HNDL data with <10 year confidentiality requirements",
          "priority": "critical",
          "source_cluster": "cross_domain",
          "dependencies": []
        },
        {
          "id": "sol_006",
          "title": "**Two-phase scanning is the unifying architectural pattern**: Phase 1 uses regex",
          "description": "**Two-phase scanning is the unifying architectural pattern**: Phase 1 uses regex/ripgrep to filter files importing crypto libraries (10,000+ files/second), Phase 2 applies language-native AST parsers to filtered files only (tree-sitter for initial filtering, syn/go/parser/libclang for deep analysis). This achieves <60s scan time for 10K files while maintaining accuracy. Cache parsed ASTs in ~/.cache/pq-discovery/ keyed by file hash to avoid re-parsing unchanged files.",
          "priority": "high",
          "source_cluster": "cross_domain",
          "dependencies": []
        },
        {
          "id": "sol_007",
          "title": "**NIST PQC mappings must be context-aware**: RSA-2048/ECC-256 → ML-KEM-768 + ML-",
          "description": "**NIST PQC mappings must be context-aware**: RSA-2048/ECC-256 → ML-KEM-768 + ML-DSA-65 (NIST Level III) is the default for commercial use and CNSA 2.0 compliance. RSA-4096/ECC-384 → ML-KEM-1024 + ML-DSA-87 (Level V) for NSA/DoD contexts (detect via keywords, file paths, or --cnsa-mode flag). Blockchain contexts require FN-DSA-512 over ML-DSA-65 for transaction signing (5x smaller signatures critical for on-chain costs). Hybrid constructions (X25519+ML-KEM-768) are default for TLS/VPN/SSH; pure PQ is opt-in for greenfield deployments.",
          "priority": "high",
          "source_cluster": "cross_domain",
          "dependencies": []
        },
        {
          "id": "sol_008",
          "title": "**Blockchain signatures are CRITICAL risk, distinct from TLS/PKI**: secp256k1 (B",
          "description": "**Blockchain signatures are CRITICAL risk, distinct from TLS/PKI**: secp256k1 (Bitcoin/Ethereum EOAs) and Ed25519 (Solana/Polkadot) have three compounding vulnerabilities absent in TLS: (1) permanent on-chain public key exposure, (2) mempool visibility enabling quantum front-running within 12-second Ethereum block time, (3) irreversible value transfer with no post-compromise remediation. Governance/multisig contracts are CRITICAL+ (systemic risk—one compromised multisig drains entire protocol). EOAs have no upgrade path; recommend ERC-4337 smart account migration with ML-DSA-65 validation.",
          "priority": "high",
          "source_cluster": "cross_domain",
          "dependencies": []
        },
        {
          "id": "sol_009",
          "title": "**Certificate and keystore parsing requires DER/ASN.1 expertise**: Use der-parse",
          "description": "**Certificate and keystore parsing requires DER/ASN.1 expertise**: Use der-parser or x509-parser crate for X.509 SubjectPublicKeyInfo and PKCS#8 PrivateKeyInfo parsing. Build a three-tier OID database: classical vulnerable (RSA, ECDSA, ECDH with all curve OIDs including secp256k1), PQC standard (ML-KEM/ML-DSA/SLH-DSA with FIPS 203/204/205 OIDs), PQC experimental (pre-standard Kyber/Dilithium OIDs). For PKCS#12 keystores, extract certificates without password; flag encrypted private keys for manual review. For JKS, use keystore-rs to parse proprietary format.",
          "priority": "high",
          "source_cluster": "cross_domain",
          "dependencies": []
        },
        {
          "id": "sol_010",
          "title": "**SARIF 2.1.0 output is the highest-leverage CI/CD integration**: Define custom ",
          "description": "**SARIF 2.1.0 output is the highest-leverage CI/CD integration**: Define custom PQC taxonomy (rule IDs: PQC-RSA-2048, PQC-ECDSA-P256, etc.) with severity levels (error for RSA-2048/ECDSA in production, warning for RSA-4096, note for test code). Include result.fixes with language-specific code replacements. This enables GitHub Code Scanning, IDE plugins, and enterprise SAST dashboards. CNSA 2.0 compliance reporting should surface deadlines: 2025 for software signing, 2030 for exclusive PQC requirement.",
          "priority": "high",
          "source_cluster": "cross_domain",
          "dependencies": []
        }
      ],
      "required_capabilities": [
        {
          "solution_id": "sol_001",
          "technologies": [],
          "skills": [
            "cross-domain integration"
          ],
          "libraries": []
        },
        {
          "solution_id": "sol_002",
          "technologies": [
            "secp256k1"
          ],
          "skills": [
            "cross-domain integration"
          ],
          "libraries": []
        },
        {
          "solution_id": "sol_003",
          "technologies": [
            "Go",
            "go"
          ],
          "skills": [
            "cross-domain integration"
          ],
          "libraries": []
        },
        {
          "solution_id": "sol_004",
          "technologies": [
            "BLS12"
          ],
          "skills": [
            "cross-domain integration"
          ],
          "libraries": []
        },
        {
          "solution_id": "sol_005",
          "technologies": [
            "ML-KEM-512",
            "ML-KEM-768",
            "RSA"
          ],
          "skills": [
            "cross-domain integration"
          ],
          "libraries": []
        },
        {
          "solution_id": "sol_006",
          "technologies": [
            "go"
          ],
          "skills": [
            "cross-domain integration"
          ],
          "libraries": []
        },
        {
          "solution_id": "sol_007",
          "technologies": [
            "ML-DSA-65",
            "ML-DSA-87",
            "ML-KEM-1024",
            "ML-KEM-768",
            "RSA"
          ],
          "skills": [
            "cross-domain integration"
          ],
          "libraries": []
        },
        {
          "solution_id": "sol_008",
          "technologies": [
            "Ed25519",
            "Go",
            "ML-DSA-65",
            "secp256k1"
          ],
          "skills": [
            "cross-domain integration"
          ],
          "libraries": []
        },
        {
          "solution_id": "sol_009",
          "technologies": [
            "Dilithium",
            "ECDSA",
            "Kyber",
            "ML-DSA",
            "ML-KEM",
            "RSA",
            "SLH-DSA ",
            "secp256k1"
          ],
          "skills": [
            "cross-domain integration"
          ],
          "libraries": []
        },
        {
          "solution_id": "sol_010",
          "technologies": [
            "ECDSA",
            "RSA"
          ],
          "skills": [
            "cross-domain integration"
          ],
          "libraries": []
        }
      ],
      "implementation_scope": {
        "domains_involved": [
          "pqsecurity",
          "languages",
          "security",
          "systems",
          "crypto",
          "blockchain"
        ],
        "complexity_estimate": "critical",
        "estimated_phases": 2,
        "cross_domain_risks": [
          "Risk-based recommendations: the tool must tailor its output to the deployment context (Internet-facing vs internal, IoT vs server, compliance-driven vs best-effort). A single 'one size fits all' recommendation (e.g., always ML-KEM-1024) is suboptimal.",
          "Transition pragmatism: hybrid constructions are the bridge from classical to PQ. The tool should default to hybrid recommendations (X25519+ML-KEM-768) for maximum compatibility, with pure PQ as an opt-in for greenfield deployments.",
          "Compliance-driven urgency: CNSA 2.0, PCI DSS 4.0, FedRAMP timelines create hard deadlines (2025-2033). The tool should surface these deadlines in its output: 'RSA-2048 is non-compliant with CNSA 2.0 Phase 3 (2030). Migrate by 2028 to allow 2-year testing/validation buffer.'",
          "Implementation quality matters as much as algorithm choice: a perfectly secure algorithm (ML-KEM-768) becomes insecure if implemented with timing leaks or without masking. The tool must address both 'what algorithm' and 'how to implement it safely.'",
          "Extensibility and maintenance: PQC standards are still evolving (FIPS 206 not finalized, draft RFCs for hybrid TLS, composite certificates). The tool's architecture must support plugin-based scanners and updatable OID/algorithm databases to stay current."
        ]
      },
      "acceptance_criteria": [
        {
          "solution_id": "sol_001",
          "criteria": [
            "Implement pattern-based static analysis for constant-time violations: detect array indexing with secret-dependent indices, data-dependent branches in crypto loops, non-constant-time comparison functions (memcmp, strcmp) on secret data. Output: 'Potential timing leak: array index depends on secret polynomial coefficient at line X. Use constant-time table lookup or eliminate table.'",
            "Detect absence of masking: scan for known masking library imports (e.g., #include \"masked_poly.h\", use of masked_* function prefixes). If not found in ML-KEM/ML-DSA implementations, warn: 'No masking detected. Implementation is vulnerable to power analysis (DPA/CPA). First-order masking required for FIPS 140-3 Level 3+, second-order for Level 4.'",
            "Flag floating-point usage in FN-DSA: 'Floating-point arithmetic detected in Gaussian sampling (line X). Floating-point operations are side-channel sensitive (rounding, denormals, exceptions leak information). Consider fixed-point or integer-only Gaussian sampling for constant-time execution.'"
          ],
          "risk_if_unmet": "critical"
        },
        {
          "solution_id": "sol_002",
          "criteria": [
            "Implement blockchain-specific detection patterns: flag secp256k1 in transaction signing contexts (web3.eth.accounts.signTransaction, ethers.Wallet.signTransaction, bitcoin-core RPC signrawtransaction) as CRITICAL with note 'mempool exposure + irreversible transfer'",
            "Flag Ed25519 in Solana/Polkadot contexts (@solana/web3.js Keypair.sign, polkadot-js signTransaction) as CRITICAL with note 'sub-second block time increases quantum attack urgency'",
            "Distinguish EOA signatures (externally owned accounts—no upgrade path) from smart contract signatures (upgradeable via ERC-4337 account abstraction). EOAs are CRITICAL, smart contracts are HIGH with mitigation note"
          ],
          "risk_if_unmet": "critical"
        },
        {
          "solution_id": "sol_003",
          "criteria": [
            "Implement governance contract detection: scan for OpenZeppelin's Governor contracts, Gnosis Safe multisig patterns, Compound's Timelock, and custom governance implementations. Flag any contract with 'admin', 'owner', or 'governance' roles that use ecrecover for signature validation",
            "Risk stratification: (1) Immutable governance (no timelock) → CRITICAL, (2) Short timelock (<7 days) → CRITICAL, (3) Long timelock (≥7 days) → HIGH with note 'timelock provides detection window but not cryptographic defense', (4) Governance with quantum-safe signatures → LOW",
            "Recommend minimum 7-day timelock for all governance actions during the quantum transition period (CRQC-5 years → CRQC). This provides a realistic window for community detection and social-layer response. Post-CRQC, timelocks are insufficient—governance must use post-quantum signatures"
          ],
          "risk_if_unmet": "critical"
        },
        {
          "solution_id": "sol_004",
          "criteria": [
            "Detect Ethereum light client implementations: scan for sync committee signature verification (BLS12-381 aggregate signature verification), beacon chain API usage (eth/v1/beacon/light_client/*), and Altair light client protocol code. Flag as CRITICAL with note 'BLS12-381 aggregate signatures are quantum-vulnerable—light client security breaks under quantum attack'",
            "Detect cross-chain bridge contracts: scan for signature verification of external chain state (e.g., Wormhole's verifyVM, LayerZero's validateProof, IBC light client verification). Flag as CRITICAL with note 'bridge guardian/validator signatures are quantum-vulnerable—forged signatures enable unbacked token minting'",
            "Recommend light client migration path: (1) Phase 1: hybrid BLS+ML-DSA sync committee signatures (from consensus SME analysis), (2) Phase 2: STARK-compressed ML-DSA signatures (200 KB proof for 512 validators), (3) Phase 3: pure post-quantum light client protocol. Note that proof size increase (48 bytes → 200 KB) may make light clients impractical for mobile/embedded—recommend investigating ZK-SNARK-based recursive proof compression"
          ],
          "risk_if_unmet": "critical"
        },
        {
          "solution_id": "sol_005",
          "criteria": [
            "Implement a risk-based recommendation engine: for detected RSA-2048/ECC-256, default to ML-KEM-768 (not 512) with a note that Level I (ML-KEM-512) is acceptable for non-HNDL data with <10 year confidentiality requirements",
            "For blockchain contexts (secp256k1 detection), explicitly recommend FN-DSA-512 over ML-DSA-65 due to 5x smaller signatures (critical for on-chain transaction costs) with a warning that FN-DSA has higher implementation complexity",
            "Include a '--conservative' flag that always recommends SLH-DSA for signatures (hash-based, immune to lattice breaks) at the cost of 10-50x larger signatures"
          ],
          "risk_if_unmet": "high"
        },
        {
          "solution_id": "sol_006",
          "criteria": [
            "**Two-phase scanning is the unifying architectural pattern**: Phase 1 uses regex/ripgrep to filter files importing crypto libraries (10,000+ files/second), Phase 2 applies language-native AST parsers to filtered files only (tree-sitter for initial filtering, syn/go/parser/libclang for deep analysis). This achieves <60s scan time for 10K files while maintaining accuracy. Cache parsed ASTs in ~/.cache/pq-discovery/ keyed by file hash to avoid re-parsing unchanged files."
          ],
          "risk_if_unmet": "medium"
        },
        {
          "solution_id": "sol_007",
          "criteria": [
            "**NIST PQC mappings must be context-aware**: RSA-2048/ECC-256 → ML-KEM-768 + ML-DSA-65 (NIST Level III) is the default for commercial use and CNSA 2.0 compliance. RSA-4096/ECC-384 → ML-KEM-1024 + ML-DSA-87 (Level V) for NSA/DoD contexts (detect via keywords, file paths, or --cnsa-mode flag). Blockchain contexts require FN-DSA-512 over ML-DSA-65 for transaction signing (5x smaller signatures critical for on-chain costs). Hybrid constructions (X25519+ML-KEM-768) are default for TLS/VPN/SSH; pure PQ is opt-in for greenfield deployments."
          ],
          "risk_if_unmet": "medium"
        },
        {
          "solution_id": "sol_008",
          "criteria": [
            "**Blockchain signatures are CRITICAL risk, distinct from TLS/PKI**: secp256k1 (Bitcoin/Ethereum EOAs) and Ed25519 (Solana/Polkadot) have three compounding vulnerabilities absent in TLS: (1) permanent on-chain public key exposure, (2) mempool visibility enabling quantum front-running within 12-second Ethereum block time, (3) irreversible value transfer with no post-compromise remediation. Governance/multisig contracts are CRITICAL+ (systemic risk—one compromised multisig drains entire protocol). EOAs have no upgrade path; recommend ERC-4337 smart account migration with ML-DSA-65 validation."
          ],
          "risk_if_unmet": "medium"
        },
        {
          "solution_id": "sol_009",
          "criteria": [
            "**Certificate and keystore parsing requires DER/ASN.1 expertise**: Use der-parser or x509-parser crate for X.509 SubjectPublicKeyInfo and PKCS#8 PrivateKeyInfo parsing. Build a three-tier OID database: classical vulnerable (RSA, ECDSA, ECDH with all curve OIDs including secp256k1), PQC standard (ML-KEM/ML-DSA/SLH-DSA with FIPS 203/204/205 OIDs), PQC experimental (pre-standard Kyber/Dilithium OIDs). For PKCS#12 keystores, extract certificates without password; flag encrypted private keys for manual review. For JKS, use keystore-rs to parse proprietary format."
          ],
          "risk_if_unmet": "medium"
        },
        {
          "solution_id": "sol_010",
          "criteria": [
            "**SARIF 2.1.0 output is the highest-leverage CI/CD integration**: Define custom PQC taxonomy (rule IDs: PQC-RSA-2048, PQC-ECDSA-P256, etc.) with severity levels (error for RSA-2048/ECDSA in production, warning for RSA-4096, note for test code). Include result.fixes with language-specific code replacements. This enables GitHub Code Scanning, IDE plugins, and enterprise SAST dashboards. CNSA 2.0 compliance reporting should surface deadlines: 2025 for software signing, 2030 for exclusive PQC requirement."
          ],
          "risk_if_unmet": "medium"
        }
      ],
      "swarm_improvement_recommendations": {
        "new_agents_needed": [
          {
            "suggested_name": "ethereum_specialist",
            "domain": "ethereum",
            "rationale": "**PQC precompile standardization timeline for EVM**: Ethereum has no EIP for ML-DSA or ML-KEM precompiles as of 2025. The tool recommends ML-DSA-65 fo"
          },
          {
            "suggested_name": "quantum_specialist",
            "domain": "quantum",
            "rationale": "**STARK-based signature aggregation proving time and hardware requirements for blockchain consensus**: The consensus cluster identified STARK aggregat"
          },
          {
            "suggested_name": "quantum_specialist",
            "domain": "quantum",
            "rationale": "**Quantum-safe migration timeline for Bitcoin and Solana**: Bitcoin has no consensus on post-quantum address formats (P2PQC BIP proposals exist but no"
          }
        ],
        "new_clusters_needed": [],
        "library_gaps": [],
        "unused_expertise": [
          "Hybrid domain's detailed TLS handshake size analysis (ClientHello/ServerHello byte counts) — relevant for performance impact warnings but not directly needed for algorithm detection/recommendation logic",
          "Compliance domain's SOC 2, HIPAA, DoD IL4/IL5 sections — useful for future compliance report generation feature but not required for MVP tool design",
          "Migration domain's detailed IoT SUIT manifest and bootloader update challenges — critical for IoT-specific recommendations but the tool's initial scope is source code scanning (IoT firmware scanning is Phase 3+)",
          "Side-channel domain's higher-order DPA and TEMPEST sections — advanced topics beyond static analysis capability (require dynamic testing infrastructure)",
          "security_network SME's TLS/DTLS handshake analysis and certificate chain validation — relevant for detecting quantum-vulnerable certificates in config files, but not directly addressed by the mission questions. Recommend adding certificate scanning as a Phase 2 feature.",
          "security_smartcontract SME's on-chain signature verification and upgradeable contract patterns — relevant for blockchain-specific scanning (Solidity ecrecover usage), but outside the scope of the current CLI tool mission. Recommend a separate 'pq-discovery-solidity' plugin.",
          "security_zk SME's zk-SNARK/zk-STARK quantum vulnerability analysis — relevant for detecting Groth16 usage (quantum-vulnerable) vs. STARK usage (quantum-resistant), but not covered by the mission questions. Recommend adding ZKP-specific detection rules in Phase 3.",
          "systems_hardware: Hardware acceleration (FPGA/ASIC for tree-sitter parsing, NVMe queue depth tuning) was not leveraged because the tool targets general-purpose developer hardware, not specialized infrastructure. However, this expertise could inform future optimizations for datacenter-scale scanning (scanning 1M+ file monorepos in <1 minute).",
          "systems_distributed: Distributed scanning across multiple machines was not addressed because the mission focuses on single-machine CLI tool design. However, this expertise could inform Phase 3 features: distributed scanning for monorepos too large for single-machine processing, or federated scanning across microservice repositories.",
          "crypto_protocol's detailed TLS 1.3 handshake structure and hybrid key exchange protocol design—relevant for understanding TLS config recommendations but not directly needed for config file parsing (which is syntax-focused, not protocol-focused)",
          "crypto_lattice's ML-KEM/ML-DSA parameter selection mathematics (NTT optimization, error distribution, concrete security estimates)—important for understanding why ML-KEM-768 is recommended for P-256 replacement, but the tool's detection logic only needs the OID-to-algorithm mapping, not the underlying lattice mathematics",
          "crypto_threshold_mpc's threshold signature protocols (FROST, threshold ML-DSA)—not relevant for static code scanning (threshold protocols are runtime constructions, not detectable from source code or config files alone)",
          "crypto_signatures' detailed BLS aggregation and post-quantum aggregation analysis—relevant for understanding why PQC migration is hard for blockchain consensus, but not directly applicable to the pq-discovery tool (which scans existing codebases, not consensus protocols)",
          "Node discovery and peer management (blockchain_node_infra) — DHT poisoning and peer identity attacks are relevant but not directly applicable to a source code scanning tool. These are runtime network attacks, not detectable via static analysis.",
          "MEV extraction and fair ordering (blockchain_tx_mev) — MEV-Boost, Flashbots, and encrypted mempools are relevant to quantum front-running mitigation but are infrastructure-level solutions, not detectable in source code. The tool can flag mempool-exposed signatures but cannot detect MEV infrastructure usage.",
          "Storage backends (blockchain_node_infra) — Quantum impact on Merkle Patricia Tries and Verkle tries is important for protocol design but not relevant to a code scanner detecting cryptographic primitive usage.",
          "Validator economics (blockchain_consensus) — Slashing conditions and staking mechanisms are not directly quantum-vulnerable (they depend on signature security, which is already covered). Economic incentives for quantum attack are important context but not a scanning target."
        ]
      }
    }
  }
}